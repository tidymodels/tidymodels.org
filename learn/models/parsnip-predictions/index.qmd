---
title: "Fitting and predicting with parsnip"
categories:
  - model fitting
  - parsnip
  - regression
  - classification
type: learn-subsection
weight: 1
description: | 
  Examples that show how to fit and predict with different combinations of model, mode, and engine.
toc: true
toc-depth: 3
include-after-body: ../../../resources.html
format:
  html:
    theme: ["style.scss"]
---

```{r}
#| label: "setup"
#| include: false
#| message: false
#| warning: false
#| eval: true
source(here::here("common.R"))

# Indicates to enable or not running Spark code
run_spark <- TRUE
run_h2o <- TRUE
run_keras <- TRUE
run_catboost <- rlang::is_installed("catboost")
run_grf <- rlang::is_installed("parsnip", version = "1.3.3.9000")
```

```{r}
#| label: "load"
#| include: false
#| eval: true
library(tidymodels)
library(sparklyr)
# Add everything here? 

#' skip format
pkgs <- c("tidymodels", "agua", "baguette", "bonsai", "censored", "discrim",
          "multilevelmod", "plsmod", "poissonreg", "rules", "sparklyr", 
          "HSAUR3", "lme4", "prodlim", "survival")
```


# Introduction

This page shows examples of how to *fit* and *predict* with different combinations of model, mode, and engine. As a reminder, in parsnip, 

- the **model type** differentiates basic modeling approaches, such as random forests, logistic regression, linear support vector machines, etc.,

- the **mode** denotes in what kind of modeling context it will be used (most commonly, classification or regression), and

- the computational **engine** indicates how the model is fit, such as with a specific R package implementation or even methods outside of R like Keras or Stan.

We'll break the examples up by their mode. For each model, we'll show different data sets used across the different engines. 

`r article_req_pkgs(pkgs)` There are numerous other "engine" packages that are required. If you use a model that is missing one or more installed packages, parsnip will prompt you to install them. There are some packages that require non-standard installation or rely on external dependencies. We'll describe these next. 

## External Dependencies

Some models available in parsnip use other computational frameworks for computations. There may be some additional downloads for engines using **catboost**, **Spark**, **h2o**, **tensorflow**/**keras**, and **torch**. You can expand the sections below to get basic installation instructions.

<details>

### catboost

catboost is a popular boosting framework. Unfortunately, the R package is not available on CRAN. First, go to [https://github.com/catboost/catboost/releases/]("https://github.com/catboost/catboost/releases/) and search for "`[R-package]`" to find the most recent release. 

The following code and be used to install and test the package (which requires the glue package to be installed): 

```{r}
#| label: catboost-install
#| eval: false
library(glue)

# Put the current version number in this variable: 
version_number <- "#.##"

template <- "https://github.com/catboost/catboost/releases/download/v{version}/catboost-R-darwin-universal2-{version}.tgz"

target_url <- glue::glue(template)
target_dest <- tempfile()
download.file(target_url, target_dest)

if (grepl("^mac", .Platform$pkgType)) {
  options <- "--no-staged-install"
} else {
  options <- character(0)
}

inst <- glue::glue("R CMD INSTALL {options} {target_dest}")
system(inst)
```

To test, fit an example model: 

```{r}
#| label: catboost-test
#| eval: false
library(catboost)

train_pool_path <- system.file("extdata", "adult_train.1000", package = "catboost")
test_pool_path <- system.file("extdata", "adult_test.1000", package = "catboost")
cd_path <- system.file("extdata", "adult.cd", package = "catboost")
train_pool <- catboost.load_pool(train_pool_path, column_description = cd_path)
test_pool <- catboost.load_pool(test_pool_path, column_description = cd_path)
fit_params <- list(
  iterations = 100,
  loss_function = 'Logloss',
  ignored_features = c(4, 9),
  border_count = 32,
  depth = 5,
  learning_rate = 0.03,
  l2_leaf_reg = 3.5,
  train_dir = tempdir())
fit_params
```

### Apache Spark

To use [Apache Spark](https://spark.apache.org/) as an engine, we will first install Spark and then need a connection to a cluster. For this article, we will set up and use a single-node Spark cluster running on a laptop.

To install, first install sparklyr:

```{r}
#| label: sparklyr-install
#| eval: false
install.packages("sparklyr")
```

and then install the Spark backend. For example, you might use: 

```{r}
#| label: spark-install
#| eval: false
library(sparklyr)
spark_install(version = "4.0")
```

Once that is working, you can get ready to fit models using: 

```{r}
#| label: spark-connect
#| eval: !expr 'run_spark'
library(sparklyr)
sc <- spark_connect("local")
```

### h2o 

h2o.ai offers a Java-based high-performance computing server for machine learning. This can be run locally or externally. There are general installation instructions at [https://docs.h2o.ai/](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html). There is a package on CRAN, but you can also install directly from [h2o](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html#install-in-r) via:

```{r}
#| label: h2o-download
#| eval: false
install.packages(
  "h2o",
  type = "source",
  repos = "http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R"
)
```

After installation is complete, you can start a local server via `h2o::h2o.init()`. 

The tidymodels [agua](https://agua.tidymodels.org/) package contains some helpers and will also need to be installed. You can use its function to start a server too:

```{r}
#| label: h2o-init
#| eval: !expr 'run_h2o'
#| results: hide
library(agua)
h2o_start()
```

### Tensorflow and Keras

R's tensorflow and keras3 packages call Python directly. To enable this, you'll have to install two R packages: 

```{r}
#| label: keras-install
#| eval: false
install.packages("keras3")
```

Once that is done, use: 
```{r}
#| label: tf-install
#| eval: false
keras3::install_keras(backend = "tensorflow")
```

There are other options for installation. See [https://tensorflow.rstudio.com/install/index.html](https://tensorflow.rstudio.com/install/index.html) for more details. 

```{r}
#| label: setup-keras
#| eval: !expr 'run_keras'
# Assumes you are going to use a virtual environment called 
pve <- grep("tensorflow", reticulate::virtualenv_list(), value = TRUE)
reticulate::use_virtualenv(pve)
```

### Torch

R's torch package is the low-level package containing the framework. Once you have installed it, you will get this message the first time you load the package: 

> Additional software needs to be downloaded and installed for torch to work correctly."

Choosing "Yes" will do the _one-time_ installation. 

</details>

To get started, let's load the tidymodels package: 

```{r}
#| label: load-tm
library(tidymodels)
theme_set(theme_bw() + theme(legend.position = "top"))
```

# Classification Models

To demonstrate classification, let's make a small training and test sets for a binary outcome. We'll center and scale the data since some models require the same units.

```{r}
#| label: bin-data
set.seed(207)
bin_split <- 
	modeldata::two_class_dat |> 
	rename(class = Class) |> 
	initial_split(prop = 0.994, strata = class)
bin_split

bin_rec <- 
  recipe(class ~ ., data = training(bin_split)) |> 
  step_normalize(all_numeric_predictors()) |> 
  prep()

bin_train <- bake(bin_rec, new_data = NULL)
bin_test <- bake(bin_rec, new_data = testing(bin_split))
```

For models that _only_ work for three or more classes, we'll simulate:

```{r}
#| label: mtl-data
#| eval: true
set.seed(1752)
mtl_data <-
 sim_multinomial(
    200,
  ~  -0.5    +  0.6 * abs(A),
  ~ ifelse(A > 0 & B > 0, 1.0 + 0.2 * A / B, - 2),
  ~ A + B -  A * B)

mtl_split <- initial_split(mtl_data, prop = 0.967, strata = class)
mtl_split

# Predictors are in the same units
mtl_train <- training(mtl_split)
mtl_test <- testing(mtl_split)
```

Finally, we have some models that handle hierarchical data, where some rows are statistically correlated with other rows. For these examples, we'll use data from a clinical trial where patients were followed over time. The outcome is binary. The data are in the HSAUR3 package. We'll split these data in a way where all rows for a specific subject are either in the training or test sets: 

```{r}
#| label: cls-hierarchical-data
set.seed(72)
cls_group_split <- 
  HSAUR3::toenail |> 
  group_initial_split(group = patientID)
cls_group_train <- training(cls_group_split)
cls_group_test <- testing(cls_group_split)
```

There are `r length(unique(cls_group_train$patientID))` subjects in the training set and `r length(unique(cls_group_test$patientID))` in the test set. 

If using the **Apache Spark** engine, we will need to identify the data source and then use it to create the splits. For this article, we will copy the `two_class_dat` and the `mtl_data` data sets into the Spark session.

```{r}
#| label: spark-bin-data
#| eval: !expr 'run_spark'
library(sparklyr)
sc <- spark_connect("local")

tbl_two_class <- copy_to(sc, modeldata::two_class_dat)

tbl_bin <- sdf_random_split(tbl_two_class, training = 0.994, test = 1-0.994, seed = 100)

tbl_sim_mtl <- copy_to(sc, mtl_data)

tbl_mtl <- sdf_random_split(tbl_sim_mtl, training = 0.967, test = 1-0.967, seed = 100)
```


## Bagged MARS (`bag_mars()`) 

:::{.panel-tabset}

## `earth` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-earth-bag-mars-classification-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-earth-bag-mars-classification
bag_mars_spec <- bag_mars() |>
  # We need to set the mode since this engine works with multiple modes
  # and earth is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-earth-bag-mars-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(268)
bag_mars_fit <- bag_mars_spec |> fit(class ~ ., data = bin_train)
bag_mars_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-earth-bag-mars-classification
predict(bag_mars_fit, type = "class", new_data = bin_test)
predict(bag_mars_fit, type = "prob", new_data = bin_test)
```

:::

## Bagged Neural Networks (`bag_mlp()`) 

:::{.panel-tabset}

## `nnet` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-nnet-bag-mlp-classification-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-nnet-bag-mlp-classification
bag_mlp_spec <- bag_mlp() |>
  # We need to set the mode since this engine works with multiple modes
  # and nnet is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-nnet-bag-mlp-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(318)
bag_mlp_fit <- bag_mlp_spec |> fit(class ~ ., data = bin_train)
bag_mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-nnet-bag-mlp-classification
predict(bag_mlp_fit, type = "class", new_data = bin_test)
predict(bag_mlp_fit, type = "prob", new_data = bin_test)
```

:::

## Bagged Decision Trees (`bag_tree()`) 

:::{.panel-tabset}

## `rpart` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-rpart-bag-tree-classification-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-rpart-bag-tree-classification
bag_tree_spec <- bag_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and rpart is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-rpart-bag-tree-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(985)
bag_tree_fit <- bag_tree_spec |> fit(class ~ ., data = bin_train)
bag_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-rpart-bag-tree-classification
predict(bag_tree_fit, type = "class", new_data = bin_test)
predict(bag_tree_fit, type = "prob", new_data = bin_test)
```

## `C5.0` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-C5.0-bag-tree-classification-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-C5.0-bag-tree-classification
bag_tree_spec <- bag_tree() |> 
  set_mode("classification") |> 
  set_engine("C5.0")
```

Now we create the model fit object:

```{r}
#| label: fit-C5.0-bag-tree-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(937)
bag_tree_fit <- bag_tree_spec |> fit(class ~ ., data = bin_train)
bag_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-C5.0-bag-tree-classification
predict(bag_tree_fit, type = "class", new_data = bin_test)
predict(bag_tree_fit, type = "prob", new_data = bin_test)
```

:::

## Bayesian Additive Regression Trees (`bart()`) 

:::{.panel-tabset}

## `dbarts` 

We create a model specification via:

```{r}
#| label: spec-dbarts-bart-classification
bart_spec <- bart() |>
  # We need to set the mode since this engine works with multiple modes
  # and dbarts is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-dbarts-bart-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(217)
bart_fit <- bart_spec |> fit(class ~ ., data = bin_train)
bart_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-dbarts-bart-classification
predict(bart_fit, type = "class", new_data = bin_test)
predict(bart_fit, type = "prob", new_data = bin_test)
predict(bart_fit, type = "conf_int", new_data = bin_test)
predict(bart_fit, type = "pred_int", new_data = bin_test)
```

:::

## Boosted Decision Trees (`boost_tree()`) 

:::{.panel-tabset}

## `xgboost` 

We create a model specification via:

```{r}
#| label: spec-xgboost-boost-tree-classification
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and xgboost is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-xgboost-boost-tree-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(738)
boost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-xgboost-boost-tree-classification
predict(boost_tree_fit, type = "class", new_data = bin_test)
predict(boost_tree_fit, type = "prob", new_data = bin_test)
```

## `C5.0` 

We create a model specification via:

```{r}
#| label: spec-C5.0-boost-tree-classification
boost_tree_spec <- boost_tree() |> 
  set_mode("classification") |> 
  set_engine("C5.0")
```

Now we create the model fit object:

```{r}
#| label: fit-C5.0-boost-tree-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(984)
boost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-C5.0-boost-tree-classification
predict(boost_tree_fit, type = "class", new_data = bin_test)
predict(boost_tree_fit, type = "prob", new_data = bin_test)
```

## `catboost` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-catboost-boost-tree-classification-bonsai
#| output: false
#| eval: !expr 'run_catboost'
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-catboost-boost-tree-classification
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("catboost")
```

Now we create the model fit object:

```{r}
#| label: fit-catboost-boost-tree-classification
#| eval: !expr 'run_catboost'
# Set the random number seed to an integer for reproducibility: 
set.seed(644)
boost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-catboost-boost-tree-classification
#| eval: !expr 'run_catboost'
predict(boost_tree_fit, type = "class", new_data = bin_test)
predict(boost_tree_fit, type = "prob", new_data = bin_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-boost-tree-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-boost-tree-classification
#| eval: !expr 'run_h2o'
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("h2o_gbm")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-boost-tree-classification
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(186)
boost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-boost-tree-classification
#| eval: !expr 'run_h2o'
predict(boost_tree_fit, type = "class", new_data = bin_test)
predict(boost_tree_fit, type = "prob", new_data = bin_test)
```

## `h2o_gbm` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-gbm-boost-tree-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-gbm-boost-tree-classification
#| eval: !expr 'run_h2o'
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("h2o_gbm")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-gbm-boost-tree-classification
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(724)
boost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-gbm-boost-tree-classification
#| eval: !expr 'run_h2o'
predict(boost_tree_fit, type = "class", new_data = bin_test)
predict(boost_tree_fit, type = "prob", new_data = bin_test)
```

## `lightgbm` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-lightgbm-boost-tree-classification-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-lightgbm-boost-tree-classification
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("lightgbm")
```

Now we create the model fit object:

```{r}
#| label: fit-lightgbm-boost-tree-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(906)
boost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-lightgbm-boost-tree-classification
predict(boost_tree_fit, type = "class", new_data = bin_test)
predict(boost_tree_fit, type = "prob", new_data = bin_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-boost-tree-classification
#| eval: !expr 'run_spark'
boost_tree_spec <- boost_tree() |> 
  set_mode("classification") |> 
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-boost-tree-classification
#| eval: !expr 'run_spark'
# Set the random number seed to an integer for reproducibility: 
set.seed(285)
boost_tree_fit <- boost_tree_spec |> fit(Class ~ ., data = tbl_bin$training)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-boost-tree-classification
#| eval: !expr 'run_spark'
predict(boost_tree_fit, type = "class", new_data = tbl_bin$test)
predict(boost_tree_fit, type = "prob", new_data = tbl_bin$test)
```

:::

## C5 Rules (`C5_rules()`) 

:::{.panel-tabset}

## `C5.0` 

This engine requires the rules extension package, so let's load this first:

```{r}
#| label: load-C5.0-C5-rules-classification-rules
#| output: false
library(rules)
```

We create a model specification via:

```{r}
#| label: spec-C5.0-C5-rules-classification
# This engine works with a single mode so no need to set that
# and C5.0 is the default engine so there is no need to set that either.
C5_rules_spec <- C5_rules()
```

Now we create the model fit object:

```{r}
#| label: fit-C5.0-C5-rules-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(93)
C5_rules_fit <- C5_rules_spec |> fit(class ~ ., data = bin_train)
C5_rules_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-C5.0-C5-rules-classification
predict(C5_rules_fit, type = "class", new_data = bin_test)
predict(C5_rules_fit, type = "prob", new_data = bin_test)
```

:::

## Decision Tree (`decision_tree()`) 

:::{.panel-tabset}

## `rpart` 

We create a model specification via:

```{r}
#| label: spec-rpart-decision-tree-classification
decision_tree_spec <- decision_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and rpart is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-rpart-decision-tree-classification
decision_tree_fit <- decision_tree_spec |> fit(class ~ ., data = bin_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-rpart-decision-tree-classification
predict(decision_tree_fit, type = "class", new_data = bin_test)
predict(decision_tree_fit, type = "prob", new_data = bin_test)
```

## `C5.0` 

We create a model specification via:

```{r}
#| label: spec-C5.0-decision-tree-classification
decision_tree_spec <- decision_tree() |> 
  set_mode("classification") |> 
  set_engine("C5.0")
```

Now we create the model fit object:

```{r}
#| label: fit-C5.0-decision-tree-classification
decision_tree_fit <- decision_tree_spec |> fit(class ~ ., data = bin_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-C5.0-decision-tree-classification
predict(decision_tree_fit, type = "class", new_data = bin_test)
predict(decision_tree_fit, type = "prob", new_data = bin_test)
```

## `partykit` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-partykit-decision-tree-classification-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-partykit-decision-tree-classification
decision_tree_spec <- decision_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("partykit")
```

Now we create the model fit object:

```{r}
#| label: fit-partykit-decision-tree-classification
decision_tree_fit <- decision_tree_spec |> fit(class ~ ., data = bin_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-partykit-decision-tree-classification
predict(decision_tree_fit, type = "class", new_data = bin_test)
predict(decision_tree_fit, type = "prob", new_data = bin_test)
```


## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-decision-tree-classification
#| eval: !expr 'run_spark'
decision_tree_spec <- decision_tree() |>
  set_mode("classification") |> 
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-decision-tree-classification
#| eval: !expr 'run_spark'
decision_tree_fit <- decision_tree_spec |> fit(Class ~ ., data = tbl_bin$training)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-decision-tree-classification
#| eval: !expr 'run_spark'
predict(decision_tree_fit, type = "class", new_data = tbl_bin$test)
predict(decision_tree_fit, type = "prob", new_data = tbl_bin$test)
```

:::

## Flexible Discriminant Analysis (`discrim_flexible()`) 

:::{.panel-tabset}

## `earth` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-earth-discrim-flexible-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-earth-discrim-flexible-classification
# This engine works with a single mode so no need to set that
# and earth is the default engine so there is no need to set that either.
discrim_flexible_spec <- discrim_flexible()
```

Now we create the model fit object:

```{r}
#| label: fit-earth-discrim-flexible-classification
discrim_flexible_fit <- discrim_flexible_spec |> fit(class ~ ., data = bin_train)
discrim_flexible_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-earth-discrim-flexible-classification
predict(discrim_flexible_fit, type = "class", new_data = bin_test)
predict(discrim_flexible_fit, type = "prob", new_data = bin_test)
```

:::

## Linear Discriminant Analysis (`discrim_linear()`) 

:::{.panel-tabset}

## `MASS` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-MASS-discrim-linear-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-MASS-discrim-linear-classification
# This engine works with a single mode so no need to set that
# and MASS is the default engine so there is no need to set that either.
discrim_linear_spec <- discrim_linear()
```

Now we create the model fit object:

```{r}
#| label: fit-MASS-discrim-linear-classification
discrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)
discrim_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-MASS-discrim-linear-classification
predict(discrim_linear_fit, type = "class", new_data = bin_test)
predict(discrim_linear_fit, type = "prob", new_data = bin_test)
```

## `mda` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-mda-discrim-linear-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-mda-discrim-linear-classification
discrim_linear_spec <- discrim_linear() |> 
  # This engine works with a single mode so no need to set that
  set_engine("mda")
```

Now we create the model fit object:

```{r}
#| label: fit-mda-discrim-linear-classification
discrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)
discrim_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-mda-discrim-linear-classification
predict(discrim_linear_fit, type = "class", new_data = bin_test)
predict(discrim_linear_fit, type = "prob", new_data = bin_test)
```

## `sda` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-sda-discrim-linear-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-sda-discrim-linear-classification
discrim_linear_spec <- discrim_linear() |> 
  # This engine works with a single mode so no need to set that
  set_engine("sda")
```

Now we create the model fit object:

```{r}
#| label: fit-sda-discrim-linear-classification
discrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)
discrim_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-sda-discrim-linear-classification
predict(discrim_linear_fit, type = "class", new_data = bin_test)
predict(discrim_linear_fit, type = "prob", new_data = bin_test)
```

## `sparsediscrim` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-sparsediscrim-discrim-linear-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-sparsediscrim-discrim-linear-classification
discrim_linear_spec <- discrim_linear() |> 
  # This engine works with a single mode so no need to set that
  set_engine("sparsediscrim")
```

Now we create the model fit object:

```{r}
#| label: fit-sparsediscrim-discrim-linear-classification
discrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)
discrim_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-sparsediscrim-discrim-linear-classification
predict(discrim_linear_fit, type = "class", new_data = bin_test)
predict(discrim_linear_fit, type = "prob", new_data = bin_test)
```

:::

## Quandratic Discriminant Analysis (`discrim_quad()`) 

:::{.panel-tabset}

## `MASS` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-MASS-discrim-quad-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-MASS-discrim-quad-classification
discrim_quad_spec <- discrim_quad()
  # This engine works with a single mode so no need to set that
  # and MASS is the default engine so there is no need to set that either.
```

Now we create the model fit object:

```{r}
#| label: fit-MASS-discrim-quad-classification
discrim_quad_fit <- discrim_quad_spec |> fit(class ~ ., data = bin_train)
discrim_quad_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-MASS-discrim-quad-classification
predict(discrim_quad_fit, type = "class", new_data = bin_test)
predict(discrim_quad_fit, type = "prob", new_data = bin_test)
```

## `sparsediscrim` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-sparsediscrim-discrim-quad-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-sparsediscrim-discrim-quad-classification
discrim_quad_spec <- discrim_quad() |> 
  # This engine works with a single mode so no need to set that
  set_engine("sparsediscrim")
```

Now we create the model fit object:

```{r}
#| label: fit-sparsediscrim-discrim-quad-classification
discrim_quad_fit <- discrim_quad_spec |> fit(class ~ ., data = bin_train)
discrim_quad_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-sparsediscrim-discrim-quad-classification
predict(discrim_quad_fit, type = "class", new_data = bin_test)
predict(discrim_quad_fit, type = "prob", new_data = bin_test)
```

:::

## Regularized Discriminant Analysis (`discrim_regularized()`) 

:::{.panel-tabset}

## `klaR` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-klaR-discrim-regularized-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-klaR-discrim-regularized-classification
# This engine works with a single mode so no need to set that
# and klaR is the default engine so there is no need to set that either.
discrim_regularized_spec <- discrim_regularized()
```

Now we create the model fit object:

```{r}
#| label: fit-klaR-discrim-regularized-classification
discrim_regularized_fit <- discrim_regularized_spec |> fit(class ~ ., data = bin_train)
discrim_regularized_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-klaR-discrim-regularized-classification
predict(discrim_regularized_fit, type = "class", new_data = bin_test)
predict(discrim_regularized_fit, type = "prob", new_data = bin_test)
```

:::

## Generalized Additive Models (`gen_additive_mod()`) 

:::{.panel-tabset}

## `mgcv` 

We create a model specification via:

```{r}
#| label: spec-mgcv-gen-additive-mod-classification
gen_additive_mod_spec <- gen_additive_mod() |>
  # We need to set the mode since this engine works with multiple modes
  # and mgcv is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-mgcv-gen-additive-mod-classification
gen_additive_mod_fit <- 
  gen_additive_mod_spec |> 
  fit(class ~ s(A) + s(B), data = bin_train)
gen_additive_mod_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-mgcv-gen-additive-mod-classification
predict(gen_additive_mod_fit, type = "class", new_data = bin_test)
predict(gen_additive_mod_fit, type = "prob", new_data = bin_test)
predict(gen_additive_mod_fit, type = "conf_int", new_data = bin_test)
```

:::

## Logistic Regression (`logistic_reg()`) 

:::{.panel-tabset}

## `glm` 

We create a model specification via:

```{r}
#| label: spec-glm-logistic-reg-classification
logistic_reg_spec <- logistic_reg()
  # This engine works with a single mode so no need to set that
  # and glm is the default engine so there is no need to set that either.
```

Now we create the model fit object:

```{r}
#| label: fit-glm-logistic-reg-classification
logistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glm-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = bin_test)
predict(logistic_reg_fit, type = "prob", new_data = bin_test)
predict(logistic_reg_fit, type = "conf_int", new_data = bin_test)
```

## `brulee` 

We create a model specification via:

```{r}
#| label: spec-brulee-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("brulee")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-logistic-reg-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(466)
logistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = bin_test)
predict(logistic_reg_fit, type = "prob", new_data = bin_test)
```

## `gee` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-gee-logistic-reg-classification-multilevelmod
#| output: false
library(multilevelmod)

```

We create a model specification via:

```{r}
#| label: spec-gee-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("gee")
```

Now we create the model fit object:

```{r}
#| label: fit-gee-logistic-reg-classification
logistic_reg_fit <- 
  logistic_reg_spec |> 
  fit(outcome ~ treatment * visit + id_var(patientID), data = cls_group_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-gee-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = cls_group_test)
predict(logistic_reg_fit, type = "prob", new_data = cls_group_test)
```

## `glmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-glmer-logistic-reg-classification-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-glmer-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmer")
```

Now we create the model fit object:

```{r}
#| label: fit-glmer-logistic-reg-classification
logistic_reg_fit <- 
  logistic_reg_spec |> 
  fit(outcome ~ treatment * visit + (1 | patientID), data = cls_group_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmer-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = cls_group_test)
predict(logistic_reg_fit, type = "prob", new_data = cls_group_test)
```

## `glmnet` 

We create a model specification via:

```{r}
#| label: spec-glmnet-logistic-reg-classification
logistic_reg_spec <- logistic_reg(penalty = 0.01) |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmnet")
```

Now we create the model fit object:

```{r}
#| label: fit-glmnet-logistic-reg-classification
logistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmnet-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = bin_test)
predict(logistic_reg_fit, type = "prob", new_data = bin_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-logistic-reg-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-logistic-reg-classification
#| eval: !expr 'run_h2o'
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-logistic-reg-classification
#| eval: !expr 'run_h2o'
logistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-logistic-reg-classification
#| eval: !expr 'run_h2o'
predict(logistic_reg_fit, type = "class", new_data = bin_test)
predict(logistic_reg_fit, type = "prob", new_data = bin_test)
```

## `keras` 

We create a model specification via:

```{r}
#| label: spec-keras-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("keras")
```

Now we create the model fit object:

```{r}
#| label: fit-keras-logistic-reg-classification
#| eval: !expr 'run_keras'
#| results: hide
# Set the random number seed to an integer for reproducibility: 
set.seed(730)
logistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)
```

```{r}
#| label: fit-keras-logistic-reg-classification-print
#| eval: !expr 'run_keras'
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-keras-logistic-reg-classification
#| eval: !expr 'run_keras'
predict(logistic_reg_fit, type = "class", new_data = bin_test)
predict(logistic_reg_fit, type = "prob", new_data = bin_test)
```

## `LiblineaR` 

We create a model specification via:

```{r}
#| label: spec-LiblineaR-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("LiblineaR")
```

Now we create the model fit object:

```{r}
#| label: fit-LiblineaR-logistic-reg-classification
logistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-LiblineaR-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = bin_test)
predict(logistic_reg_fit, type = "prob", new_data = bin_test)
```

## `stan` 

We create a model specification via:

```{r}
#| label: spec-stan-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("stan")
```

Now we create the model fit object:

```{r}
#| label: fit-stan-logistic-reg-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(96)
logistic_reg_fit <- 
  logistic_reg_spec |> 
  fit(outcome ~ treatment * visit, data = cls_group_train)
logistic_reg_fit |> print(digits = 3)
```

The holdout data can be predicted:

```{r}
#| label: predict-stan-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = cls_group_test)
predict(logistic_reg_fit, type = "prob", new_data = cls_group_test)
predict(logistic_reg_fit, type = "conf_int", new_data = cls_group_test)
predict(logistic_reg_fit, type = "pred_int", new_data = cls_group_test)
```

## `stan_glmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-stan-glmer-logistic-reg-classification-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-stan-glmer-logistic-reg-classification
logistic_reg_spec <- logistic_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("stan_glmer")
```

Now we create the model fit object:

```{r}
#| label: fit-stan-glmer-logistic-reg-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(484)
logistic_reg_fit <- 
  logistic_reg_spec |> 
  fit(outcome ~ treatment * visit + (1 | patientID), data = cls_group_train)
logistic_reg_fit |> print(digits = 3)
```

The holdout data can be predicted:

```{r}
#| label: predict-stan-glmer-logistic-reg-classification
predict(logistic_reg_fit, type = "class", new_data = cls_group_test)
predict(logistic_reg_fit, type = "prob", new_data = cls_group_test)
predict(logistic_reg_fit, type = "conf_int", new_data = cls_group_test)
predict(logistic_reg_fit, type = "pred_int", new_data = cls_group_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-logistic-reg-classification
#| eval: !expr 'run_spark'
logistic_reg_spec <- logistic_reg() |> 
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-logistic-reg-classification
#| eval: !expr 'run_spark'
logistic_reg_fit <- logistic_reg_spec |> fit(Class ~ ., data = tbl_bin$training)
logistic_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-logistic-reg-classification
#| eval: !expr 'run_spark'
predict(logistic_reg_fit, type = "class", new_data = tbl_bin$test)
predict(logistic_reg_fit, type = "prob", new_data = tbl_bin$test)
```

:::

## Multivariate Adaptive Regression Splines (`mars()`) 

:::{.panel-tabset}

## `earth` 

We create a model specification via:

```{r}
#| label: spec-earth-mars-classification
mars_spec <- mars() |>
  # We need to set the mode since this engine works with multiple modes
  # and earth is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-earth-mars-classification
mars_fit <- mars_spec |> fit(class ~ ., data = bin_train)
mars_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-earth-mars-classification
predict(mars_fit, type = "class", new_data = bin_test)
predict(mars_fit, type = "prob", new_data = bin_test)
```

:::

## Neural Networks (`mlp()`) 

:::{.panel-tabset}

## `nnet` 

We create a model specification via:

```{r}
#| label: spec-nnet-mlp-classification
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  # and nnet is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-nnet-mlp-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(839)
mlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-nnet-mlp-classification
predict(mlp_fit, type = "class", new_data = bin_test)
predict(mlp_fit, type = "prob", new_data = bin_test)
```

## `brulee` 

We create a model specification via:

```{r}
#| label: spec-brulee-mlp-classification
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("brulee")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-mlp-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(38)
mlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-mlp-classification
predict(mlp_fit, type = "class", new_data = bin_test)
predict(mlp_fit, type = "prob", new_data = bin_test)
```

## `brulee_two_layer` 

We create a model specification via:

```{r}
#| label: spec-brulee-two-layer-mlp-classification
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("brulee_two_layer")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-two-layer-mlp-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(336)
mlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-two-layer-mlp-classification
predict(mlp_fit, type = "class", new_data = bin_test)
predict(mlp_fit, type = "prob", new_data = bin_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-mlp-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-mlp-classification
#| eval: !expr 'run_h2o'
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-mlp-classification
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(306)
mlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-mlp-classification
#| eval: !expr 'run_h2o'
predict(mlp_fit, type = "class", new_data = bin_test)
predict(mlp_fit, type = "prob", new_data = bin_test)
```

## `keras` 

We create a model specification via:

```{r}
#| label: spec-keras-mlp-classification
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("keras")
```

Now we create the model fit object:

```{r}
#| label: fit-keras-mlp-classification
#| eval: !expr 'run_keras'
#| results: hide
# Set the random number seed to an integer for reproducibility: 
set.seed(216)
mlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)
```

```{r}
#| label: fit-keras-mlp-classification-print
#| eval: !expr 'run_keras'
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-keras-mlp-classification
#| eval: !expr 'run_keras'
predict(mlp_fit, type = "class", new_data = bin_test)
predict(mlp_fit, type = "prob", new_data = bin_test)
```

:::

## Multinom Regression (`multinom_reg()`) 

:::{.panel-tabset}

## `nnet` 

We create a model specification via:

```{r}
#| label: spec-nnet-multinom-reg-classification
# This engine works with a single mode so no need to set that
# and nnet is the default engine so there is no need to set that either.
multinom_reg_spec <- multinom_reg()
```

Now we create the model fit object:

```{r}
#| label: fit-nnet-multinom-reg-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(634)
multinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)
multinom_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-nnet-multinom-reg-classification
predict(multinom_reg_fit, type = "class", new_data = mtl_test)
predict(multinom_reg_fit, type = "prob", new_data = mtl_test)
```

## `brulee` 

We create a model specification via:

```{r}
#| label: spec-brulee-multinom-reg-classification
multinom_reg_spec <- multinom_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("brulee")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-multinom-reg-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(837)
multinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)
multinom_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-multinom-reg-classification
predict(multinom_reg_fit, type = "class", new_data = mtl_test)
predict(multinom_reg_fit, type = "prob", new_data = mtl_test)
```

## `glmnet` 

We create a model specification via:

```{r}
#| label: spec-glmnet-multinom-reg-classification
multinom_reg_spec <- multinom_reg(penalty = 0.01) |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmnet")
```

Now we create the model fit object:

```{r}
#| label: fit-glmnet-multinom-reg-classification
multinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)
multinom_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmnet-multinom-reg-classification
predict(multinom_reg_fit, type = "class", new_data = mtl_test)
predict(multinom_reg_fit, type = "prob", new_data = mtl_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-multinom-reg-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-multinom-reg-classification
#| eval: !expr 'run_h2o'
multinom_reg_spec <- multinom_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-multinom-reg-classification
#| eval: !expr 'run_h2o'
multinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)
multinom_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-multinom-reg-classification
#| eval: !expr 'run_h2o'
predict(multinom_reg_fit, type = "class", new_data = mtl_test)
predict(multinom_reg_fit, type = "prob", new_data = mtl_test)
```

## `keras` 

We create a model specification via:

```{r}
#| label: spec-keras-multinom-reg-classification
multinom_reg_spec <- multinom_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("keras")
```

Now we create the model fit object:

```{r}
#| label: fit-keras-multinom-reg-classification
#| eval: !expr 'run_keras'
#| results: hide
multinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)
```

```{r}
#| label: fit-keras-multinom-reg-classification-print
#| eval: !expr 'run_keras'
multinom_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-keras-multinom-reg-classification
#| eval: !expr 'run_keras'
predict(multinom_reg_fit, type = "class", new_data = mtl_test)
predict(multinom_reg_fit, type = "prob", new_data = mtl_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-multinom-reg-classification
#| eval: !expr 'run_spark'
multinom_reg_spec <- multinom_reg() |> 
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-multinom-reg-classification
#| eval: !expr 'run_spark'
multinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = tbl_mtl$training)
multinom_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-multinom-reg-classification
#| eval: !expr 'run_spark'
predict(multinom_reg_fit, type = "class", new_data = tbl_mtl$test)
predict(multinom_reg_fit, type = "prob", new_data = tbl_mtl$test)
```

:::

## Naive Bayes (`naive_Bayes()`) 

:::{.panel-tabset}

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-naive-Bayes-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-naive-Bayes-classification
#| eval: !expr 'run_h2o'
naive_Bayes_spec <- naive_Bayes() |> 
  # This engine works with a single mode so no need to set that
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-naive-Bayes-classification
#| eval: !expr 'run_h2o'
naive_Bayes_fit <- naive_Bayes_spec |> fit(class ~ ., data = bin_train)
naive_Bayes_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-naive-Bayes-classification
#| eval: !expr 'run_h2o'
predict(naive_Bayes_fit, type = "class", new_data = bin_test)
predict(naive_Bayes_fit, type = "prob", new_data = bin_test)
```

## `klaR` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-klaR-naive-Bayes-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-klaR-naive-Bayes-classification
# This engine works with a single mode so no need to set that
# and klaR is the default engine so there is no need to set that either.
naive_Bayes_spec <- naive_Bayes()
```

Now we create the model fit object:

```{r}
#| label: fit-klaR-naive-Bayes-classification
naive_Bayes_fit <- naive_Bayes_spec |> fit(class ~ ., data = bin_train)
```

The holdout data can be predicted:

```{r}
#| label: predict-klaR-naive-Bayes-classification
predict(naive_Bayes_fit, type = "class", new_data = bin_test)
predict(naive_Bayes_fit, type = "prob", new_data = bin_test)
```

## `naivebayes` 

This engine requires the discrim extension package, so let's load this first:

```{r}
#| label: load-naivebayes-naive-Bayes-classification-discrim
#| output: false
library(discrim)
```

We create a model specification via:

```{r}
#| label: spec-naivebayes-naive-Bayes-classification
naive_Bayes_spec <- naive_Bayes() |> 
  # This engine works with a single mode so no need to set that
  set_engine("naivebayes")
```

Now we create the model fit object:

```{r}
#| label: fit-naivebayes-naive-Bayes-classification
naive_Bayes_fit <- naive_Bayes_spec |> fit(class ~ ., data = bin_train)
naive_Bayes_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-naivebayes-naive-Bayes-classification
predict(naive_Bayes_fit, type = "class", new_data = bin_test)
predict(naive_Bayes_fit, type = "prob", new_data = bin_test)
```

:::

## K-Nearest Neighbors (`nearest_neighbor()`) 

:::{.panel-tabset}

## `kknn` 

We create a model specification via:

```{r}
#| label: spec-kknn-nearest-neighbor-classification
nearest_neighbor_spec <- nearest_neighbor() |>
  # We need to set the mode since this engine works with multiple modes
  # and kknn is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-kknn-nearest-neighbor-classification
nearest_neighbor_fit <- nearest_neighbor_spec |> fit(class ~ ., data = bin_train)
nearest_neighbor_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kknn-nearest-neighbor-classification
predict(nearest_neighbor_fit, type = "class", new_data = bin_test)
predict(nearest_neighbor_fit, type = "prob", new_data = bin_test)
```

:::

## Null Model (`null_model()`) 

:::{.panel-tabset}

## `parsnip` 

We create a model specification via:

```{r}
#| label: spec-parsnip-null-model-classification
null_model_spec <- null_model() |>
  # We need to set the mode since this engine works with multiple modes
  # and parsnip is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-parsnip-null-model-classification
null_model_fit <- null_model_spec |> fit(class ~ ., data = bin_train)
null_model_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-parsnip-null-model-classification
predict(null_model_fit, type = "class", new_data = bin_test)
predict(null_model_fit, type = "prob", new_data = bin_test)
```

:::

## Partial Least Squares (`pls()`) 

:::{.panel-tabset}

## `mixOmics` 

This engine requires the plsmod extension package, so let's load this first:

```{r}
#| label: load-mixOmics-pls-classification-plsmod
#| output: false
library(plsmod)
```

We create a model specification via:

```{r}
#| label: spec-mixOmics-pls-classification
pls_spec <- pls() |>
  # We need to set the mode since this engine works with multiple modes
  # and mixOmics is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-mixOmics-pls-classification
pls_fit <- pls_spec |> fit(class ~ ., data = bin_train)
pls_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-mixOmics-pls-classification
predict(pls_fit, type = "class", new_data = bin_test)
predict(pls_fit, type = "prob", new_data = bin_test)
```

:::

## Random Forests (`rand_forest()`) 

:::{.panel-tabset}

## `ranger` 

We create a model specification via:

```{r}
#| label: spec-ranger-rand-forest-classification
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  # and ranger is the default engine so there is no need to set that either.
  set_engine("ranger", keep.inbag = TRUE) |> 
  # However, we'll set the engine and use the keep.inbag=TRUE option so that we 
  # can produce interval predictions. This is not generally required. 
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-ranger-rand-forest-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(841)
rand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-ranger-rand-forest-classification
predict(rand_forest_fit, type = "class", new_data = bin_test)
predict(rand_forest_fit, type = "prob", new_data = bin_test)
predict(rand_forest_fit, type = "conf_int", new_data = bin_test)
```

## `aorsf` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-aorsf-rand-forest-classification-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-aorsf-rand-forest-classification
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("aorsf")
```

Now we create the model fit object:

```{r}
#| label: fit-aorsf-rand-forest-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(923)
rand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-aorsf-rand-forest-classification
predict(rand_forest_fit, type = "class", new_data = bin_test)
predict(rand_forest_fit, type = "prob", new_data = bin_test)
```

## `grf` 

We create a model specification via:

```{r}
#| label: spec-grf-rand-forest-classification
#| eval: !expr 'run_grf'
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("grf")
```

Now we create the model fit object:

```{r}
#| label: fit-grf-rand-forest-classification
#| eval: !expr 'run_grf'
# Set the random number seed to an integer for reproducibility: 
set.seed(546)
rand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-grf-rand-forest-classification
#| eval: !expr 'run_grf'
predict(rand_forest_fit, type = "class", new_data = bin_test)
predict(rand_forest_fit, type = "prob", new_data = bin_test)
predict(rand_forest_fit, type = "conf_int", new_data = bin_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-rand-forest-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-rand-forest-classification
#| eval: !expr 'run_h2o'
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-rand-forest-classification
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(493)
rand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-rand-forest-classification
#| eval: !expr 'run_h2o'
predict(rand_forest_fit, type = "class", new_data = bin_test)
predict(rand_forest_fit, type = "prob", new_data = bin_test)
```

## `partykit` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-partykit-rand-forest-classification-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-partykit-rand-forest-classification
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("partykit")
```

Now we create the model fit object:

```{r}
#| label: fit-partykit-rand-forest-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(252)
rand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)
```

The print method has a lot of output: 

<details>
```{r}
#| label: fit-partykit-rand-forest-classification-print
capture.output(print(rand_forest_fit))[1:100] |> cat(sep = "\n")
```
</details>

The holdout data can be predicted:

```{r}
#| label: predict-partykit-rand-forest-classification
predict(rand_forest_fit, type = "class", new_data = bin_test)
predict(rand_forest_fit, type = "prob", new_data = bin_test)
```

## `randomForest` 

We create a model specification via:

```{r}
#| label: spec-randomForest-rand-forest-classification
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("randomForest")
```

Now we create the model fit object:

```{r}
#| label: fit-randomForest-rand-forest-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(726)
rand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-randomForest-rand-forest-classification
predict(rand_forest_fit, type = "class", new_data = bin_test)
predict(rand_forest_fit, type = "prob", new_data = bin_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-rand-forest-classification
#| eval: !expr 'run_spark'
rand_forest_spec <- rand_forest() |>
  set_mode("classification") |>
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-rand-forest-classification
#| eval: !expr 'run_spark'
# Set the random number seed to an integer for reproducibility: 
set.seed(693)
rand_forest_fit <- rand_forest_spec |> fit(Class ~ ., data = tbl_bin$training)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| eval: !expr 'run_spark'
#| label: predict-spark-rand-forest-classification
predict(rand_forest_fit, type = "class", new_data = tbl_bin$test)
predict(rand_forest_fit, type = "prob", new_data = tbl_bin$test)
```

:::

## Rule Fit (`rule_fit()`) 

:::{.panel-tabset}

## `xrf` 

This engine requires the rules extension package, so let's load this first:

```{r}
#| label: load-xrf-rule-fit-classification-rules
#| output: false
library(rules)
```

We create a model specification via:

```{r}
#| label: spec-xrf-rule-fit-classification
rule_fit_spec <- rule_fit() |>
  # We need to set the mode since this engine works with multiple modes
  # and xrf is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-xrf-rule-fit-classification
# Set the random number seed to an integer for reproducibility: 
set.seed(95)
rule_fit_fit <- rule_fit_spec |> fit(class ~ ., data = bin_train)
rule_fit_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-xrf-rule-fit-classification
predict(rule_fit_fit, type = "class", new_data = bin_test)
predict(rule_fit_fit, type = "prob", new_data = bin_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-rule-fit-classification-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-rule-fit-classification
#| eval: !expr 'run_h2o'
rule_fit_spec <- rule_fit() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-rule-fit-classification
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(536)
rule_fit_fit <- rule_fit_spec |> fit(class ~ ., data = bin_train)
rule_fit_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-rule-fit-classification
#| eval: !expr 'run_h2o'
predict(rule_fit_fit, type = "class", new_data = bin_test)
predict(rule_fit_fit, type = "prob", new_data = bin_test)
```

:::

## Support Vector Machine (Linear Kernel) (`svm_linear()`) 

:::{.panel-tabset}

## `kernlab` 

We create a model specification via:

```{r}
#| label: spec-kernlab-svm-linear-classification
svm_linear_spec <- svm_linear() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("classification") |>
  set_engine("kernlab")
```

Now we create the model fit object:

```{r}
#| label: fit-kernlab-svm-linear-classification
svm_linear_fit <- svm_linear_spec |> fit(class ~ ., data = bin_train)
svm_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kernlab-svm-linear-classification
predict(svm_linear_fit, type = "class", new_data = bin_test)
predict(svm_linear_fit, type = "prob", new_data = bin_test)
```

## `LiblineaR` 

We create a model specification via:

```{r}
#| label: spec-LiblineaR-svm-linear-classification
svm_linear_spec <- svm_linear() |>
  # We need to set the mode since this engine works with multiple modes
  # and LiblineaR is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-LiblineaR-svm-linear-classification
svm_linear_fit <- svm_linear_spec |> fit(class ~ ., data = bin_train)
svm_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-LiblineaR-svm-linear-classification
predict(svm_linear_fit, type = "class", new_data = bin_test)
```

:::

## Support Vector Machine (Polynomial Kernel) (`svm_poly()`) 

:::{.panel-tabset}

## `kernlab` 

We create a model specification via:

```{r}
#| label: spec-kernlab-svm-poly-classification
svm_poly_spec <- svm_poly() |>
  # We need to set the mode since this engine works with multiple modes
  # and kernlab is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-kernlab-svm-poly-classification
svm_poly_fit <- svm_poly_spec |> fit(class ~ ., data = bin_train)
svm_poly_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kernlab-svm-poly-classification
predict(svm_poly_fit, type = "class", new_data = bin_test)
predict(svm_poly_fit, type = "prob", new_data = bin_test)
```

:::

## Support Vector Machine (Radial Basis Function Kernel) (`svm_rbf()`) 

:::{.panel-tabset}

## `kernlab` 

We create a model specification via:

```{r}
#| label: spec-kernlab-svm-rbf-classification
svm_rbf_spec <- svm_rbf() |>
  # We need to set the mode since this engine works with multiple modes
  # and kernlab is the default engine so there is no need to set that either.
  set_mode("classification")
```

Now we create the model fit object:

```{r}
#| label: fit-kernlab-svm-rbf-classification
svm_rbf_fit <- svm_rbf_spec |> fit(class ~ ., data = bin_train)
svm_rbf_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kernlab-svm-rbf-classification
predict(svm_rbf_fit, type = "class", new_data = bin_test)
predict(svm_rbf_fit, type = "prob", new_data = bin_test)
```

:::

# Regression Models

To demonstrate regression, we'll subset some data. make a training/test split, and standardize the predictors: 

```{r}
#| label: reg-data
set.seed(938)
reg_split <-
  modeldata::concrete |> 
  slice_sample(n = 100) |> 
  select(strength = compressive_strength, cement, age) |> 
  initial_split(prop = 0.95, strata = strength)
reg_split

reg_rec <- 
  recipe(strength ~ ., data = training(reg_split)) |> 
  step_normalize(all_numeric_predictors()) |> 
  prep()

reg_train <- bake(reg_rec, new_data = NULL)
reg_test <- bake(reg_rec, new_data = testing(reg_split))
```

We also have models that are specifically designed for integer count outcomes. The data for these are:

```{r}
#| label: count-data
set.seed(207)
count_split <-
  attrition |>
  select(num_years = TotalWorkingYears, age = Age, income = MonthlyIncome) |>
  initial_split(prop = 0.994)
count_split

count_rec <-
  recipe(num_years ~ ., data = training(count_split)) |>
  step_normalize(all_numeric_predictors()) |>
  prep()

count_train <- bake(count_rec, new_data = NULL)
count_test <- bake(count_rec, new_data = testing(count_split))
```

Finally, we have some models that handle hierarchical data, where some rows are statistically correlated with other rows. For these examples, we'll use a data set that models body weights as a function of time for several "subjects" (rats, actually). We'll split these data in a way where all rows for a specific subject are either in the training or test sets: 

```{r}
#| label: reg-hierarchical-data
set.seed(224)
reg_group_split <- 
  nlme::BodyWeight |> 
  # Get rid of some extra attributes added by the nlme package
  as_tibble() |> 
  # Convert to an _unordered_ factor
  mutate(Rat = factor(as.character(Rat))) |> 
  group_initial_split(group = Rat)
reg_group_train <- training(reg_group_split)
reg_group_test <- testing(reg_group_split)
```

There are `r length(unique(reg_group_train$Rat))` subjects in the training set and `r length(unique(reg_group_test$Rat))` in the test set. 

If using the **Apache Spark** engine, we will need to identify the data source, and then use it to create the splits. For this article, we will copy the `concrete` data set into the Spark session.

```{r}
#| label: spark-reg-data
#| eval: !expr 'run_spark'

library(sparklyr)
sc <- spark_connect("local")

tbl_concrete <- copy_to(sc, modeldata::concrete)

tbl_reg <- sdf_random_split(tbl_concrete, training = 0.95, test = 0.05, seed = 100)
```

## Bagged MARS (`bag_mars()`) 

:::{.panel-tabset}

## `earth` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-earth-bag-mars-regression-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-earth-bag-mars-regression
bag_mars_spec <- bag_mars() |>
  # We need to set the mode since this engine works with multiple modes
  # and earth is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-earth-bag-mars-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(147)
bag_mars_fit <- bag_mars_spec |> fit(strength ~ ., data = reg_train)
bag_mars_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-earth-bag-mars-regression
predict(bag_mars_fit, new_data = reg_test)
```

:::

## Bagged Neural Networks (`bag_mlp()`) 

:::{.panel-tabset}

## `nnet` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-nnet-bag-mlp-regression-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-nnet-bag-mlp-regression
bag_mlp_spec <- bag_mlp() |>
  # We need to set the mode since this engine works with multiple modes
  # and nnet is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-nnet-bag-mlp-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(324)
bag_mlp_fit <- bag_mlp_spec |> fit(strength ~ ., data = reg_train)
bag_mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-nnet-bag-mlp-regression
predict(bag_mlp_fit, new_data = reg_test)
```

:::

## Bagged Decision Trees (`bag_tree()`) 

:::{.panel-tabset}

## `rpart` 

This engine requires the baguette extension package, so let's load this first:

```{r}
#| label: load-rpart-bag-tree-regression-baguette
#| output: false
library(baguette)
```

We create a model specification via:

```{r}
#| label: spec-rpart-bag-tree-regression
bag_tree_spec <- bag_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and rpart is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-rpart-bag-tree-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(230)
bag_tree_fit <- bag_tree_spec |> fit(strength ~ ., data = reg_train)
bag_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-rpart-bag-tree-regression
predict(bag_tree_fit, new_data = reg_test)
```

:::

## Bayesian Additive Regression Trees (`bart()`) 

:::{.panel-tabset}

## `dbarts` 

We create a model specification via:

```{r}
#| label: spec-dbarts-bart-regression
bart_spec <- bart() |>
  # We need to set the mode since this engine works with multiple modes
  # and dbarts is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-dbarts-bart-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(134)
bart_fit <- bart_spec |> fit(strength ~ ., data = reg_train)
bart_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-dbarts-bart-regression
predict(bart_fit, new_data = reg_test)
predict(bart_fit, type = "conf_int", new_data = reg_test)
predict(bart_fit, type = "pred_int", new_data = reg_test)
```

:::

## Boosted Decision Trees (`boost_tree()`) 

:::{.panel-tabset}

## `xgboost` 

We create a model specification via:

```{r}
#| label: spec-xgboost-boost-tree-regression
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and xgboost is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-xgboost-boost-tree-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(748)
boost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-xgboost-boost-tree-regression
predict(boost_tree_fit, new_data = reg_test)
```

## `catboost` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-catboost-boost-tree-regression-bonsai
#| output: false
#| eval: !expr 'run_catboost'
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-catboost-boost-tree-regression
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("catboost")
```

Now we create the model fit object:

```{r}
#| label: fit-catboost-boost-tree-regression
#| eval: !expr 'run_catboost'
# Set the random number seed to an integer for reproducibility: 
set.seed(557)
boost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-catboost-boost-tree-regression
#| eval: !expr 'run_catboost'
predict(boost_tree_fit, new_data = reg_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-boost-tree-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-boost-tree-regression
#| eval: !expr 'run_h2o'
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("h2o_gbm")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-boost-tree-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(720)
boost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-boost-tree-regression
predict(boost_tree_fit, new_data = reg_test)
```

## `h2o_gbm` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-gbm-boost-tree-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-gbm-boost-tree-regression
#| eval: !expr 'run_h2o'
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("h2o_gbm")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-gbm-boost-tree-regression
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(90)
boost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-gbm-boost-tree-regression
predict(boost_tree_fit, new_data = reg_test)
```

## `lightgbm` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-lightgbm-boost-tree-regression-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-lightgbm-boost-tree-regression
boost_tree_spec <- boost_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("lightgbm")
```

Now we create the model fit object:

```{r}
#| label: fit-lightgbm-boost-tree-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(570)
boost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-lightgbm-boost-tree-regression
predict(boost_tree_fit, new_data = reg_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-boost-tree-regression
#| eval: !expr 'run_spark'
boost_tree_spec <- boost_tree() |>
  set_mode("regression") |>
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-boost-tree-regression
#| eval: !expr 'run_spark'
# Set the random number seed to an integer for reproducibility: 
set.seed(620)
boost_tree_fit <- boost_tree_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-boost-tree-regression
#| eval: !expr 'run_spark'
predict(boost_tree_fit, new_data = tbl_reg$test)
```

:::

## Cubist Rules (`cubist_rules()`) 

:::{.panel-tabset}

## `Cubist` 

This engine requires the rules extension package, so let's load this first:

```{r}
#| label: load-Cubist-cubist-rules-regression-rules
#| output: false
library(rules)
```

We create a model specification via:

```{r}
#| label: spec-Cubist-cubist-rules-regression
# This engine works with a single mode so no need to set that
# and Cubist is the default engine so there is no need to set that either.
cubist_rules_spec <- cubist_rules()
```

Now we create the model fit object:

```{r}
#| label: fit-Cubist-cubist-rules-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(188)
cubist_rules_fit <- cubist_rules_spec |> fit(strength ~ ., data = reg_train)
cubist_rules_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-Cubist-cubist-rules-regression
predict(cubist_rules_fit, new_data = reg_test)
```

:::

## Decision Tree (`decision_tree()`) 

:::{.panel-tabset}

## `rpart` 

We create a model specification via:

```{r}
#| label: spec-rpart-decision-tree-regression
decision_tree_spec <- decision_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and rpart is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-rpart-decision-tree-regression
decision_tree_fit <- decision_tree_spec |> fit(strength ~ ., data = reg_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-rpart-decision-tree-regression
predict(decision_tree_fit, new_data = reg_test)
```

## `partykit` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-partykit-decision-tree-regression-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-partykit-decision-tree-regression
decision_tree_spec <- decision_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("partykit")
```

Now we create the model fit object:

```{r}
#| label: fit-partykit-decision-tree-regression
decision_tree_fit <- decision_tree_spec |> fit(strength ~ ., data = reg_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-partykit-decision-tree-regression
predict(decision_tree_fit, new_data = reg_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-decision-tree-regression
#| eval: !expr 'run_spark'
decision_tree_spec <- decision_tree() |>
  set_mode("regression") |> 
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-decision-tree-regression
decision_tree_fit <- decision_tree_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-decision-tree-regression
predict(decision_tree_fit, new_data = tbl_reg$test)
```

:::

## Generalized Additive Models (`gen_additive_mod()`) 

:::{.panel-tabset}

## `mgcv` 

We create a model specification via:

```{r}
#| label: spec-mgcv-gen-additive-mod-regression
gen_additive_mod_spec <- gen_additive_mod() |>
  # We need to set the mode since this engine works with multiple modes
  # and mgcv is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-mgcv-gen-additive-mod-regression
gen_additive_mod_fit <- 
  gen_additive_mod_spec |> 
  fit(strength ~ s(age) + s(cement), data = reg_train)
gen_additive_mod_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-mgcv-gen-additive-mod-regression
predict(gen_additive_mod_fit, new_data = reg_test)
predict(gen_additive_mod_fit, type = "conf_int", new_data = reg_test)
```

:::

## Linear Reg (`linear_reg()`) 

:::{.panel-tabset}

## `lm` 

We create a model specification via:

```{r}
#| label: spec-lm-linear-reg-regression
# This engine works with a single mode so no need to set that
# and lm is the default engine so there is no need to set that either.
linear_reg_spec <- linear_reg()
```

Now we create the model fit object:

```{r}
#| label: fit-lm-linear-reg-regression
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-lm-linear-reg-regression
predict(linear_reg_fit, new_data = reg_test)
predict(linear_reg_fit, type = "conf_int", new_data = reg_test)
predict(linear_reg_fit, type = "pred_int", new_data = reg_test)
```

## `brulee` 

We create a model specification via:

```{r}
#| label: spec-brulee-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("brulee")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-linear-reg-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(1)
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-linear-reg-regression
predict(linear_reg_fit, new_data = reg_test)
```

## `gee` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-gee-linear-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-gee-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("gee")
```

Now we create the model fit object:

```{r}
#| label: fit-gee-linear-reg-regression
linear_reg_fit <- 
  linear_reg_spec |> 
  fit(weight ~ Time + Diet + id_var(Rat), data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-gee-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
```

## `glm` 

We create a model specification via:

```{r}
#| label: spec-glm-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("glm")
```

Now we create the model fit object:

```{r}
#| label: fit-glm-linear-reg-regression
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glm-linear-reg-regression
predict(linear_reg_fit, new_data = reg_test)
predict(linear_reg_fit, type = "conf_int", new_data = reg_test)
```

## `glmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-glmer-linear-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-glmer-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmer")
```

Now we create the model fit object:

```{r}
#| label: fit-glmer-linear-reg-regression
linear_reg_fit <- 
  linear_reg_spec |> 
  fit(weight ~ Diet + Time + (1|Rat), data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmer-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
```

## `glmnet` 

We create a model specification via:

```{r}
#| label: spec-glmnet-linear-reg-regression
linear_reg_spec <- linear_reg(penalty = 0.01) |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmnet")
```

Now we create the model fit object:

```{r}
#| label: fit-glmnet-linear-reg-regression
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmnet-linear-reg-regression
predict(linear_reg_fit, new_data = reg_test)
```

## `gls` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-gls-linear-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-gls-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  # Also, nlme::gls() specifies the random effects outside of the formula so
  # we set that as an engine parameter
  set_engine("gls", correlation = nlme::corCompSymm(form = ~Time|Rat))
```

Now we create the model fit object:

```{r}
#| label: fit-gls-linear-reg-regression
linear_reg_fit <- linear_reg_spec |> fit(weight ~ Time + Diet, data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-gls-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-linear-reg-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-linear-reg-regression
#| eval: !expr 'run_h2o'
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-linear-reg-regression
#| eval: !expr 'run_h2o'
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-linear-reg-regression
#| eval: !expr 'run_h2o'
predict(linear_reg_fit, new_data = reg_test)
```

## `keras` 

We create a model specification via:

```{r}
#| label: spec-keras-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("keras")
```

Now we create the model fit object:

```{r}
#| label: fit-keras-linear-reg-regression
#| eval: !expr 'run_keras'
#| results: hide
# Set the random number seed to an integer for reproducibility: 
set.seed(596)
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)
linear_reg_fit
```

```{r}
#| label: t-keras-linear-reg-regression-print
#| eval: !expr 'run_keras'
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-keras-linear-reg-regression
#| eval: !expr 'run_keras'
predict(linear_reg_fit, new_data = reg_test)
```

## `lme` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-lme-linear-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-lme-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that. 
  # nlme::lme() makes us set the random effects outside of the formula so we
  # add it as an engine parameter. 
  set_engine("lme", random = ~ Time | Rat)
```

Now we create the model fit object:

```{r}
#| label: fit-lme-linear-reg-regression
linear_reg_fit <- linear_reg_spec |> fit(weight ~ Diet + Time, data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-lme-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
```

## `lmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-lmer-linear-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-lmer-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("lmer")
```

Now we create the model fit object:

```{r}
#| label: fit-lmer-linear-reg-regression
linear_reg_fit <- 
  linear_reg_spec |> 
  fit(weight ~ Diet + Time + (1|Rat), data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-lmer-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
```

## `stan` 

We create a model specification via:

```{r}
#| label: spec-stan-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("stan")
```

Now we create the model fit object:

```{r}
#| label: fit-stan-linear-reg-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(357)
linear_reg_fit <- linear_reg_spec |> fit(weight ~ Diet + Time, data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-stan-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
predict(linear_reg_fit, type = "conf_int", new_data = reg_group_test)
predict(linear_reg_fit, type = "pred_int", new_data = reg_group_test)
```

## `stan_glmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-stan-glmer-linear-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-stan-glmer-linear-reg-regression
linear_reg_spec <- linear_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("stan_glmer")
```

Now we create the model fit object:

```{r}
#| label: fit-stan-glmer-linear-reg-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(895)
linear_reg_fit <- 
  linear_reg_spec |> 
  fit(weight ~ Diet + Time + (1|Rat), data = reg_group_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-stan-glmer-linear-reg-regression
predict(linear_reg_fit, new_data = reg_group_test)
predict(linear_reg_fit, type = "pred_int", new_data = reg_group_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-linear-reg-regression
#| eval: !expr 'run_spark'
linear_reg_spec <- linear_reg() |> 
  set_engine("spark")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-linear-reg-regression
#| eval: !expr 'run_spark'
linear_reg_fit <- linear_reg_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-linear-reg-regression
#| eval: !expr 'run_spark'
predict(linear_reg_fit, new_data = tbl_reg$test)
```

:::

## Multivariate Adaptive Regression Splines (`mars()`) 

:::{.panel-tabset}

## `earth` 

We create a model specification via:

```{r}
#| label: spec-earth-mars-regression
mars_spec <- mars() |>
  # We need to set the mode since this engine works with multiple modes
  # and earth is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-earth-mars-regression
mars_fit <- mars_spec |> fit(strength ~ ., data = reg_train)
mars_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-earth-mars-regression
predict(mars_fit, new_data = reg_test)
```

:::

## Neural Networks (`mlp()`) 

:::{.panel-tabset}

## `nnet` 

We create a model specification via:

```{r}
#| label: spec-nnet-mlp-regression
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  # and nnet is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-nnet-mlp-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(159)
mlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-nnet-mlp-regression
predict(mlp_fit, new_data = reg_test)
```

## `brulee` 

We create a model specification via:

```{r}
#| label: spec-brulee-mlp-regression
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("brulee")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-mlp-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(407)
mlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-mlp-regression
predict(mlp_fit, new_data = reg_test)
```

## `brulee_two_layer` 

We create a model specification via:

```{r}
#| label: spec-brulee-two-layer-mlp-regression
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("brulee_two_layer")
```

Now we create the model fit object:

```{r}
#| label: fit-brulee-two-layer-mlp-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(585)
mlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-brulee-two-layer-mlp-regression
predict(mlp_fit, new_data = reg_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-mlp-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-mlp-regression
#| eval: !expr 'run_h2o'
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-mlp-regression
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(93)
mlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)
mlp_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-mlp-regression
#| eval: !expr 'run_h2o'
predict(mlp_fit, new_data = reg_test)
```

## `keras` 

We create a model specification via:

```{r}
#| label: spec-keras-mlp-regression
mlp_spec <- mlp() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("keras")
```

Now we create the model fit object:

```{r}
#| label: fit-keras-mlp-regression
#| eval: !expr 'run_keras'
#| results: hide
# Set the random number seed to an integer for reproducibility: 
set.seed(879)
mlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)
```

```{r}
#| label: fit-keras-mlp-regression-print
#| eval: !expr 'run_keras'
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-keras-mlp-regression
#| eval: !expr 'run_keras'
predict(mlp_fit, new_data = reg_test)
```

:::

## K-Nearest Neighbors (`nearest_neighbor()`) 

:::{.panel-tabset}

## `kknn` 

We create a model specification via:

```{r}
#| label: spec-kknn-nearest-neighbor-regression
nearest_neighbor_spec <- nearest_neighbor() |>
  # We need to set the mode since this engine works with multiple modes
  # and kknn is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-kknn-nearest-neighbor-regression
nearest_neighbor_fit <- nearest_neighbor_spec |> fit(strength ~ ., data = reg_train)
nearest_neighbor_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kknn-nearest-neighbor-regression
predict(nearest_neighbor_fit, new_data = reg_test)
```

## Null Model (`null_model()`) 

## `parsnip` 

We create a model specification via:

```{r}
#| label: spec-parsnip-null-model-regression
null_model_spec <- null_model() |>
  # We need to set the mode since this engine works with multiple modes
  # and parsnip is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-parsnip-null-model-regression
null_model_fit <- null_model_spec |> fit(strength ~ ., data = reg_train)
null_model_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-parsnip-null-model-regression
predict(null_model_fit, new_data = reg_test)
```

:::

## Partial Least Squares (`pls()`) 

:::{.panel-tabset}

## `mixOmics` 

This engine requires the plsmod extension package, so let's load this first:

```{r}
#| label: load-mixOmics-pls-regression-plsmod
#| output: false
library(plsmod)
```

We create a model specification via:

```{r}
#| label: spec-mixOmics-pls-regression
pls_spec <- pls() |>
  # We need to set the mode since this engine works with multiple modes
  # and mixOmics is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-mixOmics-pls-regression
pls_fit <- pls_spec |> fit(strength ~ ., data = reg_train)
pls_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-mixOmics-pls-regression
predict(pls_fit, new_data = reg_test)
```

:::

## Poisson Reg (`poisson_reg()`) 

:::{.panel-tabset}

## `glm` 

This engine requires the poissonreg extension package, so let's load this first:

```{r}
#| label: load-glm-poisson-reg-regression-poissonreg
#| output: false
library(poissonreg)
```

We create a model specification via:

```{r}
#| label: spec-glm-poisson-reg-regression
# This engine works with a single mode so no need to set that
# and glm is the default engine so there is no need to set that either.
poisson_reg_spec <- poisson_reg()
```

Now we create the model fit object:

```{r}
#| label: fit-glm-poisson-reg-regression
poisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glm-poisson-reg-regression
predict(poisson_reg_fit, new_data = count_test)
```

## `gee` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-gee-poisson-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-gee-poisson-reg-regression
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("gee")
```

Now we create the model fit object:

```{r}
#| label: fit-gee-poisson-reg-regression
poisson_reg_fit <- 
  poisson_reg_spec |> 
  fit(weight ~ Diet + Time + id_var(Rat), data = reg_group_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-gee-poisson-reg-regression
# Can't reproduce this:
# predict(poisson_reg_fit, new_data = reg_group_test)
```

## `glmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-glmer-poisson-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-glmer-poisson-reg-regression
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmer")
```

Now we create the model fit object:

```{r}
#| label: fit-glmer-poisson-reg-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(826)
poisson_reg_fit <- 
  poisson_reg_spec |> 
  fit(weight ~ Diet + Time + (1|Rat), data = reg_group_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmer-poisson-reg-regression
predict(poisson_reg_fit, new_data = reg_group_test)
```

## `glmnet` 

This engine requires the poissonreg extension package, so let's load this first:

```{r}
#| label: load-glmnet-poisson-reg-regression-poissonreg
#| output: false
library(poissonreg)
```

We create a model specification via:

```{r}
#| label: spec-glmnet-poisson-reg-regression
poisson_reg_spec <- poisson_reg(penalty = 0.01) |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmnet")
```

Now we create the model fit object:

```{r}
#| label: fit-glmnet-poisson-reg-regression
poisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmnet-poisson-reg-regression
predict(poisson_reg_fit, new_data = count_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-poisson-reg-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-poisson-reg-regression
#| eval: !expr 'run_h2o'
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-poisson-reg-regression
#| eval: !expr 'run_h2o'
poisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-poisson-reg-regression
#| eval: !expr 'run_h2o'
predict(poisson_reg_fit, new_data = count_test)
```

## `hurdle` 

This engine requires the poissonreg extension package, so let's load this first:

```{r}
#| label: load-hurdle-poisson-reg-regression-poissonreg
#| output: false
library(poissonreg)
```

We create a model specification via:

```{r}
#| label: spec-hurdle-poisson-reg-regression
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("hurdle")
```

Now we create the model fit object:

```{r}
#| label: fit-hurdle-poisson-reg-regression
poisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-hurdle-poisson-reg-regression
predict(poisson_reg_fit, new_data = count_test)
```

## `stan` 

This engine requires the poissonreg extension package, so let's load this first:

```{r}
#| label: load-stan-poisson-reg-regression-poissonreg
#| output: false
library(poissonreg)
```

We create a model specification via:

```{r}
#| label: spec-stan-poisson-reg-regression
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("stan")
```

Now we create the model fit object:

```{r}
#| label: fit-stan-poisson-reg-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(213)
poisson_reg_fit <- 
  poisson_reg_spec |> 
  fit(weight ~ Diet + Time, data = reg_group_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-stan-poisson-reg-regression
predict(poisson_reg_fit, new_data = reg_group_test)
predict(poisson_reg_fit, type = "conf_int", new_data = reg_group_test)
predict(poisson_reg_fit, type = "pred_int", new_data = reg_group_test)
```

## `stan_glmer` 

This engine requires the multilevelmod extension package, so let's load this first:

```{r}
#| label: load-stan-glmer-poisson-reg-regression-multilevelmod
#| output: false
library(multilevelmod)
```

We create a model specification via:

```{r}
#| label: spec-stan-glmer-poisson-reg-regression
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("stan_glmer")
```

Now we create the model fit object:

```{r}
#| label: fit-stan-glmer-poisson-reg-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(690)
poisson_reg_fit <- 
  poisson_reg_spec |> 
  fit(weight ~ Diet + Time + (1|Rat), data = reg_group_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-stan-glmer-poisson-reg-regression
predict(poisson_reg_fit, new_data = reg_group_test)
predict(poisson_reg_fit, type = "pred_int", new_data = reg_group_test)
```

## `zeroinfl` 

This engine requires the poissonreg extension package, so let's load this first:

```{r}
#| label: load-zeroinfl-poisson-reg-regression-poissonreg
#| output: false
library(poissonreg)
```

We create a model specification via:

```{r}
#| label: spec-zeroinfl-poisson-reg-regression
poisson_reg_spec <- poisson_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("zeroinfl")
```

Now we create the model fit object:

```{r}
#| label: fit-zeroinfl-poisson-reg-regression
poisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)
poisson_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-zeroinfl-poisson-reg-regression
predict(poisson_reg_fit, new_data = count_test)
```

:::

## Random Forests (`rand_forest()`) 

:::{.panel-tabset}

## `ranger` 

We create a model specification via:

```{r}
#| label: spec-ranger-rand-forest-regression
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  # and ranger is the default engine so there is no need to set that either.
  set_engine("ranger", keep.inbag = TRUE) |> 
  # However, we'll set the engine and use the keep.inbag=TRUE option so that we 
  # can produce interval predictions. This is not generally required. 
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-ranger-rand-forest-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(860)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-ranger-rand-forest-regression
predict(rand_forest_fit, new_data = reg_test)
predict(rand_forest_fit, type = "conf_int", new_data = reg_test)
```

## `aorsf` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-aorsf-rand-forest-regression-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-aorsf-rand-forest-regression
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("aorsf")
```

Now we create the model fit object:

```{r}
#| label: fit-aorsf-rand-forest-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(47)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-aorsf-rand-forest-regression
predict(rand_forest_fit, new_data = reg_test)
```

## `grf` 

We create a model specification via:

```{r}
#| label: spec-grf-rand-forest-regression
#| eval: !expr 'run_grf'
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("grf")
```

Now we create the model fit object:

```{r}
#| label: fit-grf-rand-forest-regression
#| eval: !expr 'run_grf'
# Set the random number seed to an integer for reproducibility: 
set.seed(130)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-grf-rand-forest-regression
#| eval: !expr 'run_grf'
predict(rand_forest_fit, new_data = reg_test)
predict(rand_forest_fit, type = "conf_int", new_data = reg_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-rand-forest-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-rand-forest-regression
#| eval: !expr 'run_h2o'
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-rand-forest-regression
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(211)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-rand-forest-regression
#| eval: !expr 'run_h2o'
predict(rand_forest_fit, new_data = reg_test)
```

## `partykit` 

This engine requires the bonsai extension package, so let's load this first:

```{r}
#| label: load-partykit-rand-forest-regression-bonsai
#| output: false
library(bonsai)
```

We create a model specification via:

```{r}
#| label: spec-partykit-rand-forest-regression
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("partykit")
```

Now we create the model fit object:

```{r}
#| label: fit-partykit-rand-forest-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(981)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)
```

The print method has a lot of output: 

<details>
```{r}
#| label: fit-partykit-rand-forest-regression-print
capture.output(print(rand_forest_fit))[1:100] |> cat(sep = "\n")
```
</details>

The holdout data can be predicted:

```{r}
#| label: predict-partykit-rand-forest-regression
predict(rand_forest_fit, new_data = reg_test)
```

## `randomForest` 

We create a model specification via:

```{r}
#| label: spec-randomForest-rand-forest-regression
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("randomForest")
```

Now we create the model fit object:

```{r}
#| label: fit-randomForest-rand-forest-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(793)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-randomForest-rand-forest-regression
predict(rand_forest_fit, new_data = reg_test)
```

## `spark` 

We create a model specification via:

```{r}
#| label: spec-spark-rand-forest-regression
#| eval: !expr 'run_spark'
rand_forest_spec <- rand_forest() |>
  set_engine("spark") |> 
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-spark-rand-forest-regression
#| eval: !expr 'run_spark'
# Set the random number seed to an integer for reproducibility: 
set.seed(157)
rand_forest_fit <- rand_forest_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-spark-rand-forest-regression
#| eval: !expr 'run_spark'
predict(rand_forest_fit, new_data = tbl_reg$test)
```

:::

## Rule Fit (`rule_fit()`) 

:::{.panel-tabset}

## `xrf` 

This engine requires the rules extension package, so let's load this first:

```{r}
#| label: load-xrf-rule-fit-regression-rules
#| output: false
library(rules)
```

We create a model specification via:

```{r}
#| label: spec-xrf-rule-fit-regression
rule_fit_spec <- rule_fit() |>
  # We need to set the mode since this engine works with multiple modes
  # and xrf is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-xrf-rule-fit-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(431)
rule_fit_fit <- rule_fit_spec |> fit(strength ~ ., data = reg_train)
rule_fit_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-xrf-rule-fit-regression
predict(rule_fit_fit, new_data = reg_test)
```

## `h2o` 

This engine requires the agua extension package, so let's load this first:

```{r}
#| label: load-h2o-rule-fit-regression-agua
#| output: false
library(agua)
```

We create a model specification via:

```{r}
#| label: spec-h2o-rule-fit-regression
#| eval: !expr 'run_h2o'
rule_fit_spec <- rule_fit() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("h2o")
```

Now we create the model fit object:

```{r}
#| label: fit-h2o-rule-fit-regression
#| eval: !expr 'run_h2o'
# Set the random number seed to an integer for reproducibility: 
set.seed(236)
rule_fit_fit <- rule_fit_spec |> fit(strength ~ ., data = reg_train)
rule_fit_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-h2o-rule-fit-regression
#| eval: !expr 'run_h2o'
predict(rule_fit_fit, new_data = reg_test)
```

:::

## Support Vector Machine (Linear Kernel) (`svm_linear()`) 

:::{.panel-tabset}

## `kernlab` 

We create a model specification via:

```{r}
#| label: spec-kernlab-svm-linear-regression
svm_linear_spec <- svm_linear() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("regression") |>
  set_engine("kernlab")
```

Now we create the model fit object:

```{r}
#| label: fit-kernlab-svm-linear-regression
svm_linear_fit <- svm_linear_spec |> fit(strength ~ ., data = reg_train)
svm_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kernlab-svm-linear-regression
predict(svm_linear_fit, new_data = reg_test)
```

## `LiblineaR` 

We create a model specification via:

```{r}
#| label: spec-LiblineaR-svm-linear-regression
svm_linear_spec <- svm_linear() |>
  # We need to set the mode since this engine works with multiple modes
  # and LiblineaR is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-LiblineaR-svm-linear-regression
svm_linear_fit <- svm_linear_spec |> fit(strength ~ ., data = reg_train)
svm_linear_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-LiblineaR-svm-linear-regression
predict(svm_linear_fit, new_data = reg_test)
```

:::

## Support Vector Machine (Polynomial Kernel) (`svm_poly()`) 

:::{.panel-tabset}

## `kernlab` 

We create a model specification via:

```{r}
#| label: spec-kernlab-svm-poly-regression
svm_poly_spec <- svm_poly() |>
  # We need to set the mode since this engine works with multiple modes
  # and kernlab is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-kernlab-svm-poly-regression
svm_poly_fit <- svm_poly_spec |> fit(strength ~ ., data = reg_train)
svm_poly_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kernlab-svm-poly-regression
predict(svm_poly_fit, new_data = reg_test)
```

:::

## Support Vector Machine (Radial Basis Function Kernel) (`svm_rbf()`) 

:::{.panel-tabset}

## `kernlab` 

We create a model specification via:

```{r}
#| label: spec-kernlab-svm-rbf-regression
svm_rbf_spec <- svm_rbf() |>
  # We need to set the mode since this engine works with multiple modes
  # and kernlab is the default engine so there is no need to set that either.
  set_mode("regression")
```

Now we create the model fit object:

```{r}
#| label: fit-kernlab-svm-rbf-regression
svm_rbf_fit <- svm_rbf_spec |> fit(strength ~ ., data = reg_train)
svm_rbf_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-kernlab-svm-rbf-regression
predict(svm_rbf_fit, new_data = reg_test)
```


:::

# Censored Regression Models

Let's simulate a data set using the prodlim and survival packages: 

```{r}
#| label: cns-data
library(survival)
library(prodlim)

set.seed(1000)
cns_data <- 
  SimSurv(250) |> 
  mutate(event_time = Surv(time, event)) |> 
  select(event_time, X1, X2)

cns_split <- initial_split(cns_data, prop = 0.98)
cns_split
cns_train <- training(cns_split)
cns_test <- testing(cns_split)
```

For some types of predictions, we need the _evaluation time(s)_ for the predictions. We'll use these three times to demonstrate: 

```{r}
#| label: eval-times
eval_times <- c(1, 3, 5)
```


## Bagged Decision Trees (`bag_tree()`) 

:::{.panel-tabset}

## `rpart` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-rpart-bag-tree-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-rpart-bag-tree-censored-regression
bag_tree_spec <- bag_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and rpart is the default engine so there is no need to set that either.
  set_mode("censored regression")
```

Now we create the model fit object:

```{r}
#| label: fit-rpart-bag-tree-censored-regression
bag_tree_fit <- bag_tree_spec |> fit(event_time ~ ., data = cns_train)
bag_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-rpart-bag-tree-censored-regression
predict(bag_tree_fit, type = "time", new_data = cns_test)
predict(bag_tree_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-rpart-bag-tree-censored-regression-slice
bag_tree_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

:::

## Boosted Decision Trees (`boost_tree()`) 

:::{.panel-tabset}

## `mboost` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-mboost-boost-tree-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-mboost-boost-tree-censored-regression
boost_tree_spec <- boost_tree() |> 
  set_mode("censored regression") |> 
  set_engine("mboost")
```

Now we create the model fit object:

```{r}
#| label: fit-mboost-boost-tree-censored-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(852)
boost_tree_fit <- boost_tree_spec |> fit(event_time ~ ., data = cns_train)
boost_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-mboost-boost-tree-censored-regression
predict(boost_tree_fit, type = "time", new_data = cns_test)
predict(boost_tree_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
predict(boost_tree_fit, type = "linear_pred", new_data = cns_test)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-mboost-boost-tree-censored-regression-slice
boost_tree_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

:::

## Decision Tree (`decision_tree()`) 

:::{.panel-tabset}

## `rpart` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-rpart-decision-tree-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-rpart-decision-tree-censored-regression
decision_tree_spec <- decision_tree() |>
  # We need to set the mode since this engine works with multiple modes
  # and rpart is the default engine so there is no need to set that either.
  set_mode("censored regression")
```

Now we create the model fit object:

```{r}
#| label: fit-rpart-decision-tree-censored-regression
decision_tree_fit <- decision_tree_spec |> fit(event_time ~ ., data = cns_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-rpart-decision-tree-censored-regression
predict(decision_tree_fit, type = "time", new_data = cns_test)
predict(decision_tree_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-rpart-decision-tree-censored-regression-slice
decision_tree_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

## `partykit` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-partykit-decision-tree-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-partykit-decision-tree-censored-regression
decision_tree_spec <- decision_tree() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("censored regression") |>
  set_engine("partykit")
```

Now we create the model fit object:

```{r}
#| label: fit-partykit-decision-tree-censored-regression
decision_tree_fit <- decision_tree_spec |> fit(event_time ~ ., data = cns_train)
decision_tree_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-partykit-decision-tree-censored-regression
predict(decision_tree_fit, type = "time", new_data = cns_test)
predict(decision_tree_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-partykit-decision-tree-censored-regression-slice
decision_tree_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

:::

## Proportional Hazards (`proportional_hazards()`) 

:::{.panel-tabset}

## `survival` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-survival-proportional-hazards-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-survival-proportional-hazards-censored-regression
# This engine works with a single mode so no need to set that
# and survival is the default engine so there is no need to set that either.
proportional_hazards_spec <- proportional_hazards()
```

Now we create the model fit object:

```{r}
#| label: fit-survival-proportional-hazards-censored-regression
proportional_hazards_fit <- proportional_hazards_spec |> fit(event_time ~ ., data = cns_train)
proportional_hazards_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-survival-proportional-hazards-censored-regression
predict(proportional_hazards_fit, type = "time", new_data = cns_test)
predict(proportional_hazards_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
predict(proportional_hazards_fit, type = "linear_pred", new_data = cns_test)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-survival-proportional-hazards-censored-regression-slice
proportional_hazards_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

## `glmnet` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-glmnet-proportional-hazards-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-glmnet-proportional-hazards-censored-regression
proportional_hazards_spec <- proportional_hazards(penalty = 0.01) |> 
  # This engine works with a single mode so no need to set that
  set_engine("glmnet")
```

Now we create the model fit object:

```{r}
#| label: fit-glmnet-proportional-hazards-censored-regression
proportional_hazards_fit <- proportional_hazards_spec |> fit(event_time ~ ., data = cns_train)
proportional_hazards_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-glmnet-proportional-hazards-censored-regression
predict(proportional_hazards_fit, type = "time", new_data = cns_test)
predict(proportional_hazards_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
predict(proportional_hazards_fit, type = "linear_pred", new_data = cns_test)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-glmnet-proportional-hazards-censored-regression-slice
proportional_hazards_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

:::

## Random Forests (`rand_forest()`) 

:::{.panel-tabset}

## `aorsf` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-aorsf-rand-forest-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-aorsf-rand-forest-censored-regression
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("censored regression") |>
  set_engine("aorsf")
```

Now we create the model fit object:

```{r}
#| label: fit-aorsf-rand-forest-censored-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(2)
rand_forest_fit <- rand_forest_spec |> fit(event_time ~ ., data = cns_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-aorsf-rand-forest-censored-regression
predict(rand_forest_fit, type = "time", new_data = cns_test)
predict(rand_forest_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-aorsf-rand-forest-censored-regression-slice
rand_forest_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

## `partykit` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-partykit-rand-forest-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-partykit-rand-forest-censored-regression
rand_forest_spec <- rand_forest() |>
  # We need to set the mode since this engine works with multiple modes
  set_mode("censored regression") |>
  set_engine("partykit")
```

Now we create the model fit object:

```{r}
#| label: fit-partykit-rand-forest-censored-regression
# Set the random number seed to an integer for reproducibility: 
set.seed(89)
rand_forest_fit <- rand_forest_spec |> fit(event_time ~ ., data = cns_train)
```

The print method has a lot of output: 

<details>
```{r}
#| label: fit-partykit-rand-forest-censored-regression-print
capture.output(print(rand_forest_fit))[1:100] |> cat(sep = "\n")
```
</details>

The holdout data can be predicted:

```{r}
#| label: predict-partykit-rand-forest-censored-regression
predict(rand_forest_fit, type = "time", new_data = cns_test)
predict(rand_forest_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-partykit-rand-forest-censored-regression-slice
rand_forest_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

:::

## Parametric Survival Models (`survival_reg()`) 

:::{.panel-tabset}

## `survival` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-survival-survival-reg-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-survival-survival-reg-censored-regression
# This engine works with a single mode so no need to set that
# and survival is the default engine so there is no need to set that either.
survival_reg_spec <- survival_reg()
```

Now we create the model fit object:

```{r}
#| label: fit-survival-survival-reg-censored-regression
survival_reg_fit <- survival_reg_spec |> fit(event_time ~ ., data = cns_train)
survival_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-survival-survival-reg-censored-regression
predict(survival_reg_fit, type = "time", new_data = cns_test)
predict(survival_reg_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
predict(survival_reg_fit, type = "hazard", new_data = cns_test, eval_time = eval_times)
predict(survival_reg_fit, type = "linear_pred", new_data = cns_test)
predict(survival_reg_fit, type = "quantile", new_data = cns_test)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-survival-survival-reg-censored-regression-slice
survival_reg_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

## `flexsurv` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-flexsurv-survival-reg-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-flexsurv-survival-reg-censored-regression
survival_reg_spec <- survival_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("flexsurv")
```

Now we create the model fit object:

```{r}
#| label: fit-flexsurv-survival-reg-censored-regression
survival_reg_fit <- survival_reg_spec |> fit(event_time ~ ., data = cns_train)
survival_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-flexsurv-survival-reg-censored-regression
predict(survival_reg_fit, type = "time", new_data = cns_test)
predict(survival_reg_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
predict(survival_reg_fit, type = "hazard", new_data = cns_test, eval_time = eval_times)
predict(survival_reg_fit, type = "linear_pred", new_data = cns_test)
predict(survival_reg_fit, type = "quantile", new_data = cns_test)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-flexsurv-survival-reg-censored-regression-slice
survival_reg_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

## `flexsurvspline` 

This engine requires the censored extension package, so let's load this first:

```{r}
#| label: load-flexsurvspline-survival-reg-censored-regression-censored
#| output: false
library(censored)
```

We create a model specification via:

```{r}
#| label: spec-flexsurvspline-survival-reg-censored-regression
survival_reg_spec <- survival_reg() |> 
  # This engine works with a single mode so no need to set that
  set_engine("flexsurvspline")
```

Now we create the model fit object:

```{r}
#| label: fit-flexsurvspline-survival-reg-censored-regression
survival_reg_fit <- survival_reg_spec |> fit(event_time ~ ., data = cns_train)
survival_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-flexsurvspline-survival-reg-censored-regression
predict(survival_reg_fit, type = "time", new_data = cns_test)
predict(survival_reg_fit, type = "survival", new_data = cns_test, eval_time = eval_times)
predict(survival_reg_fit, type = "hazard", new_data = cns_test, eval_time = eval_times)
predict(survival_reg_fit, type = "linear_pred", new_data = cns_test)
predict(survival_reg_fit, type = "quantile", new_data = cns_test)
```

Each row of the survival predictions has results for each evaluation time: 

```{r}
#| label: predict-flexsurvspline-survival-reg-censored-regression-slice
survival_reg_fit |> 
  predict(type = "survival", new_data = cns_test, eval_time = eval_times) |> 
  slice(1) |> 
  pluck(".pred")
```

:::

# Quantile Regression Models

To demonstrate quantile regression, let's make a larger version of our regression data: 

```{r}
#| label: qnt-data
set.seed(938)
qnt_split <-
  modeldata::concrete |> 
  slice_sample(n = 100) |> 
  select(strength = compressive_strength, cement, age) |> 
  initial_split(prop = 0.95, strata = strength)
qnt_split

qnt_rec <- 
  recipe(strength ~ ., data = training(qnt_split)) |> 
  step_normalize(all_numeric_predictors()) |> 
  prep()

qnt_train <- bake(qnt_rec, new_data = NULL)
qnt_test <- bake(qnt_rec, new_data = testing(qnt_split))
```

We'll also predict these quantile levels: 

```{r}
#| label: qnt-lvls
qnt_lvls <- (1:3) / 4
```

## Linear Regression (`linear_reg()`) 

:::{.panel-tabset}

## `quantreg` 

We create a model specification via:

```{r}
#| label: spec-quantreg-linear-reg-quantile-regression
linear_reg_spec <- linear_reg() |> 
  set_engine("quantreg") |> 
  set_mode("quantile regression", quantile_levels = qnt_lvls)
```

Now we create the model fit object:

```{r}
#| label: fit-quantreg-linear-reg-quantile-regression
linear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = qnt_train)
linear_reg_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-quantreg-linear-reg-quantile-regression
predict(linear_reg_fit, type = "quantile", new_data = qnt_test)
```

Each row of predictions has a special vector class containing all of the quantile predictions: 

```{r}
#| label: predict-quantreg-linear-reg-quantile-regression-expand
linear_reg_fit |> 
  predict(type = "quantile", new_data = qnt_test)|> 
  slice(1) |> 
  pluck(".pred_quantile") |> 
  # Expand the results for each quantile level by converting to a tibble
  as_tibble()
```

:::

## Random Forests (`rand_forest()`) 

:::{.panel-tabset}

## `grf` 

We create a model specification via:

```{r}
#| label: spec-grf-rand-forest-quantile-regression
#| eval: !expr 'run_grf'
rand_forest_spec <- rand_forest() |>
  set_engine("grf") |> 
  set_mode("quantile regression", quantile_levels = qnt_lvls)
```

Now we create the model fit object:

```{r}
#| label: fit-grf-rand-forest-quantile-regression
#| eval: !expr 'run_grf'
# Set the random number seed to an integer for reproducibility: 
set.seed(435)
rand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = qnt_train)
rand_forest_fit
```

The holdout data can be predicted:

```{r}
#| label: predict-grf-rand-forest-quantile-regression
#| eval: !expr 'run_grf'
predict(rand_forest_fit, type = "quantile", new_data = qnt_test)
```

Each row of predictions has a special vector class containing all of the quantile predictions: 

```{r}
#| label: predict-grf-rand-forest-quantile-regression-expand
#| eval: !expr 'run_grf'
rand_forest_fit |> 
  predict(type = "quantile", new_data = qnt_test)|> 
  slice(1) |> 
  pluck(".pred_quantile") |> 
  # Expand the results for each quantile level by converting to a tibble
  as_tibble()
```

:::

```{r}
#| label: spark-disconnect
#| include: false
#| eval: !expr 'run_spark'
spark_disconnect(sc)
```

