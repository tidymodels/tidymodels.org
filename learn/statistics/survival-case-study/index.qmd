---
title: "How long until building complaints are dispositioned? A survival analysis case study"
categories:
  - statistical analysis
  - survival analysis
type: learn-subsection
weight: 9
description: | 
  Learn how to use tidymodels for survival analysis.
toc: true
toc-depth: 2
include-after-body: ../../../resources.html
---

```{r}
#| label: "setup"
#| include: false
#| message: false
#| warning: false
source(here::here("common.R"))
```

```{r}
#| label: "load"
#| include: false
library(tidymodels)
library(sessioninfo)
library(leaflet)
pkgs <- c("tidymodels", "censored", "modeldatatoo", "glmnet", "aorsf")
theme_set(theme_bw() + theme(legend.position = "top"))
```

## Introduction

`r article_req_pkgs(pkgs)`

Survival analysis is a field of statistics and machine learning for analyzing the time to an event. While it has its roots in medical research, the event of interest can be anything from customer churn to machine failure. Methods from survival analysis take into account that some observations may not yet have experienced the event of interest and are thus _censored_. 

Here we want to predict the time it takes for a complaint to be dispositioned^[In this context, the term _disposition_ means that there has been a decision or resolution regarding the complaint that is the conclusion of the process.] by the Department of Buildings in New York City. We are going to walk through a complete analysis from beginning to end, showing how to analyze time-to-event data.

Let's start with loading the tidymodels and censored packages (the parsnip extension package for survival analysis models).

```{r}
library(tidymodels)
library(censored)
```

## The buildings complaints data

The city of New York publishes data on the [complaints](https://data.cityofnewyork.us/Housing-Development/DOB-Complaints-Received/eabe-havv/about_data) received by the Department of Buildings. The data includes information on the type of complaint, the date it was entered in their records, the date it was dispositioned, and the location of the building the complaint was about. We are using a subset of the data, available in the modeldatatoo package.

```{r}
building_complaints <- modeldatatoo::data_building_complaints()
glimpse(building_complaints)
```

Before we dive into survival analysis, let's get a impression of how the complaints are distributed across the city. We have complaints in all five boroughs, albeit with a somewhat lower density of complaints in Staten Island.

```{r nyc-map}
#| out.width: "100%"
#| echo: false
#| fig.cap: "Building complaints in New York City (closed complaints in purple, active complaints in pink)."

building_complaints %>% 
  mutate(status = if_else(status == "CLOSED", "#7570B3", "#E7298A")) %>%
  leaflet() %>%
  addProviderTiles("CartoDB.Positron") %>%  
  addCircles(
    lng = ~ longitude,
    lat = ~ latitude,
    fill = TRUE,
    opacity = .01,
    fillOpacity = 1,
    color = ~ status,
    popup = ~ paste("Days to disposition:", days_to_disposition)
  )
```

In the dataset, we can see the `days_to_disposition` as well as the `status` of the complaint. For a complaint with the status `"ACTIVE"`, the time to disposition is censored, meaning we do know that it has taken at least that long, but not how long for it to be completely resolved. 

The standard form for time-to-event data are `Surv` objects which capture the time as well as the event status. As with all transformations of the response, it is advisable to do this before heading into the model fitting process with tidymodels.

```{r}
building_complaints <- building_complaints %>% 
  mutate(
    disposition_surv = Surv(days_to_disposition, status == "CLOSED"), 
    .keep = "unused"
  )
```

## Data splitting and resampling

For our resampling strategy, let's use a [3-way split](https://www.tmwr.org/resampling#validation) into training, validation, and test set.

```{r}
set.seed(403)
complaints_split <- initial_validation_split(building_complaints)
```

First, let's pull out the training data and have a brief look at the response using a [Kaplan-Meier curve](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059453/). 

```{r}
#| fig.alt: "A Kaplan-Meier curve dropping rapidly initially, then reaching about 10% survival rate at around 100 days, and finally trailing off until about 400 days."

complaints_train <- training(complaints_split)

survfit(disposition_surv ~ 1, data = complaints_train) %>% plot()
```

We can see that the majority of complaints is dispositioned relatively quickly, but some complaints are still active after 100 days.

## A first model

The censored package includes parametric, semi-parametric, and tree-based models for this type of analysis. To start, we are fitting a parametric survival model with the default of assuming a Weibull distribution on the time to disposition. We'll explore the more flexible models once we have a sense of how well this more restrictive model performs on this dataset.

```{r}
survreg_spec <- survival_reg() %>% 
  set_engine("survival") %>% 
  set_mode("censored regression")
```

We have several missing values in `complaint_priority` that we are turning into a separate category, `"unknown"`. We are also combining the less common categories for `community_board` and `unit` into an `"other"` category to reduce the number of levels in the predictors. The complaint category often does not tell us much more than the unit, with several complaint categories being handled by a specific unit only. This can lead to the model being unable to estimate some of the coefficients. Since our goal here is only to get a rough idea of how well the model performs, we are removing the complaint category for now.

```{r}
rec_other <- recipe(disposition_surv ~ ., data = complaints_train) %>% 
  step_unknown(complaint_priority) %>% 
  step_rm(complaint_category) %>% 
  step_novel(community_board, unit) %>%
  step_other(community_board, unit, threshold = 0.02)
```

We combine the recipe and the model into a workflow. This allows us to easily resample the model because all preprocessing steps are applied to the training set and the validation set for us.

```{r}
survreg_wflow <- workflow() %>% 
  add_recipe(rec_other) %>% 
  add_model(survreg_spec)
```

To fit and evaluate the model, we need the training and validation sets. While we can access them each on their own, `validation_set()` extracts them both, in a manner that emulates a single resample of the data. This enables us to use `fit_resamples()` and other tuning functions in the same way as if we had used some other resampling scheme (such as cross-validation). 

We are calculating several performance metrics: the Brier score, its integrated version, the area under the ROC curve, and the concordance index. Note that all of these are used in a version tailored to survival analysis. The concordance index uses the predicted event time to measure the model’s ability to rank the observations correctly. The Brier score and the ROC curve use the predicted probability of survival at a given time. We evaluate these metrics every 30 days up to 300 days, as provided in the `eval_time` argument. The Brier score is a measure of the accuracy of the predicted probabilities, while the ROC curve is a measure of the model’s ability to discriminate between events and non-events at the given time point. Because these metrics are defined “at a given time,” they are also referred to as *dynamic metrics*.

::: {.callout-tip}
For more information see the [Dynamic Performance Metrics for Event Time Data](../survival-metrics/) article.
:::

```{r}
complaints_rset <- validation_set(complaints_split)

survival_metrics <- metric_set(brier_survival_integrated, brier_survival,
                               roc_auc_survival, concordance_survival)
evaluation_time_points <- seq(0, 300, 30)

set.seed(1)
survreg_res <- fit_resamples(
  survreg_wflow,
  resamples = complaints_rset,
  metrics = survival_metrics,
  eval_time = evaluation_time_points, 
  control = control_resamples(save_pred = TRUE)
)
```

The structure of survival model predictions is slightly different from classification and regression model predictions:

```{r}
preds <- collect_predictions(survreg_res)
preds
```

The predicted survival time is in the `.pred_time` column and the predicted survival probabilities are in the `.pred` list column. 

```{r}
preds$.pred[[6]]
```

For each observation, `.pred` contains a tibble with the evaluation time `.eval_time` and the corresponding survival probability `.pred_survival`. The column `.weight_censored` contains the weights used in the calculation of the dynamic performance metrics. 

::: {.callout-tip}
For details on the weights see the [Accounting for Censoring in Performance Metrics for Event Time Data](../survival-metrics-details/) article.
:::

Of the metrics we calculated with these predictions, let's take a look at the AUC ROC first.

```{r survreg-roc-auc}
collect_metrics(survreg_res) %>% 
  filter(.metric == "roc_auc_survival") %>% 
  ggplot(aes(.eval_time, mean)) + 
  geom_line() + 
  labs(x = "Evaluation Time", y = "Area Under the ROC Curve")
```

We can discriminate between events and non-events reasonably well, especially in the first 30 and 60 days. How about the probabilities that the categorization into event and non-event is based on? 

```{r survreg-brier}
collect_metrics(survreg_res) %>% 
  filter(.metric == "brier_survival") %>% 
  ggplot(aes(.eval_time, mean)) + 
  geom_line() + 
  labs(x = "Evaluation Time", y = "Brier Score")
```

The accuracy of the predicted probabilities is generally good, albeit lowest for evaluation times of 30 and 60 days. The integrated Brier score is a measure of the overall accuracy of the predicted probabilities. 

```{r}
collect_metrics(survreg_res) %>% 
  filter(.metric == "brier_survival_integrated")
```

Which metric to optimise for depends on whether separation or calibration is more important in the modeling problem at hand. We'll go with calibration here. Since we don't have a particular evaluation time that we want to predict well at, we are going to use the integrated Brier score as our main performance metric.

## Try out more models

Lumping factor levels together based on frequencies can lead to a loss of information so let's also try some different approaches. We can let a random forest model group the factor levels via the tree splits. Alternatively, we can turn the factors into dummy variables and use a regularized model to select relevant factor levels.

First, let’s create the recipes for these two approaches:

```{r}
rec_unknown <- recipe(disposition_surv ~ ., data = complaints_train) %>% 
  step_unknown(complaint_priority) 

rec_dummies <- rec_unknown %>% 
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```

Next, let's create the model specifications and tag several hyperparameters for tuning. 
For the random forest, we are using the `"aorsf"` engine for accelerated oblique random survival forests. An oblique tree can split on linear combinations of the predictors, i.e., it provides more flexibility in the splits than a tree which splits on a single predictor.
For the regularized model, we are using the `"glmnet"` engine for a semi-parametric Cox proportional hazards model.

```{r}
oblique_spec <- rand_forest(mtry = tune(), min_n = tune()) %>% 
  set_engine("aorsf") %>% 
  set_mode("censored regression")

oblique_wflow <- workflow() %>% 
  add_recipe(rec_unknown) %>% 
  add_model(oblique_spec)

coxnet_spec <- proportional_hazards(penalty = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("censored regression")

coxnet_wflow <- workflow() %>% 
  add_recipe(rec_dummies) %>% 
  add_model(coxnet_spec)
```

We can tune workflows with any of the `tune_*()` functions such as `tune_grid()` for grid search or `tune_bayes()` for Bayesian optimization. Here we are using grid search for simplicity.

```{r}
set.seed(1)
oblique_res <- tune_grid(
  oblique_wflow,
  resamples = complaints_rset,
  grid = 10,
  metrics = survival_metrics,
  eval_time = evaluation_time_points, 
  control = control_grid(save_workflow = TRUE)
)

set.seed(1)
coxnet_res <- tune_grid(
  coxnet_wflow,
  resamples = complaints_rset,
  grid = 10,
  metrics = survival_metrics,
  eval_time = evaluation_time_points, 
  control = control_grid(save_workflow = TRUE)
)
```

So do any of these models perform better than the parametric survival model?

```{r}
show_best(oblique_res, metric = "brier_survival_integrated", n = 5)

show_best(coxnet_res, metric = "brier_survival_integrated", n = 5)
```

```{r}
#| echo: false
bi_best_cox <- show_best(coxnet_res, metric = "brier_survival_integrated", n = 1) %>% pull(mean)
bi_best_rf <- show_best(oblique_res, metric = "brier_survival_integrated", n = 1) %>% pull(mean)
bi_best_survreg <- show_best(survreg_res, metric = "brier_survival_integrated", n = 1) %>% pull(mean)

if (bi_best_cox <= bi_best_rf || bi_best_survreg <= bi_best_rf) {
  stop("The best model changed.")
}
```

The best regularized Cox model performs a little better than the parametric survival model, with an integrated Brier score of `r round(bi_best_cox, 4)` compared to `r round(bi_best_survreg, 4)` for the parametric model. The random forest performs yet a little better with an integrated Brier score of `r round(bi_best_rf, 4)`.

## The final model

We chose the random forest model as the final model. So let's finalize the workflow by replacing the `tune()` placeholders with the best hyperparameters.

```{r}
param_best <- select_best(oblique_res, metric = "brier_survival_integrated")

last_oblique_wflow <- finalize_workflow(oblique_wflow, param_best)
```

We can now fit the final model on the training data and evaluate it on the test data.

```{r}
set.seed(2)
last_oblique_fit <- last_fit(
  last_oblique_wflow, 
  split = complaints_split,
  metrics = survival_metrics,
  eval_time = evaluation_time_points, 
)

collect_metrics(last_oblique_fit) %>% 
  filter(.metric == "brier_survival_integrated")
```

The Brier score across the different evaluation time points is also very similar between the validation set and the test set.

```{r final-fit-brier}
brier_val <- collect_metrics(oblique_res) %>% 
  filter(.metric == "brier_survival") %>% 
  filter(mtry == param_best$mtry, min_n == param_best$min_n) %>% 
  mutate(Data = "Validation") 
brier_test <- collect_metrics(last_oblique_fit) %>% 
  filter(.metric == "brier_survival") %>% 
  mutate(Data = "Testing") %>% 
  rename(mean = .estimate)
bind_rows(brier_val, brier_test) %>% 
  ggplot(aes(.eval_time, mean, col = Data)) + 
  geom_line() + 
  labs(x = "Evaluation Time", y = "Brier Score")
```

To finish, we can extract the fitted workflow to either predict directly on new data or deploy the model.

```{r}
complaints_model <- extract_workflow(last_oblique_fit)

complaints_5 <- testing(complaints_split) %>% slice(1:5)
predict(complaints_model, new_data = complaints_5, type = "time")
```

For more information on survival analysis with tidymodels see the [`survival analysis` tag](https://www.tidymodels.org/learn/index.html#category=survival%20analysis).

## Session information {#session-info}

```{r}
#| label: "si"
#| echo: false
small_session(pkgs)
```
