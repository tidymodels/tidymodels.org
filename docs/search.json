[
  {
    "objectID": "content/learn/statistics/k-means/index.html",
    "href": "content/learn/statistics/k-means/index.html",
    "title": "K-means clustering with tidy data principles",
    "section": "",
    "text": "This article only requires the tidymodels package.\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions:\n\ntidy()\naugment()\nglance()\n\nLet’s start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\nlibrary(tidymodels)\n\nset.seed(27)\n\ncenters <- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points <- \n  centers %>%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %>% \n  select(-num_points) %>% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n\n\n\n\n\n\n\n\nThis is an ideal case for k-means clustering."
  },
  {
    "objectID": "content/learn/statistics/k-means/index.html#how-does-k-means-work",
    "href": "content/learn/statistics/k-means/index.html#how-does-k-means-work",
    "title": "K-means clustering with tidy data principles",
    "section": "How does K-means work?",
    "text": "How does K-means work?\nRather than using equations, this short animation using the artwork of Allison Horst explains the clustering process:"
  },
  {
    "objectID": "content/learn/statistics/k-means/index.html#clustering-in-r",
    "href": "content/learn/statistics/k-means/index.html#clustering-in-r",
    "title": "K-means clustering with tidy data principles",
    "section": "Clustering in R",
    "text": "Clustering in R\nWe’ll use the built-in kmeans() function, which accepts a data frame with all numeric columns as it’s primary argument.\n\npoints <- \n  labelled_points %>% \n  select(-cluster)\n\nkclust <- kmeans(points, centers = 3)\nkclust\n#> K-means clustering with 3 clusters of sizes 148, 51, 101\n#> \n#> Cluster means:\n#>            x1        x2\n#> 1  0.08853475  1.045461\n#> 2 -3.14292460 -2.000043\n#> 3  5.00401249 -1.045811\n#> \n#> Clustering vector:\n#>   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#>  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#>  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1\n#> [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#> [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#> [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#> [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#> [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#> [297] 2 2 2 2\n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 298.9415 108.8112 243.2092\n#>  (between_SS / total_SS =  82.5 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nsummary(kclust)\n#>              Length Class  Mode   \n#> cluster      300    -none- numeric\n#> centers        6    -none- numeric\n#> totss          1    -none- numeric\n#> withinss       3    -none- numeric\n#> tot.withinss   1    -none- numeric\n#> betweenss      1    -none- numeric\n#> size           3    -none- numeric\n#> iter           1    -none- numeric\n#> ifault         1    -none- numeric\n\nThe output is a list of vectors, where each component has a different length. There’s one of length 300, the same as our original data set. There are two elements of length 3 (withinss and tot.withinss) and centers is a matrix with 3 rows. And then there are the elements of length 1: totss, tot.withinss, betweenss, and iter. (The value ifault indicates possible algorithm problems.)\nThese differing lengths have important meaning when we want to tidy our data set; they signify that each type of component communicates a different kind of information.\n\ncluster (300 values) contains information about each point\ncenters, withinss, and size (3 values) contain information about each cluster\ntotss, tot.withinss, betweenss, and iter (1 value) contain information about the full clustering\n\nWhich of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there’s no straightforward way to combine them), they are extracted by separate functions. augment adds the point classifications to the original data set:\n\naugment(kclust, points)\n#> # A tibble: 300 × 3\n#>       x1     x2 .cluster\n#>    <dbl>  <dbl> <fct>   \n#>  1  6.91 -2.74  3       \n#>  2  6.14 -2.45  3       \n#>  3  4.24 -0.946 3       \n#>  4  3.54  0.287 3       \n#>  5  3.91  0.408 3       \n#>  6  5.30 -1.58  3       \n#>  7  5.01 -1.77  3       \n#>  8  6.16 -1.68  3       \n#>  9  7.13 -2.17  3       \n#> 10  5.24 -2.42  3       \n#> # … with 290 more rows\n\nThe tidy() function summarizes on a per-cluster level:\n\ntidy(kclust)\n#> # A tibble: 3 × 5\n#>        x1    x2  size withinss cluster\n#>     <dbl> <dbl> <int>    <dbl> <fct>  \n#> 1  0.0885  1.05   148     299. 1      \n#> 2 -3.14   -2.00    51     109. 2      \n#> 3  5.00   -1.05   101     243. 3\n\nAnd as it always does, the glance() function extracts a single-row summary:\n\nglance(kclust)\n#> # A tibble: 1 × 4\n#>   totss tot.withinss betweenss  iter\n#>   <dbl>        <dbl>     <dbl> <int>\n#> 1 3724.         651.     3073.     2"
  },
  {
    "objectID": "content/learn/statistics/k-means/index.html#exploratory-clustering",
    "href": "content/learn/statistics/k-means/index.html#exploratory-clustering",
    "title": "K-means clustering with tidy data principles",
    "section": "Exploratory clustering",
    "text": "Exploratory clustering\nWhile these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like dplyr.\nLet’s say we want to explore the effect of different choices of k, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of k, then create columns containing the tidied, glanced and augmented data:\n\nkclusts <- \n  tibble(k = 1:9) %>%\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, points)\n  )\n\nkclusts\n#> # A tibble: 9 × 5\n#>       k kclust   tidied           glanced          augmented         \n#>   <int> <list>   <list>           <list>           <list>            \n#> 1     1 <kmeans> <tibble [1 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 2     2 <kmeans> <tibble [2 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 3     3 <kmeans> <tibble [3 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 4     4 <kmeans> <tibble [4 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 5     5 <kmeans> <tibble [5 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 6     6 <kmeans> <tibble [6 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 7     7 <kmeans> <tibble [7 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 8     8 <kmeans> <tibble [8 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n#> 9     9 <kmeans> <tibble [9 × 5]> <tibble [1 × 4]> <tibble [300 × 3]>\n\nWe can turn these into three separate data sets each representing a different type of data: using tidy(), using augment(), and using glance(). Each of these goes into a separate data set as they represent different types of data.\n\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\n\nNow we can plot the original points using the data from augment(), with each point colored according to the predicted cluster.\n\np1 <- \n  ggplot(assignments, aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\np1\n\n\n\n\n\n\n\n\nAlready we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when k is too high or too low. We can then add the centers of the cluster using the data from tidy():\n\np2 <- p1 + geom_point(data = clusters, size = 10, shape = \"x\")\np2\n\n\n\n\n\n\n\n\nThe data from glance() fills a different but equally important purpose; it lets us view trends of some summary statistics across values of k. Of particular interest is the total within sum of squares, saved in the tot.withinss column.\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nThis represents the variance within the clusters. It decreases as k increases, but notice a bend (or “elbow”) around k = 3. This bend indicates that additional clusters beyond the third have little value. (See here for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output."
  },
  {
    "objectID": "content/learn/statistics/k-means/index.html#session-information",
    "href": "content/learn/statistics/k-means/index.html#session-information",
    "title": "K-means clustering with tidy data principles",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/statistics/bootstrap/index.html",
    "href": "content/learn/statistics/bootstrap/index.html",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "",
    "text": "This article only requires the tidymodels package.\nCombining fitted models in a tidy way is useful for performing bootstrapping or permutation tests. These approaches have been explored before, for instance by Andrew MacDonald here, and Hadley has explored efficient support for bootstrapping as a potential enhancement to dplyr. The tidymodels package broom fits naturally with dplyr in performing these analyses.\nBootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.\nLet’s say we want to fit a nonlinear model to the weight/mileage relationship in the mtcars data set.\n\nlibrary(tidymodels)\n\nggplot(mtcars, aes(mpg, wt)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWe might use the method of nonlinear least squares (via the nls() function) to fit a model.\n\nnlsfit <- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))\nsummary(nlsfit)\n#> \n#> Formula: mpg ~ k/wt + b\n#> \n#> Parameters:\n#>   Estimate Std. Error t value Pr(>|t|)    \n#> k   45.829      4.249  10.786 7.64e-12 ***\n#> b    4.386      1.536   2.855  0.00774 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.774 on 30 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 2.877e-08\n\nggplot(mtcars, aes(wt, mpg)) +\n    geom_point() +\n    geom_line(aes(y = predict(nlsfit)))\n\n\n\n\n\n\n\n\nWhile this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data."
  },
  {
    "objectID": "content/learn/statistics/bootstrap/index.html#bootstrapping-models",
    "href": "content/learn/statistics/bootstrap/index.html#bootstrapping-models",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Bootstrapping models",
    "text": "Bootstrapping models\nWe can use the bootstraps() function in the rsample package to sample bootstrap replications. First, we construct 2000 bootstrap replicates of the data, each of which has been randomly sampled with replacement. The resulting object is an rset, which is a data frame with a column of rsplit objects.\nAn rsplit object has two main components: an analysis data set and an assessment data set, accessible via analysis(rsplit) and assessment(rsplit) respectively. For bootstrap samples, the analysis data set is the bootstrap sample itself, and the assessment data set consists of all the out-of-bag samples.\n\nset.seed(27)\nboots <- bootstraps(mtcars, times = 2000, apparent = TRUE)\nboots\n#> # Bootstrap sampling with apparent sample \n#> # A tibble: 2,001 × 2\n#>    splits          id           \n#>    <list>          <chr>        \n#>  1 <split [32/13]> Bootstrap0001\n#>  2 <split [32/10]> Bootstrap0002\n#>  3 <split [32/13]> Bootstrap0003\n#>  4 <split [32/11]> Bootstrap0004\n#>  5 <split [32/9]>  Bootstrap0005\n#>  6 <split [32/10]> Bootstrap0006\n#>  7 <split [32/11]> Bootstrap0007\n#>  8 <split [32/13]> Bootstrap0008\n#>  9 <split [32/11]> Bootstrap0009\n#> 10 <split [32/11]> Bootstrap0010\n#> # … with 1,991 more rows\n\nLet’s create a helper function to fit an nls() model on each bootstrap sample, and then use purrr::map() to apply this function to all the bootstrap samples at once. Similarly, we create a column of tidy coefficient information by unnesting.\n\nfit_nls_on_bootstrap <- function(split) {\n    nls(mpg ~ k / wt + b, analysis(split), start = list(k = 1, b = 0))\n}\n\nboot_models <-\n  boots %>% \n  mutate(model = map(splits, fit_nls_on_bootstrap),\n         coef_info = map(model, tidy))\n\nboot_coefs <- \n  boot_models %>% \n  unnest(coef_info)\n\nThe unnested coefficient information contains a summary of each replication combined in a single data frame:\n\nboot_coefs\n#> # A tibble: 4,002 × 8\n#>    splits          id            model  term  estimate std.er…¹ stati…²  p.value\n#>    <list>          <chr>         <list> <chr>    <dbl>    <dbl>   <dbl>    <dbl>\n#>  1 <split [32/13]> Bootstrap0001 <nls>  k        42.1      4.05   10.4  1.91e-11\n#>  2 <split [32/13]> Bootstrap0001 <nls>  b         5.39     1.43    3.78 6.93e- 4\n#>  3 <split [32/10]> Bootstrap0002 <nls>  k        49.9      5.66    8.82 7.82e-10\n#>  4 <split [32/10]> Bootstrap0002 <nls>  b         3.73     1.92    1.94 6.13e- 2\n#>  5 <split [32/13]> Bootstrap0003 <nls>  k        37.8      2.68   14.1  9.01e-15\n#>  6 <split [32/13]> Bootstrap0003 <nls>  b         6.73     1.17    5.75 2.78e- 6\n#>  7 <split [32/11]> Bootstrap0004 <nls>  k        45.6      4.45   10.2  2.70e-11\n#>  8 <split [32/11]> Bootstrap0004 <nls>  b         4.75     1.62    2.93 6.38e- 3\n#>  9 <split [32/9]>  Bootstrap0005 <nls>  k        43.6      4.63    9.41 1.85e-10\n#> 10 <split [32/9]>  Bootstrap0005 <nls>  b         5.89     1.68    3.51 1.44e- 3\n#> # … with 3,992 more rows, and abbreviated variable names ¹​std.error, ²​statistic"
  },
  {
    "objectID": "content/learn/statistics/bootstrap/index.html#confidence-intervals",
    "href": "content/learn/statistics/bootstrap/index.html#confidence-intervals",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe can then calculate confidence intervals (using what is called the percentile method):\n\npercentile_intervals <- int_pctl(boot_models, coef_info)\npercentile_intervals\n#> # A tibble: 2 × 6\n#>   term   .lower .estimate .upper .alpha .method   \n#>   <chr>   <dbl>     <dbl>  <dbl>  <dbl> <chr>     \n#> 1 b      0.0475      4.12   7.31   0.05 percentile\n#> 2 k     37.6        46.7   59.8    0.05 percentile\n\nOr we can use histograms to get a more detailed idea of the uncertainty in each estimate:\n\nggplot(boot_coefs, aes(estimate)) +\n  geom_histogram(bins = 30) +\n  facet_wrap( ~ term, scales = \"free\") +\n  geom_vline(aes(xintercept = .lower), data = percentile_intervals, col = \"blue\") +\n  geom_vline(aes(xintercept = .upper), data = percentile_intervals, col = \"blue\")\n\n\n\n\n\n\n\n\nThe rsample package also has functions for other types of confidence intervals."
  },
  {
    "objectID": "content/learn/statistics/bootstrap/index.html#possible-model-fits",
    "href": "content/learn/statistics/bootstrap/index.html#possible-model-fits",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Possible model fits",
    "text": "Possible model fits\nWe can use augment() to visualize the uncertainty in the fitted curve. Since there are so many bootstrap samples, we’ll only show a sample of the model fits in our visualization:\n\nboot_aug <- \n  boot_models %>% \n  sample_n(200) %>% \n  mutate(augmented = map(model, augment)) %>% \n  unnest(augmented)\n\nboot_aug\n#> # A tibble: 6,400 × 8\n#>    splits          id            model  coef_info   mpg    wt .fitted .resid\n#>    <list>          <chr>         <list> <list>    <dbl> <dbl>   <dbl>  <dbl>\n#>  1 <split [32/11]> Bootstrap1644 <nls>  <tibble>   16.4  4.07    15.6  0.829\n#>  2 <split [32/11]> Bootstrap1644 <nls>  <tibble>   19.7  2.77    21.9 -2.21 \n#>  3 <split [32/11]> Bootstrap1644 <nls>  <tibble>   19.2  3.84    16.4  2.84 \n#>  4 <split [32/11]> Bootstrap1644 <nls>  <tibble>   21.4  2.78    21.8 -0.437\n#>  5 <split [32/11]> Bootstrap1644 <nls>  <tibble>   26    2.14    27.8 -1.75 \n#>  6 <split [32/11]> Bootstrap1644 <nls>  <tibble>   33.9  1.84    32.0  1.88 \n#>  7 <split [32/11]> Bootstrap1644 <nls>  <tibble>   32.4  2.2     27.0  5.35 \n#>  8 <split [32/11]> Bootstrap1644 <nls>  <tibble>   30.4  1.62    36.1 -5.70 \n#>  9 <split [32/11]> Bootstrap1644 <nls>  <tibble>   21.5  2.46    24.4 -2.86 \n#> 10 <split [32/11]> Bootstrap1644 <nls>  <tibble>   26    2.14    27.8 -1.75 \n#> # … with 6,390 more rows\n\n\nggplot(boot_aug, aes(wt, mpg)) +\n  geom_line(aes(y = .fitted, group = id), alpha = .2, col = \"blue\") +\n  geom_point()\n\n\n\n\n\n\n\n\nWith only a few small changes, we could easily perform bootstrapping with other kinds of predictive or hypothesis testing models, since the tidy() and augment() functions works for many statistical outputs. As another example, we could use smooth.spline(), which fits a cubic smoothing spline to data:\n\nfit_spline_on_bootstrap <- function(split) {\n    data <- analysis(split)\n    smooth.spline(data$wt, data$mpg, df = 4)\n}\n\nboot_splines <- \n  boots %>% \n  sample_n(200) %>% \n  mutate(spline = map(splits, fit_spline_on_bootstrap),\n         aug_train = map(spline, augment))\n\nsplines_aug <- \n  boot_splines %>% \n  unnest(aug_train)\n\nggplot(splines_aug, aes(x, y)) +\n  geom_line(aes(y = .fitted, group = id), alpha = 0.2, col = \"blue\") +\n  geom_point()"
  },
  {
    "objectID": "content/learn/statistics/bootstrap/index.html#session-information",
    "href": "content/learn/statistics/bootstrap/index.html#session-information",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/statistics/xtabs/index.html",
    "href": "content/learn/statistics/xtabs/index.html",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nIn this vignette, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\n\ndata(ad_data, package = \"modeldata\")\nad_data %>%\n  select(Genotype, Class)\n#> # A tibble: 333 × 2\n#>    Genotype Class   \n#>    <fct>    <fct>   \n#>  1 E3E3     Control \n#>  2 E3E4     Control \n#>  3 E3E4     Control \n#>  4 E3E4     Control \n#>  5 E3E3     Control \n#>  6 E4E4     Impaired\n#>  7 E2E3     Control \n#>  8 E2E3     Control \n#>  9 E3E3     Control \n#> 10 E2E3     Impaired\n#> # … with 323 more rows\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "content/learn/statistics/xtabs/index.html#test-of-independence",
    "href": "content/learn/statistics/xtabs/index.html#test-of-independence",
    "title": "Statistical analysis of contingency tables",
    "section": "Test of independence",
    "text": "Test of independence\nTo carry out a chi-squared test of independence, we’ll examine the association between their cognitive ability (impaired and healthy) and the genetic makeup. This is what the relationship looks like in the sample data:\n\n\n\n\n\n\n\n\n\nIf there were no relationship, we would expect to see the purple bars reaching to the same length, regardless of cognitive ability. Are the differences we see here, though, just due to random noise?\nFirst, to calculate the observed statistic, we can use specify() and calculate().\n\n# calculate the observed statistic\nobserved_indep_statistic <- ad_data %>%\n  specify(Genotype ~ Class) %>%\n  calculate(stat = \"Chisq\")\n\nThe observed \\(\\chi^2\\) statistic is 21.5774809. Now, we want to compare this statistic to a null distribution, generated under the assumption that these variables are not actually related, to get a sense of how likely it would be for us to see this observed statistic if there were actually no association between cognitive ability and genetics.\nWe can generate() the null distribution in one of two ways: using randomization or theory-based methods. The randomization approach permutes the response and explanatory variables, so that each person’s genetics is matched up with a random cognitive rating from the sample in order to break up any association between the two.\n\n# generate the null distribution using randomization\nnull_distribution_simulated <- ad_data %>%\n  specify(Genotype ~ Class) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\") %>%\n  calculate(stat = \"Chisq\")\n\nNote that, in the line specify(Genotype ~ Class) above, we could use the equivalent syntax specify(response = Genotype, explanatory = Class). The same goes in the code below, which generates the null distribution using theory-based methods instead of randomization.\n\n# generate the null distribution by theoretical approximation\nnull_distribution_theoretical <- ad_data %>%\n  specify(Genotype ~ Class) %>%\n  hypothesize(null = \"independence\") %>%\n  # note that we skip the generation step here!\n  calculate(stat = \"Chisq\")\n\nTo get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_simulated %>%\n  visualize() + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nWe could also visualize the observed statistic against the theoretical null distribution. Note that we skip the generate() and calculate() steps when using the theoretical approach, and that we now need to provide method = \"theoretical\" to visualize().\n\n# visualize the theoretical null distribution and test statistic!\nad_data %>%\n  specify(Genotype ~ Class) %>%\n  hypothesize(null = \"independence\") %>%\n  visualize(method = \"theoretical\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nTo visualize both the randomization-based and theoretical null distributions to get a sense of how the two relate, we can pipe the randomization-based null distribution into visualize(), and further provide method = \"both\".\n\n# visualize both null distributions and the test statistic!\nnull_distribution_simulated %>%\n  visualize(method = \"both\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nEither way, it looks like our observed test statistic would be fairly unlikely if there were actually no association between cognition and genotype. More exactly, we can calculate the p-value:\n\n# calculate the p value from the observed statistic and null distribution\np_value_independence <- null_distribution_simulated %>%\n  get_p_value(obs_stat = observed_indep_statistic,\n              direction = \"greater\")\n\np_value_independence\n#> # A tibble: 1 × 1\n#>   p_value\n#>     <dbl>\n#> 1  0.0004\n\nThus, if there were really no relationship between cognition and genotype, the probability that we would see a statistic as or more extreme than 21.5774809 is approximately 4^{-4}.\nNote that, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out Chi-Squared tests of independence on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, Genotype ~ Class)\n#> # A tibble: 1 × 3\n#>   statistic chisq_df  p_value\n#>       <dbl>    <int>    <dbl>\n#> 1      21.6        5 0.000630"
  },
  {
    "objectID": "content/learn/statistics/xtabs/index.html#goodness-of-fit",
    "href": "content/learn/statistics/xtabs/index.html#goodness-of-fit",
    "title": "Statistical analysis of contingency tables",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nNow, moving on to a chi-squared goodness of fit test, we’ll take a look at just the genotype data. Many papers have investigated the relationship of Apolipoprotein E to diseases. For example, Song et al (2004) conducted a meta-analysis of numerous studies that looked at this gene and heart disease. In their paper, they describe the frequency of the different genotypes across many samples. For the cognition study, it might be interesting to see if our sample of genotypes was consistent with this literature (treating the rates, for this analysis, as known).\nThe rates of the meta-analysis and our observed data are:\n\n# Song, Y., Stampfer, M. J., & Liu, S. (2004). Meta-Analysis: Apolipoprotein E \n# Genotypes and Risk for Coronary Heart Disease. Annals of Internal Medicine, \n# 141(2), 137.\nmeta_rates <- c(\"E2E2\" = 0.71, \"E2E3\" = 11.4, \"E2E4\" = 2.32,\n                \"E3E3\" = 61.0, \"E3E4\" = 22.6, \"E4E4\" = 2.22)\nmeta_rates <- meta_rates/sum(meta_rates) # these add up to slightly > 100%\n\nobs_rates <- table(ad_data$Genotype)/nrow(ad_data)\nround(cbind(obs_rates, meta_rates) * 100, 2)\n#>      obs_rates meta_rates\n#> E2E2      0.60       0.71\n#> E2E3     11.11      11.37\n#> E2E4      2.40       2.31\n#> E3E3     50.15      60.85\n#> E3E4     31.83      22.54\n#> E4E4      3.90       2.21\n\nSuppose our null hypothesis is that Genotype follows the same frequency distribution as the meta-analysis. Lets now test whether this difference in distributions is statistically significant.\nFirst, to carry out this hypothesis test, we would calculate our observed statistic.\n\n# calculating the null distribution\nobserved_gof_statistic <- ad_data %>%\n  specify(response = Genotype) %>%\n  hypothesize(null = \"point\", p = meta_rates) %>%\n  calculate(stat = \"Chisq\")\n\nThe observed statistic is 23.3838483. Now, generating a null distribution, by just dropping in a call to generate():\n\n# generating a null distribution\nnull_distribution_gof <- ad_data %>%\n  specify(response = Genotype) %>%\n  hypothesize(null = \"point\", p = meta_rates) %>%\n  generate(reps = 5000, type = \"simulate\") %>%\n  calculate(stat = \"Chisq\")\n\nAgain, to get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_gof %>%\n  visualize() + \n  shade_p_value(observed_gof_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nThis statistic seems like it would be unlikely if our rates were the same as the rates from the meta-analysis! How unlikely, though? Calculating the p-value:\n\n# calculate the p-value\np_value_gof <- null_distribution_gof %>%\n  get_p_value(observed_gof_statistic,\n              direction = \"greater\")\n\np_value_gof\n#> # A tibble: 1 × 1\n#>   p_value\n#>     <dbl>\n#> 1  0.0012\n\nThus, if each genotype occurred at the same rate as the Song paper, the probability that we would see a distribution like the one we did is approximately 0.0012.\nAgain, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out chi-squared goodness of fit tests on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, response = Genotype, p = meta_rates)\n#> # A tibble: 1 × 3\n#>   statistic chisq_df  p_value\n#>       <dbl>    <dbl>    <dbl>\n#> 1      23.4        5 0.000285"
  },
  {
    "objectID": "content/learn/statistics/xtabs/index.html#session-information",
    "href": "content/learn/statistics/xtabs/index.html#session-information",
    "title": "Statistical analysis of contingency tables",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/statistics/tidy-analysis/index.html",
    "href": "content/learn/statistics/tidy-analysis/index.html",
    "title": "Correlation and regression fundamentals with tidy data principles",
    "section": "",
    "text": "This article only requires the tidymodels package.\nWhile the tidymodels package broom is useful for summarizing the result of a single analysis in a consistent format, it is really designed for high-throughput applications, where you must combine results from multiple analyses. These could be subgroups of data, analyses using different models, bootstrap replicates, permutations, and so on. In particular, it plays well with the nest()/unnest() functions from tidyr and the map() function in purrr."
  },
  {
    "objectID": "content/learn/statistics/tidy-analysis/index.html#correlation-analysis",
    "href": "content/learn/statistics/tidy-analysis/index.html#correlation-analysis",
    "title": "Correlation and regression fundamentals with tidy data principles",
    "section": "Correlation analysis",
    "text": "Correlation analysis\nLet’s demonstrate this with a simple data set, the built-in Orange. We start by coercing Orange to a tibble. This gives a nicer print method that will be especially useful later on when we start working with list-columns.\n\nlibrary(tidymodels)\n\ndata(Orange)\n\nOrange <- as_tibble(Orange)\nOrange\n#> # A tibble: 35 × 3\n#>    Tree    age circumference\n#>    <ord> <dbl>         <dbl>\n#>  1 1       118            30\n#>  2 1       484            58\n#>  3 1       664            87\n#>  4 1      1004           115\n#>  5 1      1231           120\n#>  6 1      1372           142\n#>  7 1      1582           145\n#>  8 2       118            33\n#>  9 2       484            69\n#> 10 2       664           111\n#> # … with 25 more rows\n\nThis contains 35 observations of three variables: Tree, age, and circumference. Tree is a factor with five levels describing five trees. As might be expected, age and circumference are correlated:\n\ncor(Orange$age, Orange$circumference)\n#> [1] 0.9135189\n\nlibrary(ggplot2)\n\nggplot(Orange, aes(age, circumference, color = Tree)) +\n  geom_line()\n\n\n\n\n\n\n\n\nSuppose you want to test for correlations individually within each tree. You can do this with dplyr’s group_by:\n\nOrange %>% \n  group_by(Tree) %>%\n  summarize(correlation = cor(age, circumference))\n#> # A tibble: 5 × 2\n#>   Tree  correlation\n#>   <ord>       <dbl>\n#> 1 3           0.988\n#> 2 1           0.985\n#> 3 5           0.988\n#> 4 2           0.987\n#> 5 4           0.984\n\n(Note that the correlations are much higher than the aggregated one, and also we can now see the correlation is similar across trees).\nSuppose that instead of simply estimating a correlation, we want to perform a hypothesis test with cor.test():\n\nct <- cor.test(Orange$age, Orange$circumference)\nct\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  Orange$age and Orange$circumference\n#> t = 12.9, df = 33, p-value = 1.931e-14\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.8342364 0.9557955\n#> sample estimates:\n#>       cor \n#> 0.9135189\n\nThis test output contains multiple values we may be interested in. Some are vectors of length 1, such as the p-value and the estimate, and some are longer, such as the confidence interval. We can get this into a nicely organized tibble using the tidy() function:\n\ntidy(ct)\n#> # A tibble: 1 × 8\n#>   estimate statistic  p.value parameter conf.low conf.high method        alter…¹\n#>      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>         <chr>  \n#> 1    0.914      12.9 1.93e-14        33    0.834     0.956 Pearson's pr… two.si…\n#> # … with abbreviated variable name ¹​alternative\n\nOften, we want to perform multiple tests or fit multiple models, each on a different part of the data. In this case, we recommend a nest-map-unnest workflow. For example, suppose we want to perform correlation tests for each different tree. We start by nesting our data based on the group of interest:\n\nnested <- \n  Orange %>% \n  nest(data = c(age, circumference))\n\nThen we perform a correlation test for each nested tibble using purrr::map():\n\nnested %>% \n  mutate(test = map(data, ~ cor.test(.x$age, .x$circumference)))\n#> # A tibble: 5 × 3\n#>   Tree  data             test   \n#>   <ord> <list>           <list> \n#> 1 1     <tibble [7 × 2]> <htest>\n#> 2 2     <tibble [7 × 2]> <htest>\n#> 3 3     <tibble [7 × 2]> <htest>\n#> 4 4     <tibble [7 × 2]> <htest>\n#> 5 5     <tibble [7 × 2]> <htest>\n\nThis results in a list-column of S3 objects. We want to tidy each of the objects, which we can also do with map().\n\nnested %>% \n  mutate(\n    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col\n    tidied = map(test, tidy)\n  ) \n#> # A tibble: 5 × 4\n#>   Tree  data             test    tidied          \n#>   <ord> <list>           <list>  <list>          \n#> 1 1     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 2 2     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 3 3     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 4 4     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 5 5     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n\nFinally, we want to unnest the tidied data frames so we can see the results in a flat tibble. All together, this looks like:\n\nOrange %>% \n  nest(data = c(age, circumference)) %>% \n  mutate(\n    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col\n    tidied = map(test, tidy)\n  ) %>% \n  unnest(cols = tidied) %>% \n  select(-data, -test)\n#> # A tibble: 5 × 9\n#>   Tree  estimate statistic   p.value parameter conf.low conf.high method alter…¹\n#>   <ord>    <dbl>     <dbl>     <dbl>     <int>    <dbl>     <dbl> <chr>  <chr>  \n#> 1 1        0.985      13.0 0.0000485         5    0.901     0.998 Pears… two.si…\n#> 2 2        0.987      13.9 0.0000343         5    0.914     0.998 Pears… two.si…\n#> 3 3        0.988      14.4 0.0000290         5    0.919     0.998 Pears… two.si…\n#> 4 4        0.984      12.5 0.0000573         5    0.895     0.998 Pears… two.si…\n#> 5 5        0.988      14.1 0.0000318         5    0.916     0.998 Pears… two.si…\n#> # … with abbreviated variable name ¹​alternative"
  },
  {
    "objectID": "content/learn/statistics/tidy-analysis/index.html#regression-models",
    "href": "content/learn/statistics/tidy-analysis/index.html#regression-models",
    "title": "Correlation and regression fundamentals with tidy data principles",
    "section": "Regression models",
    "text": "Regression models\nThis type of workflow becomes even more useful when applied to regressions. Untidy output for a regression looks like:\n\nlm_fit <- lm(age ~ circumference, data = Orange)\nsummary(lm_fit)\n#> \n#> Call:\n#> lm(formula = age ~ circumference, data = Orange)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -317.88 -140.90  -17.20   96.54  471.16 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    16.6036    78.1406   0.212    0.833    \n#> circumference   7.8160     0.6059  12.900 1.93e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 203.1 on 33 degrees of freedom\n#> Multiple R-squared:  0.8345, Adjusted R-squared:  0.8295 \n#> F-statistic: 166.4 on 1 and 33 DF,  p-value: 1.931e-14\n\nWhen we tidy these results, we get multiple rows of output for each model:\n\ntidy(lm_fit)\n#> # A tibble: 2 × 5\n#>   term          estimate std.error statistic  p.value\n#>   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)      16.6     78.1       0.212 8.33e- 1\n#> 2 circumference     7.82     0.606    12.9   1.93e-14\n\nNow we can handle multiple regressions at once using exactly the same workflow as before:\n\nOrange %>%\n  nest(data = c(-Tree)) %>% \n  mutate(\n    fit = map(data, ~ lm(age ~ circumference, data = .x)),\n    tidied = map(fit, tidy)\n  ) %>% \n  unnest(tidied) %>% \n  select(-data, -fit)\n#> # A tibble: 10 × 6\n#>    Tree  term          estimate std.error statistic   p.value\n#>    <ord> <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n#>  1 1     (Intercept)    -265.      98.6      -2.68  0.0436   \n#>  2 1     circumference    11.9      0.919    13.0   0.0000485\n#>  3 2     (Intercept)    -132.      83.1      -1.59  0.172    \n#>  4 2     circumference     7.80     0.560    13.9   0.0000343\n#>  5 3     (Intercept)    -210.      85.3      -2.46  0.0574   \n#>  6 3     circumference    12.0      0.835    14.4   0.0000290\n#>  7 4     (Intercept)     -76.5     88.3      -0.867 0.426    \n#>  8 4     circumference     7.17     0.572    12.5   0.0000573\n#>  9 5     (Intercept)     -54.5     76.9      -0.709 0.510    \n#> 10 5     circumference     8.79     0.621    14.1   0.0000318\n\nYou can just as easily use multiple predictors in the regressions, as shown here on the mtcars dataset. We nest the data into automatic vs. manual cars (the am column), then perform the regression within each nested tibble.\n\ndata(mtcars)\nmtcars <- as_tibble(mtcars)  # to play nicely with list-cols\nmtcars\n#> # A tibble: 32 × 11\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#> # … with 22 more rows\n\nmtcars %>%\n  nest(data = c(-am)) %>% \n  mutate(\n    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),  # S3 list-col\n    tidied = map(fit, tidy)\n  ) %>% \n  unnest(tidied) %>% \n  select(-data, -fit)\n#> # A tibble: 8 × 6\n#>      am term        estimate std.error statistic  p.value\n#>   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1     1 (Intercept)   4.28      3.46      1.24   0.247   \n#> 2     1 mpg          -0.101     0.0294   -3.43   0.00750 \n#> 3     1 qsec          0.0398    0.151     0.264  0.798   \n#> 4     1 gear         -0.0229    0.349    -0.0656 0.949   \n#> 5     0 (Intercept)   4.92      1.40      3.52   0.00309 \n#> 6     0 mpg          -0.192     0.0443   -4.33   0.000591\n#> 7     0 qsec          0.0919    0.0983    0.935  0.365   \n#> 8     0 gear          0.147     0.368     0.398  0.696\n\nWhat if you want not just the tidy() output, but the augment() and glance() outputs as well, while still performing each regression only once? Since we’re using list-columns, we can just fit the model once and use multiple list-columns to store the tidied, glanced and augmented outputs.\n\nregressions <- \n  mtcars %>%\n  nest(data = c(-am)) %>% \n  mutate(\n    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),\n    tidied = map(fit, tidy),\n    glanced = map(fit, glance),\n    augmented = map(fit, augment)\n  )\n\nregressions %>% \n  select(tidied) %>% \n  unnest(tidied)\n#> # A tibble: 8 × 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)   4.28      3.46      1.24   0.247   \n#> 2 mpg          -0.101     0.0294   -3.43   0.00750 \n#> 3 qsec          0.0398    0.151     0.264  0.798   \n#> 4 gear         -0.0229    0.349    -0.0656 0.949   \n#> 5 (Intercept)   4.92      1.40      3.52   0.00309 \n#> 6 mpg          -0.192     0.0443   -4.33   0.000591\n#> 7 qsec          0.0919    0.0983    0.935  0.365   \n#> 8 gear          0.147     0.368     0.398  0.696\n\nregressions %>% \n  select(glanced) %>% \n  unnest(glanced)\n#> # A tibble: 2 × 12\n#>   r.squared adj.r.squ…¹ sigma stati…² p.value    df   logLik   AIC   BIC devia…³\n#>       <dbl>       <dbl> <dbl>   <dbl>   <dbl> <dbl>    <dbl> <dbl> <dbl>   <dbl>\n#> 1     0.833       0.778 0.291   15.0  7.59e-4     3 -5.80e-3  10.0  12.8   0.762\n#> 2     0.625       0.550 0.522    8.32 1.70e-3     3 -1.24e+1  34.7  39.4   4.08 \n#> # … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#> #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\nregressions %>% \n  select(augmented) %>% \n  unnest(augmented)\n#> # A tibble: 32 × 10\n#>       wt   mpg  qsec  gear .fitted  .resid  .hat .sigma  .cooksd .std.resid\n#>    <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>      <dbl>\n#>  1  2.62  21    16.5     4    2.73 -0.107  0.517  0.304 0.0744      -0.527 \n#>  2  2.88  21    17.0     4    2.75  0.126  0.273  0.304 0.0243       0.509 \n#>  3  2.32  22.8  18.6     4    2.63 -0.310  0.312  0.279 0.188       -1.29  \n#>  4  2.2   32.4  19.5     4    1.70  0.505  0.223  0.233 0.278        1.97  \n#>  5  1.62  30.4  18.5     4    1.86 -0.244  0.269  0.292 0.0889      -0.982 \n#>  6  1.84  33.9  19.9     4    1.56  0.274  0.286  0.286 0.125        1.12  \n#>  7  1.94  27.3  18.9     4    2.19 -0.253  0.151  0.293 0.0394      -0.942 \n#>  8  2.14  26    16.7     5    2.21 -0.0683 0.277  0.307 0.00732     -0.276 \n#>  9  1.51  30.4  16.9     5    1.77 -0.259  0.430  0.284 0.263       -1.18  \n#> 10  3.17  15.8  14.5     5    3.15  0.0193 0.292  0.308 0.000644     0.0789\n#> # … with 22 more rows\n\nBy combining the estimates and p-values across all groups into the same tidy data frame (instead of a list of output model objects), a new class of analyses and visualizations becomes straightforward. This includes:\n\nsorting by p-value or estimate to find the most significant terms across all tests,\np-value histograms, and\nvolcano plots comparing p-values to effect size estimates.\n\nIn each of these cases, we can easily filter, facet, or distinguish based on the term column. In short, this makes the tools of tidy data analysis available for the results of data analysis and models, not just the inputs."
  },
  {
    "objectID": "content/learn/statistics/tidy-analysis/index.html#session-information",
    "href": "content/learn/statistics/tidy-analysis/index.html#session-information",
    "title": "Correlation and regression fundamentals with tidy data principles",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/statistics/infer/index.html",
    "href": "content/learn/statistics/infer/index.html",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "",
    "text": "This article only requires the tidymodels package.\nThe tidymodels package infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question:\n\nIs the effect or difference in our observed data real, or due to chance?\n\nTo answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis.\nIf you are new to hypothesis testing, take a look at\n\nSection 9.2 of Statistical Inference via Data Science\nThe American Statistical Association’s recent statement on p-values\n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in,\nhypothesize() allows you to declare the null hypothesis,\ngenerate() allows you to generate data reflecting the null hypothesis, and\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of gss, a data set available in infer containing a sample of 500 observations of 11 variables from the General Social Survey.\n\nlibrary(tidymodels) # Includes the infer package\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n#> Rows: 500\n#> Columns: 11\n#> $ year    <dbl> 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#> $ age     <dbl> 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#> $ sex     <fct> male, female, male, male, male, female, female, female, female…\n#> $ college <fct> degree, no degree, degree, no degree, degree, no degree, no de…\n#> $ partyid <fct> ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#> $ hompop  <dbl> 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#> $ hours   <dbl> 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#> $ income  <ord> $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#> $ class   <fct> middle class, working class, working class, working class, mid…\n#> $ finrela <fct> below average, below average, below average, above average, ab…\n#> $ weight  <dbl> 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See ?gss for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let’s suppose that this data set is a representative sample of a population we want to learn about: American adults."
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#specify-variables",
    "href": "content/learn/statistics/infer/index.html#specify-variables",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Specify variables",
    "text": "Specify variables\nThe specify() function can be used to specify which of the variables in the data set you’re interested in. If you’re only interested in, say, the age of the respondents, you might write:\n\ngss %>%\n  specify(response = age)\n#> Response: age (numeric)\n#> # A tibble: 500 × 1\n#>      age\n#>    <dbl>\n#>  1    36\n#>  2    34\n#>  3    24\n#>  4    42\n#>  5    31\n#>  6    32\n#>  7    48\n#>  8    36\n#>  9    30\n#> 10    33\n#> # … with 490 more rows\n\nOn the front end, the output of specify() just looks like it selects off the columns in the dataframe that you’ve specified. What do we see if we check the class of this object, though?\n\ngss %>%\n  specify(response = age) %>%\n  class()\n#> [1] \"infer\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nWe can see that the infer class has been appended on top of the dataframe classes; this new class stores some extra metadata.\nIf you’re interested in two variables (age and partyid, for example) you can specify() their relationship in one of two (equivalent) ways:\n\n# as a formula\ngss %>%\n  specify(age ~ partyid)\n#> Response: age (numeric)\n#> Explanatory: partyid (factor)\n#> # A tibble: 500 × 2\n#>      age partyid\n#>    <dbl> <fct>  \n#>  1    36 ind    \n#>  2    34 rep    \n#>  3    24 ind    \n#>  4    42 ind    \n#>  5    31 rep    \n#>  6    32 rep    \n#>  7    48 dem    \n#>  8    36 ind    \n#>  9    30 rep    \n#> 10    33 dem    \n#> # … with 490 more rows\n\n# with the named arguments\ngss %>%\n  specify(response = age, explanatory = partyid)\n#> Response: age (numeric)\n#> Explanatory: partyid (factor)\n#> # A tibble: 500 × 2\n#>      age partyid\n#>    <dbl> <fct>  \n#>  1    36 ind    \n#>  2    34 rep    \n#>  3    24 ind    \n#>  4    42 ind    \n#>  5    31 rep    \n#>  6    32 rep    \n#>  7    48 dem    \n#>  8    36 ind    \n#>  9    30 rep    \n#> 10    33 dem    \n#> # … with 490 more rows\n\nIf you’re doing inference on one proportion or a difference in proportions, you will need to use the success argument to specify which level of your response variable is a success. For instance, if you’re interested in the proportion of the population with a college degree, you might use the following code:\n\n# specifying for inference on proportions\ngss %>%\n  specify(response = college, success = \"degree\")\n#> Response: college (factor)\n#> # A tibble: 500 × 1\n#>    college  \n#>    <fct>    \n#>  1 degree   \n#>  2 no degree\n#>  3 degree   \n#>  4 no degree\n#>  5 degree   \n#>  6 no degree\n#>  7 no degree\n#>  8 degree   \n#>  9 degree   \n#> 10 no degree\n#> # … with 490 more rows"
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#declare-the-hypothesis",
    "href": "content/learn/statistics/infer/index.html#declare-the-hypothesis",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Declare the hypothesis",
    "text": "Declare the hypothesis\nThe next step in the infer pipeline is often to declare a null hypothesis using hypothesize(). The first step is to supply one of “independence” or “point” to the null argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to hypothesize():\n\ngss %>%\n  specify(college ~ partyid, success = \"degree\") %>%\n  hypothesize(null = \"independence\")\n#> Response: college (factor)\n#> Explanatory: partyid (factor)\n#> Null Hypothesis: independence\n#> # A tibble: 500 × 2\n#>    college   partyid\n#>    <fct>     <fct>  \n#>  1 degree    ind    \n#>  2 no degree rep    \n#>  3 degree    ind    \n#>  4 no degree ind    \n#>  5 degree    rep    \n#>  6 no degree rep    \n#>  7 no degree dem    \n#>  8 degree    ind    \n#>  9 degree    rep    \n#> 10 no degree dem    \n#> # … with 490 more rows\n\nIf you’re doing inference on a point estimate, you will also need to provide one of p (the true proportion of successes, between 0 and 1), mu (the true mean), med (the true median), or sigma (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40)\n#> Response: hours (numeric)\n#> Null Hypothesis: point\n#> # A tibble: 500 × 1\n#>    hours\n#>    <dbl>\n#>  1    50\n#>  2    31\n#>  3    40\n#>  4    40\n#>  5    40\n#>  6    53\n#>  7    32\n#>  8    20\n#>  9    40\n#> 10    40\n#> # … with 490 more rows\n\nAgain, from the front-end, the dataframe outputted from hypothesize() looks almost exactly the same as it did when it came out of specify(), but infer now “knows” your null hypothesis."
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#generate-the-distribution",
    "href": "content/learn/statistics/infer/index.html#generate-the-distribution",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Generate the distribution",
    "text": "Generate the distribution\nOnce we’ve asserted our null hypothesis using hypothesize(), we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the type argument:\n\nbootstrap: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.\n\npermute: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.\n\nsimulate: A value will be sampled from a theoretical distribution with parameters specified in hypothesize() for each replicate. (This option is currently only applicable for testing point estimates.)\n\nContinuing on with our example above, about the average number of hours worked a week, we might write:\n\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\")\n#> Response: hours (numeric)\n#> Null Hypothesis: point\n#> # A tibble: 2,500,000 × 2\n#> # Groups:   replicate [5,000]\n#>    replicate hours\n#>        <int> <dbl>\n#>  1         1  23.6\n#>  2         1  38.6\n#>  3         1  38.6\n#>  4         1  46.6\n#>  5         1  38.6\n#>  6         1  38.6\n#>  7         1  10.6\n#>  8         1  38.6\n#>  9         1  58.6\n#> 10         1  22.6\n#> # … with 2,499,990 more rows\n\nIn the above example, we take 5000 bootstrap samples to form our null distribution.\nTo generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:\n\ngss %>%\n  specify(partyid ~ age) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\")\n#> Response: partyid (factor)\n#> Explanatory: age (numeric)\n#> Null Hypothesis: independence\n#> # A tibble: 2,500,000 × 3\n#> # Groups:   replicate [5,000]\n#>    partyid   age replicate\n#>    <fct>   <dbl>     <int>\n#>  1 rep        36         1\n#>  2 ind        34         1\n#>  3 dem        24         1\n#>  4 ind        42         1\n#>  5 rep        31         1\n#>  6 ind        32         1\n#>  7 dem        48         1\n#>  8 ind        36         1\n#>  9 rep        30         1\n#> 10 ind        33         1\n#> # … with 2,499,990 more rows"
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#calculate-statistics",
    "href": "content/learn/statistics/infer/index.html#calculate-statistics",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Calculate statistics",
    "text": "Calculate statistics\nDepending on whether you’re carrying out computation-based inference or theory-based inference, you will either supply calculate() with the output of generate() or hypothesize(), respectively. The function, for one, takes in a stat argument, which is currently one of \"mean\", \"median\", \"sum\", \"sd\", \"prop\", \"count\", \"diff in means\", \"diff in medians\", \"diff in props\", \"Chisq\", \"F\", \"t\", \"z\", \"slope\", or \"correlation\". For example, continuing our example above to calculate the null distribution of mean hours worked per week:\n\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n#> Response: hours (numeric)\n#> Null Hypothesis: point\n#> # A tibble: 5,000 × 2\n#>    replicate  stat\n#>        <int> <dbl>\n#>  1         1  39.4\n#>  2         2  40.2\n#>  3         3  40.1\n#>  4         4  40.1\n#>  5         5  40.9\n#>  6         6  39.2\n#>  7         7  40.6\n#>  8         8  40.0\n#>  9         9  39.4\n#> 10        10  40.1\n#> # … with 4,990 more rows\n\nThe output of calculate() here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you’re carrying out inference on differences in means, medians, or proportions, or \\(t\\) and \\(z\\) statistics, you will need to supply an order argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don’t, we might write:\n\ngss %>%\n  specify(age ~ college) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\") %>%\n  calculate(\"diff in means\", order = c(\"degree\", \"no degree\"))\n#> Response: age (numeric)\n#> Explanatory: college (factor)\n#> Null Hypothesis: independence\n#> # A tibble: 5,000 × 2\n#>    replicate    stat\n#>        <int>   <dbl>\n#>  1         1 -1.69  \n#>  2         2 -1.75  \n#>  3         3 -0.0642\n#>  4         4 -0.478 \n#>  5         5  0.667 \n#>  6         6  0.738 \n#>  7         7  1.14  \n#>  8         8  1.13  \n#>  9         9 -0.223 \n#> 10        10 -1.05  \n#> # … with 4,990 more rows"
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#other-utilities",
    "href": "content/learn/statistics/infer/index.html#other-utilities",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Other utilities",
    "text": "Other utilities\nThe infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with visualize()), calculate p-values (with get_p_value()), and calculate confidence intervals (with get_confidence_interval()).\nTo illustrate, we’ll go back to the example of determining whether the mean number of hours worked per week is 40 hours.\n\n# find the point estimate\npoint_estimate <- gss %>%\n  specify(response = hours) %>%\n  calculate(stat = \"mean\")\n\n# generate a null distribution\nnull_dist <- gss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n\n(Notice the warning: Removed 1244 rows containing missing values. This would be worth noting if you were actually carrying out this hypothesis test.)\nOur point estimate 41.382 seems pretty close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn’t 40.\nWe could initially just visualize the null distribution.\n\nnull_dist %>%\n  visualize()\n\n\n\n\n\n\n\n\nWhere does our sample’s observed statistic lie on this distribution? We can use the obs_stat argument to specify this.\n\nnull_dist %>%\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\n\n\n\n\n\n\n\nNotice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the + operator to apply the shade_p_value() function. This is because visualize() outputs a plot object from ggplot2 instead of a dataframe, and the + operator is needed to add the p-value layer to the plot object.) The red bar looks like it’s slightly far out on the right tail of the null distribution, so observing a sample mean of 41.382 hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?\n\n# get a two-tailed p-value\np_value <- null_dist %>%\n  get_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\np_value\n#> # A tibble: 1 × 1\n#>   p_value\n#>     <dbl>\n#> 1  0.0368\n\nIt looks like the p-value is 0.0368, which is pretty small—if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (1.382 hours) from 40 would be 0.0368. This may or may not be statistically significantly different, depending on the significance level \\(\\alpha\\) you decided on before you ran this analysis. If you had set \\(\\alpha = .05\\), then this difference would be statistically significant, but if you had set \\(\\alpha = .01\\), then it would not be.\nTo get a confidence interval around our estimate, we can write:\n\n# start with the null distribution\nnull_dist %>%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = point_estimate,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n#> # A tibble: 1 × 2\n#>   lower_ci upper_ci\n#>      <dbl>    <dbl>\n#> 1     40.1     42.7\n\nAs you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level \\(\\alpha = .05\\)."
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#theoretical-methods",
    "href": "content/learn/statistics/infer/index.html#theoretical-methods",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Theoretical methods",
    "text": "Theoretical methods\nThe infer package also provides functionality to use theoretical methods for \"Chisq\", \"F\" and \"t\" test statistics.\nGenerally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the generate() step. For example, if we wanted to find a null distribution for the relationship between age (age) and party identification (partyid) using randomization, we could write:\n\nnull_f_distn <- gss %>%\n   specify(age ~ partyid) %>%\n   hypothesize(null = \"independence\") %>%\n   generate(reps = 5000, type = \"permute\") %>%\n   calculate(stat = \"F\")\n\nTo find the null distribution using theory-based methods, instead, skip the generate() step entirely:\n\nnull_f_distn_theoretical <- gss %>%\n   specify(age ~ partyid) %>%\n   hypothesize(null = \"independence\") %>%\n   calculate(stat = \"F\")\n\nWe’ll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.\n\nF_hat <- gss %>% \n  specify(age ~ partyid) %>%\n  calculate(stat = \"F\")\n\nNow, instead of just piping the null distribution into visualize(), as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide method = \"theoretical\" to visualize().\n\nvisualize(null_f_distn_theoretical, method = \"theoretical\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n\n\n\n\n\n\n\n\nTo get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into visualize() and also specify method = \"both\"\n\nvisualize(null_f_distn, method = \"both\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n\n\n\n\n\n\n\n\nThat’s it! This vignette covers most all of the key functionality of infer. See help(package = \"infer\") for a full list of functions and vignettes."
  },
  {
    "objectID": "content/learn/statistics/infer/index.html#session-information",
    "href": "content/learn/statistics/infer/index.html#session-information",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/develop/metrics/index.html",
    "href": "content/learn/develop/metrics/index.html",
    "title": "Custom performance metrics",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: rlang and tidymodels.\nThe yardstick package already includes a large number of metrics, but there’s obviously a chance that you might have a custom metric that hasn’t been implemented yet. In that case, you can use a few of the tools yardstick exposes to create custom metrics.\nWhy create custom metrics? With the infrastructure yardstick provides, you get:\n\nStandardization between your metric and other preexisting metrics\nAutomatic error handling for types and lengths\nAutomatic selection of binary / multiclass metric implementations\nAutomatic NA handling\nSupport for grouped data frames\nSupport for use alongside other metrics in metric_set()\n\nThe implementation for metrics differ slightly depending on whether you are implementing a numeric, class, or class probability metric. Examples for numeric and classification metrics are given below. We would encourage you to look into the implementation of roc_auc() after reading this vignette if you want to work on a class probability metric."
  },
  {
    "objectID": "content/learn/develop/metrics/index.html#numeric-example-mse",
    "href": "content/learn/develop/metrics/index.html#numeric-example-mse",
    "title": "Custom performance metrics",
    "section": "Numeric example: MSE",
    "text": "Numeric example: MSE\nMean squared error (sometimes MSE or from here on, mse()) is a numeric metric that measures the average of the squared errors. Numeric metrics are generally the simplest to create with yardstick, as they do not have multiclass implementations. The formula for mse() is:\n\\[ MSE = \\frac{1}{N} \\sum_{i=1}^{N} (truth_i - estimate_i) ^ 2 = mean( (truth - estimate) ^ 2) \\]\nAll metrics should have a data frame version, and a vector version. The data frame version here will be named mse(), and the vector version will be mse_vec().\n\nVector implementation\nTo start, create the vector version. Generally, all metrics have the same arguments unless the metric requires an extra parameter (such as beta in f_meas()). To create the vector function, you need to do two things:\n\nCreate an internal implementation function, mse_impl().\nPass on that implementation function to metric_vec_template().\n\nBelow, mse_impl() contains the actual implementation of the metric, and takes truth and estimate as arguments along with any metric specific arguments.\nThe yardstick function metric_vec_template() accepts the implementation function along with the other arguments to mse_vec() and actually executes mse_impl(). Additionally, it has a cls argument to specify the allowed class type of truth and estimate. If the classes are the same, a single character class can be passed, and if they are different a character vector of length 2 can be supplied.\nThe metric_vec_template() helper handles the removal of NA values in your metric, so your implementation function does not have to worry about them. It performs type checking using cls and also checks that the estimator is valid, the second of which is covered in the classification example. This way, all you have to worry about is the core implementation.\n\nlibrary(tidymodels)\n\nmse_vec <- function(truth, estimate, na_rm = TRUE, ...) {\n  \n  mse_impl <- function(truth, estimate) {\n    mean((truth - estimate) ^ 2)\n  }\n  \n  metric_vec_template(\n    metric_impl = mse_impl,\n    truth = truth, \n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"numeric\",\n    ...\n  )\n  \n}\n\nAt this point, you’ve created the vector version of the mean squared error metric.\n\ndata(\"solubility_test\")\n\nmse_vec(\n  truth = solubility_test$solubility, \n  estimate = solubility_test$prediction\n)\n#> Warning: `metric_vec_template()` was deprecated in yardstick 1.2.0.\n#> ℹ Please use `check_numeric_metric()`, `check_class_metric()`,\n#>   `check_class_metric()`, `yardstick_remove_missing()`, and\n#>   `yardstick_any_missing()` instead.\n#> [1] 0.5214438\n\nIntelligent error handling is immediately available.\n\nmse_vec(truth = \"apple\", estimate = 1)\n#> Error in `validate_class()`:\n#> ! `truth` should be a numeric but a character was supplied.\n\nmse_vec(truth = 1, estimate = factor(\"xyz\"))\n#> Error in `validate_class()`:\n#> ! `estimate` should be a numeric but a factor was supplied.\n\nNA values are removed if na_rm = TRUE (the default). If na_rm = FALSE and any NA values are detected, then the metric automatically returns NA.\n\n# NA values removed\nmse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5))\n#> [1] 0.01\n\n# NA returned\nmse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5), na_rm = FALSE)\n#> [1] NA\n\n\n\nData frame implementation\nThe data frame version of the metric should be fairly simple. It is a generic function with a data.frame method that calls the yardstick helper, metric_summarizer(), and passes along the mse_vec() function to it along with versions of truth and estimate that have been wrapped in rlang::enquo() and then unquoted with !! so that non-standard evaluation can be supported.\n\nlibrary(rlang)\n\nmse <- function(data, ...) {\n  UseMethod(\"mse\")\n}\n\nmse <- new_numeric_metric(mse, direction = \"minimize\")\n\nmse.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {\n  \n  metric_summarizer(\n    metric_nm = \"mse\",\n    metric_fn = mse_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    na_rm = na_rm,\n    ...\n  )\n  \n}\n\nAnd that’s it. The yardstick package handles the rest with an internal call to summarise().\n\nmse(solubility_test, truth = solubility, estimate = prediction)\n\n# Error handling\nmse(solubility_test, truth = solubility, estimate = factor(\"xyz\"))\n\nLet’s test it out on a grouped data frame.\n\nlibrary(dplyr)\n\nset.seed(1234)\nsize <- 100\ntimes <- 10\n\n# create 10 resamples\nsolubility_resampled <- bind_rows(\n  replicate(\n    n = times,\n    expr = sample_n(solubility_test, size, replace = TRUE),\n    simplify = FALSE\n  ),\n  .id = \"resample\"\n)\n\nsolubility_resampled %>%\n  group_by(resample) %>%\n  mse(solubility, prediction)\n#> Warning: `metric_summarizer()` was deprecated in yardstick 1.2.0.\n#> ℹ Please use `numeric_metric_summarizer()`,\n#>   `class_metric_summarizer()`, or `prob_metric_summarizer()` instead.\n#> # A tibble: 10 × 4\n#>    resample .metric .estimator .estimate\n#>    <chr>    <chr>   <chr>          <dbl>\n#>  1 1        mse     standard       0.512\n#>  2 10       mse     standard       0.454\n#>  3 2        mse     standard       0.513\n#>  4 3        mse     standard       0.414\n#>  5 4        mse     standard       0.543\n#>  6 5        mse     standard       0.456\n#>  7 6        mse     standard       0.652\n#>  8 7        mse     standard       0.642\n#>  9 8        mse     standard       0.404\n#> 10 9        mse     standard       0.479"
  },
  {
    "objectID": "content/learn/develop/metrics/index.html#class-example-miss-rate",
    "href": "content/learn/develop/metrics/index.html#class-example-miss-rate",
    "title": "Custom performance metrics",
    "section": "Class example: miss rate",
    "text": "Class example: miss rate\nMiss rate is another name for the false negative rate, and is a classification metric in the same family as sens() and spec(). It follows the formula:\n\\[ miss\\_rate = \\frac{FN}{FN + TP} \\]\nThis metric, like other classification metrics, is more easily computed when expressed as a confusion matrix. As you will see in the example, you can achieve this with a call to base::table(estimate, truth) which correctly puts the “correct” result in the columns of the confusion matrix.\nClassification metrics are more complicated than numeric ones because you have to think about extensions to the multiclass case. For now, let’s start with the binary case.\n\nVector implementation\nThe vector implementation for classification metrics initially has the same setup as numeric metrics, but has an additional argument, estimator that determines the type of estimator to use (binary or some kind of multiclass implementation or averaging). This argument is auto-selected for the user, so default it to NULL. Additionally, pass it along to metric_vec_template() so that it can check the provided estimator against the classes of truth and estimate to see if they are allowed.\n\n# Logic for `event_level`\nevent_col <- function(xtab, event_level) {\n  if (identical(event_level, \"first\")) {\n    colnames(xtab)[[1]]\n  } else {\n    colnames(xtab)[[2]]\n  }\n}\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  estimator <- finalize_estimator(truth, estimator)\n  \n  miss_rate_impl <- function(truth, estimate) {\n    # Create \n    xtab <- table(estimate, truth)\n    col <- event_col(xtab, event_level)\n    col2 <- setdiff(colnames(xtab), col)\n    \n    tp <- xtab[col, col]\n    fn <- xtab[col2, col]\n    \n    fn / (fn + tp)\n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\nAnother change from the numeric metric is that a call to finalize_estimator() is made. This is the infrastructure that auto-selects the type of estimator to use.\n\ndata(\"two_class_example\")\nmiss_rate_vec(two_class_example$truth, two_class_example$predicted)\n#> [1] 0.120155\n\nWhat happens if you try and pass in a multiclass result?\n\ndata(\"hpc_cv\")\nfold1 <- filter(hpc_cv, Resample == \"Fold01\")\nmiss_rate_vec(fold1$obs, fold1$pred)\n#>          F          M          L \n#> 0.06214689 0.00000000 0.00000000\n\nThis isn’t great, as currently multiclass miss_rate() isn’t supported and it would have been better to throw an error if the estimator was not \"binary\". Currently, finalize_estimator() uses its default implementation which selected \"macro\" as the estimator since truth was a factor with more than 2 classes. When we implement multiclass averaging, this is what you want, but if your metric only works with a binary implementation (or has other specialized multiclass versions), you might want to guard against this.\nTo fix this, a generic counterpart to finalize_estimator(), called finalize_estimator_internal(), exists that helps you restrict the input types. If you provide a method to finalize_estimator_internal() where the method name is the same as your metric name, and then set the metric_class argument in finalize_estimator() to be the same thing, you can control how the auto-selection of the estimator is handled.\nDon’t worry about the metric_dispatcher argument. This is handled for you and just exists as a dummy argument to dispatch off of.\nIt is also good practice to call validate_estimator() which handles the case where a user passed in the estimator themselves. This validates that the supplied estimator is one of the allowed types and error otherwise.\n\nfinalize_estimator_internal.miss_rate <- function(metric_dispatcher, x, estimator) {\n  \n  validate_estimator(estimator, estimator_override = \"binary\")\n  if (!is.null(estimator)) {\n    return(estimator)\n  }\n  \n  lvls <- levels(x)\n  if (length(lvls) > 2) {\n    stop(\"A multiclass `truth` input was provided, but only `binary` is supported.\")\n  } \n  \"binary\"\n}\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  # calls finalize_estimator_internal() internally\n  estimator <- finalize_estimator(truth, estimator, metric_class = \"miss_rate\")\n  \n  miss_rate_impl <- function(truth, estimate) {\n    # Create \n    xtab <- table(estimate, truth)\n    col <- event_col(xtab, event_level)\n    col2 <- setdiff(colnames(xtab), col)\n    \n    tp <- xtab[col, col]\n    fn <- xtab[col2, col]\n    \n    fn / (fn + tp)\n    \n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\n# Error thrown by our custom handler\n# miss_rate_vec(fold1$obs, fold1$pred)\n\n# Error thrown by validate_estimator()\n# miss_rate_vec(fold1$obs, fold1$pred, estimator = \"macro\")\n\n\n\nSupporting multiclass miss rate\nLike many other classification metrics such as precision() or recall(), miss rate does not have a natural multiclass extension, but one can be created using methods such as macro, weighted macro, and micro averaging. If you have not, I encourage you to read vignette(\"multiclass\", \"yardstick\") for more information about how these methods work.\nGenerally, they require more effort to get right than the binary case, especially if you want to have a performant version. Luckily, a somewhat standard template is used in yardstick and can be used here as well.\nLet’s first remove the “binary” restriction we created earlier.\n\nrm(finalize_estimator_internal.miss_rate)\n\nThe main changes below are:\n\nThe binary implementation is moved to miss_rate_binary().\nmiss_rate_estimator_impl() is a helper function for switching between binary and multiclass implementations. It also applies the weighting required for multiclass estimators. It is called from miss_rate_impl() and also accepts the estimator argument using R’s function scoping rules.\nmiss_rate_multiclass() provides the implementation for the multiclass case. It calculates the true positive and false negative values as vectors with one value per class. For the macro case, it returns a vector of miss rate calculations, and for micro, it first sums the individual pieces and returns a single miss rate calculation. In the macro case, the vector is then weighted appropriately in miss_rate_estimator_impl() depending on whether or not it was macro or weighted macro.\n\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  # calls finalize_estimator_internal() internally\n  estimator <- finalize_estimator(truth, estimator, metric_class = \"miss_rate\")\n  \n  miss_rate_impl <- function(truth, estimate) {\n    xtab <- table(estimate, truth)\n    # Rather than implement the actual method here, we rely on\n    # an *_estimator_impl() function that can handle binary\n    # and multiclass cases\n    miss_rate_estimator_impl(xtab, estimator, event_level)\n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\n\n# This function switches between binary and multiclass implementations\nmiss_rate_estimator_impl <- function(data, estimator, event_level) {\n  if(estimator == \"binary\") {\n    miss_rate_binary(data, event_level)\n  } else {\n    # Encapsulates the macro, macro weighted, and micro cases\n    wt <- get_weights(data, estimator)\n    res <- miss_rate_multiclass(data, estimator)\n    weighted.mean(res, wt)\n  }\n}\n\n\nmiss_rate_binary <- function(data, event_level) {\n  col <- event_col(data, event_level)\n  col2 <- setdiff(colnames(data), col)\n  \n  tp <- data[col, col]\n  fn <- data[col2, col]\n  \n  fn / (fn + tp)\n}\n\nmiss_rate_multiclass <- function(data, estimator) {\n  \n  # We need tp and fn for all classes individually\n  # we can get this by taking advantage of the fact\n  # that tp + fn = colSums(data)\n  tp <- diag(data)\n  tpfn <- colSums(data)\n  fn <- tpfn - tp\n  \n  # If using a micro estimator, we sum the individual\n  # pieces before performing the miss rate calculation\n  if (estimator == \"micro\") {\n    tp <- sum(tp)\n    fn <- sum(fn)\n  }\n  \n  # return the vector \n  tp / (tp + fn)\n}\n\nFor the macro case, this separation of weighting from the core implementation might seem strange, but there is good reason for it. Some metrics are combinations of other metrics, and it is nice to be able to reuse code when calculating more complex metrics. For example, f_meas() is a combination of recall() and precision(). When calculating a macro averaged f_meas(), the weighting must be applied 1 time, at the very end of the calculation. recall_multiclass() and precision_multiclass() are defined similarly to how miss_rate_multiclass() is defined and returns the unweighted vector of calculations. This means we can directly use this in f_meas(), and then weight everything once at the end of that calculation.\nLet’s try it out now:\n\n# two class\nmiss_rate_vec(two_class_example$truth, two_class_example$predicted)\n#> [1] 0.120155\n\n# multiclass\nmiss_rate_vec(fold1$obs, fold1$pred)\n#> [1] 0.5483506\n\n\nData frame implementation\nLuckily, the data frame implementation is as simple as the numeric case, we just need to add an extra estimator argument and pass that through.\n\nmiss_rate <- function(data, ...) {\n  UseMethod(\"miss_rate\")\n}\n\nmiss_rate <- new_class_metric(miss_rate, direction = \"minimize\")\n\nmiss_rate.data.frame <- function(data, \n                                 truth, \n                                 estimate, \n                                 estimator = NULL, \n                                 na_rm = TRUE, \n                                 event_level = \"first\",\n                                 ...) {\n  metric_summarizer(\n    metric_nm = \"miss_rate\",\n    metric_fn = miss_rate_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    estimator = estimator,\n    na_rm = na_rm,\n    event_level = event_level,\n    ...\n  )\n}\n\n\n# Macro weighted automatically selected\nfold1 %>%\n  miss_rate(obs, pred)\n\n# Switch to micro\nfold1 %>%\n  miss_rate(obs, pred, estimator = \"micro\")\n\n# Macro weighted by resample\nhpc_cv %>%\n  group_by(Resample) %>%\n  miss_rate(obs, pred, estimator = \"macro_weighted\")\n\n# Error handling\nmiss_rate(hpc_cv, obs, VF)"
  },
  {
    "objectID": "content/learn/develop/metrics/index.html#using-custom-metrics",
    "href": "content/learn/develop/metrics/index.html#using-custom-metrics",
    "title": "Custom performance metrics",
    "section": "Using custom metrics",
    "text": "Using custom metrics\nThe metric_set() function validates that all metric functions are of the same metric type by checking the class of the function. If any metrics are not of the right class, metric_set() fails. By using new_numeric_metric() and new_class_metric() in the above custom metrics, they work out of the box without any additional adjustments.\n\nnumeric_mets <- metric_set(mse, rmse)\n\nnumeric_mets(solubility_test, solubility, prediction)\n#> # A tibble: 2 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mse     standard       0.521\n#> 2 rmse    standard       0.722"
  },
  {
    "objectID": "content/learn/develop/metrics/index.html#session-information",
    "href": "content/learn/develop/metrics/index.html#session-information",
    "title": "Custom performance metrics",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang      * 1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/develop/models/index.html",
    "href": "content/learn/develop/models/index.html",
    "title": "How to build a parsnip model",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: mda, modeldata, and tidymodels.\nThe parsnip package constructs models and predictions by representing those actions in expressions. There are a few reasons for this:\n\nIt eliminates a lot of duplicate code.\nSince the expressions are not evaluated until fitting, it eliminates many package dependencies.\n\nA parsnip model function is itself very general. For example, the logistic_reg() function itself doesn’t have any model code within it. Instead, each model function is associated with one or more computational engines. These might be different R packages or some function in another language (that can be evaluated by R).\nThis article describes the process of creating a new model function. Before proceeding, take a minute and read our guidelines on creating modeling packages to understand the general themes and conventions that we use."
  },
  {
    "objectID": "content/learn/develop/models/index.html#an-example-model",
    "href": "content/learn/develop/models/index.html#an-example-model",
    "title": "How to build a parsnip model",
    "section": "An example model",
    "text": "An example model\nAs an example, we’ll create a function for mixture discriminant analysis. There are a few packages that implement this but we’ll focus on mda::mda:\n\nstr(mda::mda)\n#> function (formula = formula(data), data = sys.frame(sys.parent()), subclasses = 3, \n#>     sub.df = NULL, tot.df = NULL, dimension = sum(subclasses) - 1, eps = 100 * \n#>         .Machine$double.eps, iter = 5, weights = mda.start(x, g, subclasses, \n#>         trace, ...), method = polyreg, keep.fitted = (n * dimension < 5000), \n#>     trace = FALSE, ...)\n\nThe main hyperparameter is the number of subclasses. We’ll name our function discrim_mixture."
  },
  {
    "objectID": "content/learn/develop/models/index.html#aspects-of-models",
    "href": "content/learn/develop/models/index.html#aspects-of-models",
    "title": "How to build a parsnip model",
    "section": "Aspects of models",
    "text": "Aspects of models\nBefore proceeding, it helps to to review how parsnip categorizes models:\n\nThe model type is related to the structural aspect of the model. For example, the model type linear_reg represents linear models (slopes and intercepts) that model a numeric outcome. Other model types in the package are nearest_neighbor, decision_tree, and so on.\nWithin a model type is the mode, related to the modeling goal. Currently the two modes in the package are regression and classification. Some models have methods for both models (e.g. nearest neighbors) while others have only a single mode (e.g. logistic regression).\nThe computation engine is a combination of the estimation method and the implementation. For example, for linear regression, one engine is \"lm\" which uses ordinary least squares analysis via the lm() function. Another engine is \"stan\" which uses the Stan infrastructure to estimate parameters using Bayes rule.\n\nWhen adding a model into parsnip, the user has to specify which modes and engines are used. The package also enables users to add a new mode or engine to an existing model."
  },
  {
    "objectID": "content/learn/develop/models/index.html#the-general-process",
    "href": "content/learn/develop/models/index.html#the-general-process",
    "title": "How to build a parsnip model",
    "section": "The general process",
    "text": "The general process\nThe parsnip package stores information about the models in an internal environment object. The environment can be accessed via the function get_model_env(). The package includes a variety of functions that can get or set the different aspects of the models.\nIf you are adding a new model from your own package, you can use these functions to add new entries into the model environment.\n\nStep 1. Register the model, modes, and arguments\nWe will add the MDA model using the model type discrim_mixture. Since this is a classification method, we only have to register a single mode:\n\nlibrary(tidymodels)\nset_new_model(\"discrim_mixture\")\nset_model_mode(model = \"discrim_mixture\", mode = \"classification\")\nset_model_engine(\n  \"discrim_mixture\", \n  mode = \"classification\", \n  eng = \"mda\"\n)\nset_dependency(\"discrim_mixture\", eng = \"mda\", pkg = \"mda\")\n\nThese functions should silently finish. There is also a function that can be used to show what aspects of the model have been added to parsnip:\n\nshow_model_info(\"discrim_mixture\")\n#> Information for `discrim_mixture`\n#>  modes: unknown, classification \n#> \n#>  engines: \n#>    classification: mdaNA\n#> \n#> ¹The model can use case weights.\n#> \n#>  no registered arguments.\n#> \n#>  no registered fit modules.\n#> \n#>  no registered prediction modules.\n\nThe next step would be to declare the main arguments to the model. These are declared independent of the mode. To specify the argument, there are a few slots to fill in:\n\nThe name that parsnip uses for the argument. In general, we try to use non-jargony names for arguments (e.g. “penalty” instead of “lambda” for regularized regression). We recommend consulting the model argument table available here to see if an existing argument name can be used before creating a new one.\nThe argument name that is used by the underlying modeling function.\nA function reference for a constructor that will be used to generate tuning parameter values. This should be a character vector with a named element called fun that is the constructor function. There is an optional element pkg that can be used to call the function using its namespace. If referencing functions from the dials package, quantitative parameters can have additional arguments in the list for trans and range while qualitative parameters can pass values via this list.\nA logical value for whether the argument can be used to generate multiple predictions for a single R object. For example, for boosted trees, if a model is fit with 10 boosting iterations, many modeling packages allow the model object to make predictions for any iterations less than the one used to fit the model. In general this is not the case so one would use has_submodels = FALSE.\n\nFor mda::mda(), the main tuning parameter is subclasses which we will rewrite as sub_classes.\n\nset_model_arg(\n  model = \"discrim_mixture\",\n  eng = \"mda\",\n  parsnip = \"sub_classes\",\n  original = \"subclasses\",\n  func = list(pkg = \"foo\", fun = \"bar\"),\n  has_submodel = FALSE\n)\nshow_model_info(\"discrim_mixture\")\n#> Information for `discrim_mixture`\n#>  modes: unknown, classification \n#> \n#>  engines: \n#>    classification: mdaNA\n#> \n#> ¹The model can use case weights.\n#> \n#>  arguments: \n#>    mda: \n#>       sub_classes --> subclasses\n#> \n#>  no registered fit modules.\n#> \n#>  no registered prediction modules.\n\n\n\nStep 2. Create the model function\nThis is a fairly simple function that can follow a basic template. The main arguments to our function will be:\n\nThe mode. If the model can do more than one mode, you might default this to “unknown”. In our case, since it is only a classification model, it makes sense to default it to that mode so that the users won’t have to specify it.\nThe argument names (sub_classes here). These should be defaulted to NULL.\n\nA basic version of the function is:\n\ndiscrim_mixture <-\n  function(mode = \"classification\",  sub_classes = NULL) {\n    # Check for correct mode\n    if (mode  != \"classification\") {\n      rlang::abort(\"`mode` should be 'classification'\")\n    }\n    \n    # Capture the arguments in quosures\n    args <- list(sub_classes = rlang::enquo(sub_classes))\n    \n    # Save some empty slots for future parts of the specification\n    new_model_spec(\n      \"discrim_mixture\",\n      args = args,\n      eng_args = NULL,\n      mode = mode,\n      method = NULL,\n      engine = NULL\n    )\n  }\n\nThis is pretty simple since the data are not exposed to this function.\n\n\n\n\n\n\nWarning\n\n\n\nWe strongly suggest favoring rlang::abort() and rlang::warn() over stop() and warning(). The former return better traceback results and have safer defaults for handling call objects.\n\n\n\n\nStep 3. Add a fit module\nNow that parsnip knows about the model, mode, and engine, we can give it the information on fitting the model for our engine. The information needed to fit the model is contained in another list. The elements are:\n\ninterface is a single character value that could be “formula”, “data.frame”, or “matrix”. This defines the type of interface used by the underlying fit function (mda::mda, in this case). This helps the translation of the data to be in an appropriate format for the that function.\nprotect is an optional list of function arguments that should not be changeable by the user. In this case, we probably don’t want users to pass data values to these arguments (until the fit() function is called).\nfunc is the package and name of the function that will be called. If you are using a locally defined function, only fun is required.\ndefaults is an optional list of arguments to the fit function that the user can change, but whose defaults can be set here. This isn’t needed in this case, but is described later in this document.\n\nFor the first engine:\n\nset_fit(\n  model = \"discrim_mixture\",\n  eng = \"mda\",\n  mode = \"classification\",\n  value = list(\n    interface = \"formula\",\n    protect = c(\"formula\", \"data\"),\n    func = c(pkg = \"mda\", fun = \"mda\"),\n    defaults = list()\n  )\n)\n\nshow_model_info(\"discrim_mixture\")\n#> Information for `discrim_mixture`\n#>  modes: unknown, classification \n#> \n#>  engines: \n#>    classification: mda\n#> \n#> ¹The model can use case weights.\n#> \n#>  arguments: \n#>    mda: \n#>       sub_classes --> subclasses\n#> \n#>  fit modules:\n#>  engine           mode\n#>     mda classification\n#> \n#>  no registered prediction modules.\n\nWe also set up the information on how the predictors should be handled. These options ensure that the data that parsnip gives to the underlying model allows for a model fit that is as similar as possible to what it would have produced directly.\n\npredictor_indicators describes whether and how to create indicator/dummy variables from factor predictors. There are three options: \"none\" (do not expand factor predictors), \"traditional\" (apply the standard model.matrix() encodings), and \"one_hot\" (create the complete set including the baseline level for all factors).\ncompute_intercept controls whether model.matrix() should include the intercept in its formula. This affects more than the inclusion of an intercept column. With an intercept, model.matrix() computes dummy variables for all but one factor level. Without an intercept, model.matrix() computes a full set of indicators for the first factor variable, but an incomplete set for the remainder.\nremove_intercept removes the intercept column after model.matrix() is finished. This can be useful if the model function (e.g. lm()) automatically generates an intercept.\nallow_sparse_x specifies whether the model can accommodate a sparse representation for predictors during fitting and tuning.\n\n\nset_encoding(\n  model = \"discrim_mixture\",\n  eng = \"mda\",\n  mode = \"classification\",\n  options = list(\n    predictor_indicators = \"traditional\",\n    compute_intercept = TRUE,\n    remove_intercept = TRUE,\n    allow_sparse_x = FALSE\n  )\n)\n\n\n\nStep 4. Add modules for prediction\nSimilar to the fitting module, we specify the code for making different types of predictions. To make hard class predictions, the class object contains the details. The elements of the list are:\n\npre and post are optional functions that can preprocess the data being fed to the prediction code and to postprocess the raw output of the predictions. These won’t be needed for this example, but a section below has examples of how these can be used when the model code is not easy to use. If the data being predicted has a simple type requirement, you can avoid using a pre function with the args below.\nfunc is the prediction function (in the same format as above). In many cases, packages have a predict method for their model’s class but this is typically not exported. In this case (and the example below), it is simple enough to make a generic call to predict() with no associated package.\nargs is a list of arguments to pass to the prediction function. These will most likely be wrapped in rlang::expr() so that they are not evaluated when defining the method. For mda, the code would be predict(object, newdata, type = \"class\"). What is actually given to the function is the parsnip model fit object, which includes a sub-object called fit() that houses the mda model object. If the data need to be a matrix or data frame, you could also use newdata = quote(as.data.frame(newdata)) or similar.\n\nThe parsnip prediction code will expect the result to be an unnamed character string or factor. This will be coerced to a factor with the same levels as the original data.\nTo add this method to the model environment, a similar set() function is used:\n\nclass_info <- \n  list(\n    pre = NULL,\n    post = NULL,\n    func = c(fun = \"predict\"),\n    args =\n      # These lists should be of the form:\n      # {predict.mda argument name} = {values provided from parsnip objects}\n      list(\n        # We don't want the first two arguments evaluated right now\n        # since they don't exist yet. `type` is a simple object that\n        # doesn't need to have its evaluation deferred. \n        object = quote(object$fit),\n        newdata = quote(new_data),\n        type = \"class\"\n      )\n  )\n\nset_pred(\n  model = \"discrim_mixture\",\n  eng = \"mda\",\n  mode = \"classification\",\n  type = \"class\",\n  value = class_info\n)\n\nA similar call can be used to define the class probability module (if they can be computed). The format is identical to the class module but the output is expected to be a tibble with columns for each factor level.\nAs an example of the post function, the data frame created by mda:::predict.mda() will be converted to a tibble. The arguments are x (the raw results coming from the predict method) and object (the parsnip model fit object). The latter has a sub-object called lvl which is a character string of the outcome’s factor levels (if any).\nWe register the probability module. There is a template function that makes this slightly easier to format the objects:\n\nprob_info <-\n  pred_value_template(\n    post = function(x, object) {\n      tibble::as_tibble(x)\n    },\n    func = c(fun = \"predict\"),\n    # Now everything else is put into the `args` slot\n    object = quote(object$fit),\n    newdata = quote(new_data),\n    type = \"posterior\"\n  )\n\nset_pred(\n  model = \"discrim_mixture\",\n  eng = \"mda\",\n  mode = \"classification\",\n  type = \"prob\",\n  value = prob_info\n)\n\nshow_model_info(\"discrim_mixture\")\n#> Information for `discrim_mixture`\n#>  modes: unknown, classification \n#> \n#>  engines: \n#>    classification: mda\n#> \n#> ¹The model can use case weights.\n#> \n#>  arguments: \n#>    mda: \n#>       sub_classes --> subclasses\n#> \n#>  fit modules:\n#>  engine           mode\n#>     mda classification\n#> \n#>  prediction modules:\n#>              mode engine     methods\n#>    classification    mda class, prob\n\nIf this model could be used for regression situations, we could also add a “numeric” module. For pred, the model requires an unnamed numeric vector output (usually).\nExamples are here and here.\n\n\nDoes it work?\nAs a developer, one thing that may come in handy is the translate() function. This will tell you what the model’s eventual syntax will be.\nFor example:\n\ndiscrim_mixture(sub_classes = 2) %>%\n  translate(engine = \"mda\")\n#> discrim mixture Model Specification (classification)\n#> \n#> Main Arguments:\n#>   sub_classes = 2\n#> \n#> Computational engine: mda \n#> \n#> Model fit template:\n#> mda::mda(formula = missing_arg(), data = missing_arg(), subclasses = 2)\n\nLet’s try it on a data set from the modeldata package:\n\ndata(\"two_class_dat\", package = \"modeldata\")\nset.seed(4622)\nexample_split <- initial_split(two_class_dat, prop = 0.99)\nexample_train <- training(example_split)\nexample_test  <-  testing(example_split)\n\nmda_spec <- discrim_mixture(sub_classes = 2) %>% \n  set_engine(\"mda\")\n\nmda_fit <- mda_spec %>%\n  fit(Class ~ ., data = example_train, engine = \"mda\")\nmda_fit\n#> parsnip model object\n#> \n#> Call:\n#> mda::mda(formula = Class ~ ., data = data, subclasses = ~2)\n#> \n#> Dimension: 2 \n#> \n#> Percent Between-Group Variance Explained:\n#>     v1     v2 \n#>  82.63 100.00 \n#> \n#> Degrees of Freedom (per dimension): 3 \n#> \n#> Training Misclassification Error: 0.17241 ( N = 783 )\n#> \n#> Deviance: 671.391\n\npredict(mda_fit, new_data = example_test, type = \"prob\") %>%\n  bind_cols(example_test %>% select(Class))\n#> # A tibble: 8 × 3\n#>   .pred_Class1 .pred_Class2 Class \n#>          <dbl>        <dbl> <fct> \n#> 1       0.679         0.321 Class1\n#> 2       0.690         0.310 Class1\n#> 3       0.384         0.616 Class2\n#> 4       0.300         0.700 Class1\n#> 5       0.0262        0.974 Class2\n#> 6       0.405         0.595 Class2\n#> 7       0.793         0.207 Class1\n#> 8       0.0949        0.905 Class2\n\npredict(mda_fit, new_data = example_test) %>% \n bind_cols(example_test %>% select(Class))\n#> # A tibble: 8 × 2\n#>   .pred_class Class \n#>   <fct>       <fct> \n#> 1 Class1      Class1\n#> 2 Class1      Class1\n#> 3 Class2      Class2\n#> 4 Class2      Class1\n#> 5 Class2      Class2\n#> 6 Class2      Class2\n#> 7 Class1      Class1\n#> 8 Class2      Class2"
  },
  {
    "objectID": "content/learn/develop/models/index.html#add-an-engine",
    "href": "content/learn/develop/models/index.html#add-an-engine",
    "title": "How to build a parsnip model",
    "section": "Add an engine",
    "text": "Add an engine\nThe process for adding an engine to an existing model is almost the same as building a new model but simpler with fewer steps. You only need to add the engine-specific aspects of the model. For example, if we wanted to fit a linear regression model using M-estimation, we could only add a new engine. The code for the rlm() function in MASS is pretty similar to lm(), so we can copy that code and change the package/function names:\n\nset_model_engine(\"linear_reg\", \"regression\", eng = \"rlm\")\nset_dependency(\"linear_reg\", eng = \"rlm\", pkg = \"MASS\")\n\nset_fit(\n  model = \"linear_reg\",\n  eng = \"rlm\",\n  mode = \"regression\",\n  value = list(\n    interface = \"formula\",\n    protect = c(\"formula\", \"data\", \"weights\"),\n    func = c(pkg = \"MASS\", fun = \"rlm\"),\n    defaults = list()\n  )\n)\n\nset_encoding(\n  model = \"linear_reg\",\n  eng = \"rlm\",\n  mode = \"regression\",\n  options = list(\n    predictor_indicators = \"traditional\",\n    compute_intercept = TRUE,\n    remove_intercept = TRUE,\n    allow_sparse_x = FALSE\n  )\n)\n\nset_pred(\n  model = \"linear_reg\",\n  eng = \"rlm\",\n  mode = \"regression\",\n  type = \"numeric\",\n  value = list(\n    pre = NULL,\n    post = NULL,\n    func = c(fun = \"predict\"),\n    args =\n      list(\n        object = expr(object$fit),\n        newdata = expr(new_data),\n        type = \"response\"\n      )\n  )\n)\n\n# testing:\nlinear_reg() %>% \n  set_engine(\"rlm\") %>% \n  fit(mpg ~ ., data = mtcars)\n#> parsnip model object\n#> \n#> Call:\n#> rlm(formula = mpg ~ ., data = data)\n#> Converged in 8 iterations\n#> \n#> Coefficients:\n#> (Intercept)         cyl        disp          hp        drat          wt \n#> 17.82250038 -0.27878615  0.01593890 -0.02536343  0.46391132 -4.14355431 \n#>        qsec          vs          am        gear        carb \n#>  0.65307203  0.24975463  1.43412689  0.85943158 -0.01078897 \n#> \n#> Degrees of freedom: 32 total; 21 residual\n#> Scale estimate: 2.15"
  },
  {
    "objectID": "content/learn/develop/models/index.html#add-parsnip-models-to-another-package",
    "href": "content/learn/develop/models/index.html#add-parsnip-models-to-another-package",
    "title": "How to build a parsnip model",
    "section": "Add parsnip models to another package",
    "text": "Add parsnip models to another package\nThe process here is almost the same. All of the previous functions are still required but their execution is a little different.\nFor parsnip to register them, that package must already be loaded. For this reason, it makes sense to have parsnip in the “Depends” category.\nThe first difference is that the functions that define the model must be inside of a wrapper function that is called when your package is loaded. For our example here, this might look like:\n\nmake_discrim_mixture_mda <- function() {\n  parsnip::set_new_model(\"discrim_mixture\")\n\n  parsnip::set_model_mode(\"discrim_mixture\", \"classification\")\n\n  # and so one...\n}\n\nThis function is then executed when your package is loaded:\n\n.onLoad <- function(libname, pkgname) {\n  # This defines discrim_mixture in the model database\n  make_discrim_mixture_mda()\n}\n\nFor an example package that uses parsnip definitions, take a look at the discrim package.\n\n\n\n\n\n\nWarning\n\n\n\nTo use a new model and/or engine in the broader tidymodels infrastructure, we recommend your model definition declarations (e.g. set_new_model() and similar) reside in a package. If these definitions are in a script only, the new model may not work with the tune package, for example for parallel processing.\n\n\nIt is also important for parallel processing support to list the home package as a dependency. If the discrim_mixture() function lived in a package called mixedup, include the line:\nset_dependency(\"discrim_mixture\", eng = \"mda\", pkg = \"mixedup\")\nParallel processing requires this explicit dependency setting. When parallel worker processes are created, there is heterogeneity across technologies regarding which packages are loaded. Multicore methods on macOS and Linux will load all of the packages that were loaded in the main R process. However, parallel processing using psock clusters have no additional packages loaded. If the home package for a parsnip model is not loaded in the worker processes, the model will not have an entry in parsnip’s internal database (and produce an error)."
  },
  {
    "objectID": "content/learn/develop/models/index.html#your-model-tuning-parameters-and-you",
    "href": "content/learn/develop/models/index.html#your-model-tuning-parameters-and-you",
    "title": "How to build a parsnip model",
    "section": "Your model, tuning parameters, and you",
    "text": "Your model, tuning parameters, and you\nThe tune package can be used to find reasonable values of model arguments via tuning. There are some S3 methods that are useful to define for your model. discrim_mixture() has one main tuning parameter: sub_classes. To work with tune it is helpful (but not required) to use an S3 method called tunable() to define which arguments should be tuned and how values of those arguments should be generated.\ntunable() takes the model specification as its argument and returns a tibble with columns:\n\nname: The name of the argument.\ncall_info: A list that describes how to call a function that returns a dials parameter object.\nsource: A character string that indicates where the tuning value comes from (i.e., a model, a recipe etc.). Here, it is just \"model_spec\".\ncomponent: A character string with more information about the source. For models, this is just the name of the function (e.g. \"discrim_mixture\").\ncomponent_id: A character string to indicate where a unique identifier is for the object. For a model, this is indicates the type of model argument (e.g. “main”).\n\nThe main piece of information that requires some detail is call_info. This is a list column in the tibble. Each element of the list is a list that describes the package and function that can be used to create a dials parameter object.\nFor example, for a nearest-neighbors neighbors parameter, this value is just:\n\ninfo <- list(pkg = \"dials\", fun = \"neighbors\")\n\n# FYI: how it is used under-the-hood: \nnew_param_call <- rlang::call2(.fn = info$fun, .ns = info$pkg)\nrlang::eval_tidy(new_param_call)\n#> # Nearest Neighbors (quantitative)\n#> Range: [1, 10]\n\nFor discrim_mixture(), a dials object is needed that returns an integer that is the number of sub-classes that should be create. We can create a dials parameter function for this:\n\nsub_classes <- function(range = c(1L, 10L), trans = NULL) {\n  new_quant_param(\n    type = \"integer\",\n    range = range,\n    inclusive = c(TRUE, TRUE),\n    trans = trans,\n    label = c(sub_classes = \"# Sub-Classes\"),\n    finalize = NULL\n  )\n}\n\nIf this were in the dials package, we could use:\n\ntunable.discrim_mixture <- function(x, ...) {\n  tibble::tibble(\n    name = c(\"sub_classes\"),\n    call_info = list(list(pkg = NULL, fun = \"sub_classes\")),\n    source = \"model_spec\",\n    component = \"discrim_mixture\",\n    component_id = \"main\"\n  )\n}\n\nOnce this method is in place, the tuning functions can be used:\n\nmda_spec <- \n  discrim_mixture(sub_classes = tune()) %>% \n  set_engine(\"mda\")\n\nset.seed(452)\ncv <- vfold_cv(example_train)\nmda_tune_res <- mda_spec %>%\n  tune_grid(Class ~ ., cv, grid = 4)\nshow_best(mda_tune_res, metric = \"roc_auc\")\n#> # A tibble: 4 × 7\n#>   sub_classes .metric .estimator  mean     n std_err .config             \n#>         <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1           2 roc_auc binary     0.890    10  0.0143 Preprocessor1_Model3\n#> 2           3 roc_auc binary     0.889    10  0.0142 Preprocessor1_Model4\n#> 3           6 roc_auc binary     0.884    10  0.0147 Preprocessor1_Model2\n#> 4           8 roc_auc binary     0.881    10  0.0146 Preprocessor1_Model1"
  },
  {
    "objectID": "content/learn/develop/models/index.html#pro-tips-what-ifs-exceptions-faq-and-minutiae",
    "href": "content/learn/develop/models/index.html#pro-tips-what-ifs-exceptions-faq-and-minutiae",
    "title": "How to build a parsnip model",
    "section": "Pro-tips, what-ifs, exceptions, FAQ, and minutiae",
    "text": "Pro-tips, what-ifs, exceptions, FAQ, and minutiae\nThere are various things that came to mind while developing this resource.\nDo I have to return a simple vector for predict and predict_class?\nPreviously, when discussing the pred information:\n\nFor pred, the model requires an unnamed numeric vector output (usually).\n\nThere are some models (e.g. glmnet, plsr, Cubist, etc.) that can make predictions for different models from the same fitted model object. We want to facilitate that here so, for these cases, the current convention is to return a tibble with the prediction in a column called values and have extra columns for any parameters that define the different sub-models.\nFor example, if I fit a linear regression model via glmnet and get four values of the regularization parameter (lambda):\n\nlinear_reg() %>%\n  set_engine(\"glmnet\", nlambda = 4) %>% \n  fit(mpg ~ ., data = mtcars) %>%\n  multi_predict(new_data = mtcars[1:3, -1])\n\nHowever, the API is still being developed. Currently, there is not an interface in the prediction functions to pass in the values of the parameters to make predictions with (lambda, in this case).\nWhat do I do about how my model handles factors or categorical data?\nSome modeling functions in R create indicator/dummy variables from categorical data when you use a model formula (typically using model.matrix()), and some do not. Some examples of models that do not create indicator variables include tree-based models, naive Bayes models, and multilevel or hierarchical models. The tidymodels ecosystem assumes a model.matrix()-like default encoding for categorical data used in a model formula, but you can change this encoding using set_encoding(). For example, you can set predictor encodings that say, “leave my data alone,” and keep factors as is:\n\nset_encoding(\n  model = \"decision_tree\",\n  eng = \"rpart\",\n  mode = \"regression\",\n  options = list(\n    predictor_indicators = \"none\",\n    compute_intercept = FALSE,\n    remove_intercept = FALSE\n  )\n)\n\n\n\n\n\n\n\nNote\n\n\n\nThere are three options for predictor_indicators: - “none” (do not expand factor predictors) - “traditional” (apply the standard model.matrix() encoding) - “one_hot” (create the complete set including the baseline level for all factors)\n\n\nTo learn more about encoding categorical predictors, check out this blog post.\nWhat is the defaults slot and why do I need it?\nYou might want to set defaults that can be overridden by the user. For example, for logistic regression with glm, it make sense to default family = binomial. However, if someone wants to use a different link function, they should be able to do that. For that model/engine definition, it has:\n\ndefaults = list(family = expr(binomial))\n\nSo that is the default:\n\nlogistic_reg() %>% translate(engine = \"glm\")\n\n# but you can change it:\n\nlogistic_reg() %>%\n  set_engine(\"glm\", family = expr(binomial(link = \"probit\"))) %>% \n  translate()\n\nThat’s what defaults are for.\nNote that we wrapped binomial inside of expr(). If we didn’t, it would substitute the results of executing binomial() inside of the expression (and that’s a mess).\nWhat if I want more complex defaults?\nThe translate function can be used to check values or set defaults once the model’s mode is known. To do this, you can create a model-specific S3 method that first calls the general method (translate.model_spec()) and then makes modifications or conducts error traps.\nFor example, the ranger and randomForest package functions have arguments for calculating importance. One is a logical and the other is a string. Since this is likely to lead to a bunch of frustration and GitHub issues, we can put in a check:\n\n# Simplified version\ntranslate.rand_forest <- function (x, engine, ...){\n  # Run the general method to get the real arguments in place\n  x <- translate.default(x, engine, ...)\n  \n  # Check and see if they make sense for the engine and/or mode:\n  if (x$engine == \"ranger\") {\n    if (any(names(x$method$fit$args) == \"importance\")) \n      if (is.logical(x$method$fit$args$importance)) \n        rlang::abort(\"`importance` should be a character value. See ?ranger::ranger.\")\n  }\n  x\n}\n\nAs another example, nnet::nnet() has an option for the final layer to be linear (called linout). If mode = \"regression\", that should probably be set to TRUE. You couldn’t do this with the args (described above) since you need the function translated first.\nMy model fit requires more than one function call. So….?\nThe best course of action is to write wrapper so that it can be one call. This was the case with xgboost and keras.\nWhy would I preprocess my data?\nThere might be non-trivial transformations that the model prediction code requires (such as converting to a sparse matrix representation, etc.)\nThis would not include making dummy variables and model.matrix stuff. The parsnip infrastructure already does that for you.\nWhy would I post-process my predictions?\nWhat comes back from some R functions may be somewhat… arcane or problematic. As an example, for xgboost, if you fit a multi-class boosted tree, you might expect the class probabilities to come back as a matrix (narrator: they don’t). If you have four classes and make predictions on three samples, you get a vector of 12 probability values. You need to convert these to a rectangular data set.\nAnother example is the predict method for ranger, which encapsulates the actual predictions in a more complex object structure.\nThese are the types of problems that the post-processor will solve.\nAre there other modes?\nNot yet but there will be. For example, it might make sense to have a different mode when doing risk-based modeling via Cox regression models. That would enable different classes of objects and those might be needed since the types of models don’t make direct predictions of the outcome.\nIf you have a suggestion, please add a GitHub issue to discuss it."
  },
  {
    "objectID": "content/learn/develop/models/index.html#session-information",
    "href": "content/learn/develop/models/index.html#session-information",
    "title": "How to build a parsnip model",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  mda        * 0.5-3      2022-05-05 [1] CRAN (R 4.2.0)\n#>  modeldata  * 1.0.1      2022-09-06 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/develop/recipes/index.html",
    "href": "content/learn/develop/recipes/index.html",
    "title": "Create your own recipe step function",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: modeldata and tidymodels.\nThere are many existing recipe steps in packages like recipes, themis, textrecipes, and others. A full list of steps in CRAN packages can be found here. However, you might need to define your own preprocessing operations; this article describes how to do that. If you are looking for good examples of existing steps, we suggest looking at the code for centering or PCA to start.\nFor check operations (e.g. check_class()), the process is very similar. Notes on this are available at the end of this article.\nThe general process to follow is to:\n\nDefine a step constructor function.\nCreate the minimal S3 methods for prep(), bake(), and print().\nOptionally add some extra methods to work with other tidymodels packages, such as tunable() and tidy().\n\nAs an example, we will create a step for converting data into percentiles."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#a-new-step-definition",
    "href": "content/learn/develop/recipes/index.html#a-new-step-definition",
    "title": "Create your own recipe step function",
    "section": "A new step definition",
    "text": "A new step definition\nLet’s create a step that replaces the value of a variable with its percentile from the training set. The example data we’ll use is from the modeldata package:\n\nlibrary(modeldata)\ndata(biomass)\nstr(biomass)\n#> 'data.frame':    536 obs. of  8 variables:\n#>  $ sample  : chr  \"Akhrot Shell\" \"Alabama Oak Wood Waste\" \"Alder\" \"Alfalfa\" ...\n#>  $ dataset : chr  \"Training\" \"Training\" \"Training\" \"Training\" ...\n#>  $ carbon  : num  49.8 49.5 47.8 45.1 46.8 ...\n#>  $ hydrogen: num  5.64 5.7 5.8 4.97 5.4 5.75 5.99 5.7 5.5 5.9 ...\n#>  $ oxygen  : num  42.9 41.3 46.2 35.6 40.7 ...\n#>  $ nitrogen: num  0.41 0.2 0.11 3.3 1 2.04 2.68 1.7 0.8 1.2 ...\n#>  $ sulfur  : num  0 0 0.02 0.16 0.02 0.1 0.2 0.2 0 0.1 ...\n#>  $ HHV     : num  20 19.2 18.3 18.2 18.4 ...\n\nbiomass_tr <- biomass[biomass$dataset == \"Training\",]\nbiomass_te <- biomass[biomass$dataset == \"Testing\",]\n\nTo illustrate the transformation with the carbon variable, note the training set distribution of this variable with a vertical line below for the first value of the test set.\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nggplot(biomass_tr, aes(x = carbon)) + \n  geom_histogram(binwidth = 5, col = \"blue\", fill = \"blue\", alpha = .5) + \n  geom_vline(xintercept = biomass_te$carbon[1], lty = 2)\n\n\n\n\n\n\n\n\nBased on the training set, 42.1% of the data are less than a value of 46.35. There are some applications where it might be advantageous to represent the predictor values as percentiles rather than their original values.\nOur new step will do this computation for any numeric variables of interest. We will call this new recipe step step_percentile(). The code below is designed for illustration and not speed or best practices. We’ve left out a lot of error trapping that we would want in a real implementation."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#create-the-function",
    "href": "content/learn/develop/recipes/index.html#create-the-function",
    "title": "Create your own recipe step function",
    "section": "Create the function",
    "text": "Create the function\nTo start, there is a user-facing function. Let’s call that step_percentile(). This is just a simple wrapper around a constructor function, which defines the rules for any step object that defines a percentile transformation. We’ll call this constructor step_percentile_new().\nThe function step_percentile() takes the same arguments as your function and simply adds it to a new recipe. The ... signifies the variable selectors that can be used.\n\nstep_percentile <- function(\n  recipe, \n  ..., \n  role = NA, \n  trained = FALSE, \n  ref_dist = NULL,\n  options = list(probs = (0:100)/100, names = TRUE),\n  skip = FALSE,\n  id = rand_id(\"percentile\")\n  ) {\n\n  ## The variable selectors are not immediately evaluated by using\n  ##  the `quos()` function in `rlang`. `ellipse_check()` captures \n  ##  the values and also checks to make sure that they are not empty.  \n  terms <- ellipse_check(...) \n\n  add_step(\n    recipe, \n    step_percentile_new(\n      terms = terms, \n      trained = trained,\n      role = role, \n      ref_dist = ref_dist,\n      options = options,\n      skip = skip,\n      id = id\n    )\n  )\n}\n\nYou should always keep the first four arguments (recipe though trained) the same as listed above. Some notes:\n\nthe role argument is used when you either 1) create new variables and want their role to be pre-set or 2) replace the existing variables with new values. The latter is what we will be doing and using role = NA will leave the existing role intact.\ntrained is set by the package when the estimation step has been run. You should default your function definition’s argument to FALSE.\nskip is a logical. Whenever a recipe is prepped, each step is trained and then baked. However, there are some steps that should not be applied when a call to bake() is used. For example, if a step is applied to the variables with roles of “outcomes”, these data would not be available for new samples.\nid is a character string that can be used to identify steps in package code. rand_id() will create an ID that has the prefix and a random character sequence.\n\nWe can estimate the percentiles of new data points based on the percentiles from the training set with approx(). Our step_percentile contains a ref_dist object to store these percentiles (pre-computed from the training set in prep()) for later use in bake().\nWe will use stats::quantile() to compute the grid. However, we might also want to have control over the granularity of this grid, so the options argument will be used to define how that calculation is done. We could use the ellipses (aka ...) so that any options passed to step_percentile() that are not one of its arguments will then be passed to stats::quantile(). However, we recommend making a separate list object with the options and use these inside the function because ... is already used to define the variable selection.\nIt is also important to consider if there are any main arguments to the step. For example, for spline-related steps such as step_ns(), users typically want to adjust the argument for the degrees of freedom in the spline (e.g. splines::ns(x, df)). Rather than letting users add df to the options argument:\n\nAllow the important arguments to be main arguments to the step function.\nFollow the tidymodels conventions for naming arguments. Whenever possible, avoid jargon and keep common argument names.\n\nThere are benefits to following these principles (as shown below)."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#initialize-a-new-object",
    "href": "content/learn/develop/recipes/index.html#initialize-a-new-object",
    "title": "Create your own recipe step function",
    "section": "Initialize a new object",
    "text": "Initialize a new object\nNow, the constructor function can be created.\nThe function cascade is:\nstep_percentile() calls recipes::add_step()\n└──> recipes::add_step() calls step_percentile_new()\n    └──> step_percentile_new() calls recipes::step()\nstep() is a general constructor for recipes that mainly makes sure that the resulting step object is a list with an appropriate S3 class structure. Using subclass = \"percentile\" will set the class of new objects to \"step_percentile\".\n\nstep_percentile_new <- \n  function(terms, role, trained, ref_dist, options, skip, id) {\n    step(\n      subclass = \"percentile\", \n      terms = terms,\n      role = role,\n      trained = trained,\n      ref_dist = ref_dist,\n      options = options,\n      skip = skip,\n      id = id\n    )\n  }\n\nThis constructor function should have no default argument values. Defaults should be set in the user-facing step object."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#create-the-prep-method",
    "href": "content/learn/develop/recipes/index.html#create-the-prep-method",
    "title": "Create your own recipe step function",
    "section": "Create the prep method",
    "text": "Create the prep method\nYou will need to create a new prep() method for your step’s class. To do this, three arguments that the method should have are:\nfunction(x, training, info = NULL)\nwhere\n\nx will be the step_percentile object,\ntraining will be a tibble that has the training set data, and\ninfo will also be a tibble that has information on the current set of data available. This information is updated as each step is evaluated by its specific prep() method so it may not have the variables from the original data. The columns in this tibble are variable (the variable name), type (currently either “numeric” or “nominal”), role (defining the variable’s role), and source (either “original” or “derived” depending on where it originated).\n\nYou can define other arguments as well.\nThe first thing that you might want to do in the prep() function is to translate the specification listed in the terms argument to column names in the current data. There is a function called recipes_eval_select() that can be used to obtain this.\n\n\n\n\n\n\nWarning\n\n\n\nThe recipes_eval_select() function is not one you interact with as a typical recipes user, but it is helpful if you develop your own custom recipe steps.\n\n\n\nprep.step_percentile <- function(x, training, info = NULL, ...) {\n  col_names <- recipes_eval_select(x$terms, training, info) \n  # TODO finish the rest of the function\n}\n\nAfter this function call, it is a good idea to check that the selected columns have the appropriate type (e.g. numeric for this example). See recipes::check_type() to do this for basic types.\nOnce we have this, we can save the approximation grid. For the grid, we will use a helper function that enables us to run rlang::exec() to splice in any extra arguments contained in the options list to the call to quantile():\n\nget_train_pctl <- function(x, args = NULL) {\n  res <- rlang::exec(\"quantile\", x = x, !!!args)\n  # Remove duplicate percentile values\n  res[!duplicated(res)]\n}\n\n# For example:\nget_train_pctl(biomass_tr$carbon, list(probs = 0:1))\n#>    0%  100% \n#> 14.61 97.18\nget_train_pctl(biomass_tr$carbon)\n#>     0%    25%    50%    75%   100% \n#> 14.610 44.715 47.100 49.725 97.180\n\nNow, the prep() method can be created:\n\nprep.step_percentile <- function(x, training, info = NULL, ...) {\n  col_names <- recipes_eval_select(x$terms, training, info)\n  ## You can add error trapping for non-numeric data here and so on. \n  \n  ## We'll use the names later so make sure they are available\n  if (x$options$names == FALSE) {\n    rlang::abort(\"`names` should be set to TRUE\")\n  }\n  \n  if (!any(names(x$options) == \"probs\")) {\n    x$options$probs <- (0:100)/100\n  } else {\n    x$options$probs <- sort(unique(x$options$probs))\n  }\n  \n  # Compute percentile grid\n  ref_dist <- purrr::map(training[, col_names],  get_train_pctl, args = x$options)\n\n  ## Use the constructor function to return the updated object. \n  ## Note that `trained` is now set to TRUE\n  \n  step_percentile_new(\n    terms = x$terms, \n    trained = TRUE,\n    role = x$role, \n    ref_dist = ref_dist,\n    options = x$options,\n    skip = x$skip,\n    id = x$id\n  )\n}\n\nWe suggest favoring rlang::abort() and rlang::warn() over stop() and warning(). The former can be used for better traceback results."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#create-the-bake-method",
    "href": "content/learn/develop/recipes/index.html#create-the-bake-method",
    "title": "Create your own recipe step function",
    "section": "Create the bake method",
    "text": "Create the bake method\nRemember that the prep() function does not apply the step to the data; it only estimates any required values such as ref_dist. We will need to create a new method for our step_percentile() class. The minimum arguments for this are\nfunction(object, new_data, ...)\nwhere object is the updated step function that has been through the corresponding prep() code and new_data is a tibble of data to be processed.\nHere is the code to convert the new data to percentiles. The input data (x below) comes in as a numeric vector and the output is a vector of approximate percentiles:\n\npctl_by_approx <- function(x, ref) {\n  # In case duplicates were removed, get the percentiles from\n  # the names of the reference object\n  grid <- as.numeric(gsub(\"%$\", \"\", names(ref))) \n  approx(x = ref, y = grid, xout = x)$y/100\n}\n\nThese computations are done column-wise using purrr::map2_dfc() to modify the new data in-place:\n\nbake.step_percentile <- function(object, new_data, ...) {\n  ## For illustration (and not speed), we will loop through the affected variables\n  ## and do the computations\n  vars <- names(object$ref_dist)\n  \n  new_data[, vars] <-\n    purrr::map2_dfc(new_data[, vars], object$ref_dist, pctl_by_approx)\n  \n  ## Always convert to tibbles on the way out\n  tibble::as_tibble(new_data)\n}\n\n\n\n\n\n\n\nNote\n\n\n\nYou need to import recipes::prep() and recipes::bake() to create your own step function in a package."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#run-the-example",
    "href": "content/learn/develop/recipes/index.html#run-the-example",
    "title": "Create your own recipe step function",
    "section": "Run the example",
    "text": "Run the example\nLet’s use the example data to make sure that it works:\n\nrec_obj <- \n  recipe(HHV ~ ., data = biomass_tr) %>%\n  step_percentile(ends_with(\"gen\")) %>%\n  prep(training = biomass_tr)\n\nbiomass_te %>% select(ends_with(\"gen\")) %>% slice(1:2)\n#>   hydrogen oxygen nitrogen\n#> 1     5.67  47.20     0.30\n#> 2     5.50  48.06     2.85\nbake(rec_obj, biomass_te %>% slice(1:2), ends_with(\"gen\"))\n#> # A tibble: 2 × 3\n#>   hydrogen oxygen nitrogen\n#>      <dbl>  <dbl>    <dbl>\n#> 1     0.45  0.903    0.21 \n#> 2     0.38  0.922    0.928\n\n# Checking to get approximate result: \nmean(biomass_tr$hydrogen <= biomass_te$hydrogen[1])\n#> [1] 0.4517544\nmean(biomass_tr$oxygen   <= biomass_te$oxygen[1])\n#> [1] 0.9013158\n\nThe plot below shows how the original hydrogen percentiles line up with the estimated values:\n\nhydrogen_values <- \n  bake(rec_obj, biomass_te, hydrogen) %>% \n  bind_cols(biomass_te %>% select(original = hydrogen))\n\nggplot(biomass_tr, aes(x = hydrogen)) + \n  # Plot the empirical distribution function of the \n  # hydrogen training set values as a black line\n  stat_ecdf() + \n  # Overlay the estimated percentiles for the new data: \n  geom_point(data = hydrogen_values, \n             aes(x = original, y = hydrogen), \n             col = \"red\", alpha = .5, cex = 2) + \n  labs(x = \"New Hydrogen Values\", y = \"Percentile Based on Training Set\")\n\n\n\n\n\n\n\n\nThese line up very nicely!"
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#custom-check-operations",
    "href": "content/learn/develop/recipes/index.html#custom-check-operations",
    "title": "Create your own recipe step function",
    "section": "Custom check operations",
    "text": "Custom check operations\nThe process here is exactly the same as steps; the internal functions have a similar naming convention:\n\nadd_check() instead of add_step()\ncheck() instead of step(), and so on.\n\nIt is strongly recommended that:\n\nThe operations start with check_ (i.e. check_range() and check_range_new())\nThe check uses rlang::abort(paste0(...)) when the conditions are not met\nThe original data are returned (unaltered) by the check when the conditions are satisfied."
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#other-step-methods",
    "href": "content/learn/develop/recipes/index.html#other-step-methods",
    "title": "Create your own recipe step function",
    "section": "Other step methods",
    "text": "Other step methods\nThere are a few other S3 methods that can be created for your step function. They are not required unless you plan on using your step in the broader tidymodels package set.\n\nA print method\nIf you don’t add a print method for step_percentile, it will still print but it will be printed as a list of (potentially large) objects and look a bit ugly. The recipes package contains a helper function called printer() that should be useful in most cases. We are using it here for the custom print method for step_percentile. It requires the original terms specification and the column names this specification is evaluated to by prep(). For the former, our step object is structured so that the list object ref_dist has the names of the selected variables:\n\nprint.step_percentile <-\n  function(x, width = max(20, options()$width - 35), ...) {\n    cat(\"Percentile transformation on \", sep = \"\")\n    printer(\n      # Names before prep (could be selectors)\n      untr_obj = x$terms,\n      # Names after prep:\n      tr_obj = names(x$ref_dist),\n      # Has it been prepped? \n      trained = x$trained,\n      # An estimate of how many characters to print on a line: \n      width = width\n    )\n    invisible(x)\n  }\n\n# Results before `prep()`:\nrecipe(HHV ~ ., data = biomass_tr) %>%\n  step_percentile(ends_with(\"gen\"))\n#> Recipe\n#> \n#> Inputs:\n#> \n#>       role #variables\n#>    outcome          1\n#>  predictor          7\n#> \n#> Operations:\n#> \n#> Percentile transformation on ends_with(\"gen\")\n\n# Results after `prep()`: \nrec_obj\n#> Recipe\n#> \n#> Inputs:\n#> \n#>       role #variables\n#>    outcome          1\n#>  predictor          7\n#> \n#> Training data contained 456 data points and no missing data.\n#> \n#> Operations:\n#> \n#> Percentile transformation on hydrogen, oxygen, nitrogen [trained]\n\n\n\nMethods for declaring required packages\nSome recipe steps use functions from other packages. When this is the case, the step_*() function should check to see if the package is installed. The function recipes::recipes_pkg_check() will do this. For example:\n> recipes::recipes_pkg_check(\"some_package\")\n1 package is needed for this step and is not installed. (some_package). Start \na clean R session then run: install.packages(\"some_package\")\nThere is an S3 method that can be used to declare what packages should be loaded when using the step. For a hypothetical step that relies on the hypothetical package, this might look like:\n\nrequired_pkgs.step_hypothetical <- function(x, ...) {\n  c(\"hypothetical\", \"myrecipespkg\")\n}\n\nIn this example, myrecipespkg is the package where the step resides (if it is in a package).\nThe reason to declare what packages should be loaded is parallel processing. When parallel worker processes are created, there is heterogeneity across technologies regarding which packages are loaded. Multicore methods on macOS and Linux load all of the packages that were loaded in the main R process. However, parallel processing using psock clusters have no additional packages loaded. If the home package for a recipe step is not loaded in the worker processes, the prep() methods cannot be found and an error occurs.\nIf this S3 method is used for your step, you can rely on this for checking the installation:\n\nrecipes::recipes_pkg_check(required_pkgs.step_hypothetical())\n\nIf you’d like an example of this in a package, please take a look at the embed or themis package.\n\n\nA tidy method\nThe broom::tidy() method is a means to return information about the step in a usable format. For our step, it would be helpful to know the reference values.\nWhen the recipe has been prepped, those data are in the list ref_dist. A small function can be used to reformat that data into a tibble. It is customary to return the main values as value:\n\nformat_pctl <- function(x) {\n  tibble::tibble(\n    value = unname(x),\n    percentile = as.numeric(gsub(\"%$\", \"\", names(x))) \n  )\n}\n\n# For example: \npctl_step_object <- rec_obj$steps[[1]]\npctl_step_object\n#> Percentile transformation on hydrogen, oxygen, nitrogen [trained]\nformat_pctl(pctl_step_object$ref_dist[[\"hydrogen\"]])\n#> # A tibble: 87 × 2\n#>    value percentile\n#>    <dbl>      <dbl>\n#>  1 0.03           0\n#>  2 0.934          1\n#>  3 1.60           2\n#>  4 2.07           3\n#>  5 2.45           4\n#>  6 2.74           5\n#>  7 3.15           6\n#>  8 3.49           7\n#>  9 3.71           8\n#> 10 3.99           9\n#> # … with 77 more rows\n\nThe tidy method could return these values for each selected column. Before prep(), missing values can be used as placeholders.\n\ntidy.step_percentile <- function(x, ...) {\n  if (is_trained(x)) {\n    res <- map_dfr(x$ref_dist, format_pctl, .id = \"term\")\n  }\n  else {\n    term_names <- sel2char(x$terms)\n    res <-\n      tibble(\n        terms = term_names,\n        value = rlang::na_dbl,\n        percentile = rlang::na_dbl\n      )\n  }\n  # Always return the step id: \n  res$id <- x$id\n  res\n}\n\ntidy(rec_obj, number = 1)\n#> # A tibble: 274 × 4\n#>    term     value percentile id              \n#>    <chr>    <dbl>      <dbl> <chr>           \n#>  1 hydrogen 0.03           0 percentile_HcwF5\n#>  2 hydrogen 0.934          1 percentile_HcwF5\n#>  3 hydrogen 1.60           2 percentile_HcwF5\n#>  4 hydrogen 2.07           3 percentile_HcwF5\n#>  5 hydrogen 2.45           4 percentile_HcwF5\n#>  6 hydrogen 2.74           5 percentile_HcwF5\n#>  7 hydrogen 3.15           6 percentile_HcwF5\n#>  8 hydrogen 3.49           7 percentile_HcwF5\n#>  9 hydrogen 3.71           8 percentile_HcwF5\n#> 10 hydrogen 3.99           9 percentile_HcwF5\n#> # … with 264 more rows\n\n\n\nMethods for tuning parameters\nThe tune package can be used to find reasonable values of step arguments by model tuning. There are some S3 methods that are useful to define for your step. The percentile example doesn’t really have any tunable parameters, so we will demonstrate using step_poly(), which returns a polynomial expansion of selected columns. Its function definition has the arguments:\n\nargs(step_poly)\n#> function (recipe, ..., role = \"predictor\", trained = FALSE, objects = NULL, \n#>     degree = 2, options = list(), skip = FALSE, id = rand_id(\"poly\")) \n#> NULL\n\nThe argument degree is tunable.\nTo work with tune it is helpful (but not required) to use an S3 method called tunable() to define which arguments should be tuned and how values of those arguments should be generated.\ntunable() takes the step object as its argument and returns a tibble with columns:\n\nname: The name of the argument.\ncall_info: A list that describes how to call a function that returns a dials parameter object.\nsource: A character string that indicates where the tuning value comes from (i.e., a model, a recipe etc.). Here, it is just \"recipe\".\ncomponent: A character string with more information about the source. For recipes, this is just the name of the step (e.g. \"step_poly\").\ncomponent_id: A character string to indicate where a unique identifier is for the object. For recipes, this is just the id value of the step object.\n\nThe main piece of information that requires some detail is call_info. This is a list column in the tibble. Each element of the list is a list that describes the package and function that can be used to create a dials parameter object.\nFor example, for a nearest-neighbors neighbors parameter, this value is just:\n\ninfo <- list(pkg = \"dials\", fun = \"neighbors\")\n\n# FYI: how it is used under-the-hood: \nnew_param_call <- rlang::call2(.fn = info$fun, .ns = info$pkg)\nrlang::eval_tidy(new_param_call)\n#> # Nearest Neighbors (quantitative)\n#> Range: [1, 10]\n\nFor step_poly(), a dials object is needed that returns an integer that is the number of new columns to create. It turns out that there are a few different types of tuning parameters related to degree:\n> lsf.str(\"package:dials\", pattern = \"degree\")\ndegree : function (range = c(1, 3), trans = NULL)  \ndegree_int : function (range = c(1L, 3L), trans = NULL)  \nprod_degree : function (range = c(1L, 2L), trans = NULL)  \nspline_degree : function (range = c(3L, 10L), trans = NULL)  \nLooking at the range values, some return doubles and others return integers. For our problem, degree_int() would be a good choice.\nFor step_poly() the tunable() S3 method could be:\n\ntunable.step_poly <- function (x, ...) {\n  tibble::tibble(\n    name = c(\"degree\"),\n    call_info = list(list(pkg = \"dials\", fun = \"degree_int\")),\n    source = \"recipe\",\n    component = \"step_poly\",\n    component_id = x$id\n  )\n}"
  },
  {
    "objectID": "content/learn/develop/recipes/index.html#session-information",
    "href": "content/learn/develop/recipes/index.html#session-information",
    "title": "Create your own recipe step function",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  modeldata  * 1.0.1      2022-09-06 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/develop/parameters/index.html",
    "href": "content/learn/develop/parameters/index.html",
    "title": "How to create a tuning parameter function",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: dials and scales.\nSome models and recipe steps contain parameters that dials does not know about. You can construct new quantitative and qualitative parameters using new_quant_param() or new_qual_param(), respectively. This article is a guide to creating new parameters."
  },
  {
    "objectID": "content/learn/develop/parameters/index.html#quantitative-parameters",
    "href": "content/learn/develop/parameters/index.html#quantitative-parameters",
    "title": "How to create a tuning parameter function",
    "section": "Quantitative parameters",
    "text": "Quantitative parameters\nAs an example, let’s consider the multivariate adaptive regression spline (MARS) model, which creates nonlinear features from predictors and adds them to a linear regression models. The earth package is an excellent implementation of this method.\nMARS creates an initial set of features and then prunes them back to an appropriate size. This can be done automatically by earth::earth() or the number of final terms can be set by the user. The parsnip function mars() has a parameter called num_terms that defines this.\nWhat if we want to create a parameter for the number of initial terms included in the model. There is no argument in parsnip::mars() for this but we will make one now. The argument name in earth::earth() is nk, which is not very descriptive. Our parameter will be called num_initial_terms.\nWe use the new_quant_param() function since this is a numeric parameter. The main two arguments to a numeric parameter function are range and trans.\nThe range specifies the possible values of the parameter. For our example, a minimal value might be one or two. What is the upper limit? The default in the earth package is\n\nmin(200, max(20, 2 * ncol(x))) + 1\n\nwhere x is the predictor matrix. We often put in values that are either sensible defaults or are minimal enough to work for the majority of data sets. For now, let’s specify an upper limit of 10 but this will be discussed more in the next section.\nThe other argument is trans, which represents a transformation that should be applied to the parameter values when working with them. For example, many regularization methods have a penalty parameter that tends to range between zero and some upper bound (let’s say 1). The effect of going from a penalty value of 0.01 to 0.1 is much more impactful than going from 0.9 to 1.0. In such a case, it might make sense to work with this parameter in transformed units (such as the log, in this example). If new parameter values are generated at random, it helps if they are uniformly simulated in the transformed units and then converted back to the original units.\nThe trans parameter accepts a transformation object from the scales package. For example:\n\nlibrary(scales)\nlsf.str(\"package:scales\", pattern = \"_trans$\")\n#> asn_trans : function ()  \n#> atanh_trans : function ()  \n#> boxcox_trans : function (p, offset = 0)  \n#> compose_trans : function (...)  \n#> date_trans : function ()  \n#> exp_trans : function (base = exp(1))  \n#> hms_trans : function ()  \n#> identity_trans : function ()  \n#> log_trans : function (base = exp(1))  \n#> log10_trans : function ()  \n#> log1p_trans : function ()  \n#> log2_trans : function ()  \n#> logit_trans : function ()  \n#> modulus_trans : function (p, offset = 1)  \n#> probability_trans : function (distribution, ...)  \n#> probit_trans : function ()  \n#> pseudo_log_trans : function (sigma = 1, base = exp(1))  \n#> reciprocal_trans : function ()  \n#> reverse_trans : function ()  \n#> sqrt_trans : function ()  \n#> time_trans : function (tz = NULL)  \n#> yj_trans : function (p)\nscales::log10_trans()\n#> Transformer: log-10 [1e-100, Inf]\n\nA value of NULL means that no transformation should be used.\nA quantitative parameter function should have these two arguments and, in the function body, a call new_quant_param(). There are a few arguments to this function:\n\nlibrary(tidymodels)\nargs(new_quant_param)\n#> function (type = c(\"double\", \"integer\"), range = NULL, inclusive = NULL, \n#>     default = deprecated(), trans = NULL, values = NULL, label = NULL, \n#>     finalize = NULL) \n#> NULL\n\n\nPossible types are double precision and integers. The value of type should agree with the values of range in the function definition.\nIt’s OK for our tuning to include the minimum or maximum, so we’ll use c(TRUE, TRUE) for inclusive. If the value cannot include one end of the range, set one or both of these values to FALSE.\nThe label should be a named character string where the name is the parameter name and the value represents what will be printed automatically.\nfinalize is an argument that can set parts of the range. This is discussed more below.\n\nHere’s an example of a basic quantitative parameter object:\n\nnum_initial_terms <- function(range = c(1L, 10L), trans = NULL) {\n  new_quant_param(\n    type = \"integer\",\n    range = range,\n    inclusive = c(TRUE, TRUE),\n    trans = trans,\n    label = c(num_initial_terms = \"# Initial MARS Terms\"),\n    finalize = NULL\n  )\n}\n\nnum_initial_terms()\n#> # Initial MARS Terms (quantitative)\n#> Range: [1, 10]\n\n# Sample from the parameter:\nset.seed(4832856)\nnum_initial_terms() %>% value_sample(5)\n#> [1]  6  4  9 10  4\n\n\nFinalizing parameters\nIt might be the case that the range of the parameter is unknown. For example, parameters that are related to the number of columns in a data set cannot be exactly specified in the absence of data. In those cases, a placeholder of unknown() can be added. This will force the user to “finalize” the parameter object for their particular data set. Let’s redefine our function with an unknown() value:\n\nnum_initial_terms <- function(range = c(1L, unknown()), trans = NULL) {\n  new_quant_param(\n    type = \"integer\",\n    range = range,\n    inclusive = c(TRUE, TRUE),\n    trans = trans,\n    label = c(num_initial_terms = \"# Initial MARS Terms\"),\n    finalize = NULL\n  )\n}\nnum_initial_terms()\n\n# Can we sample? \nnum_initial_terms() %>% value_sample(5)\n\nThe finalize argument of num_initial_terms() can take a function that uses data to set the range. For example, the package already includes a few functions for finalization:\n\nlsf.str(\"package:dials\", pattern = \"^get_\")\n#> get_batch_sizes : function (object, x, frac = c(1/10, 1/3), ...)  \n#> get_log_p : function (object, x, ...)  \n#> get_n : function (object, x, log_vals = FALSE, ...)  \n#> get_n_frac : function (object, x, log_vals = FALSE, frac = 1/3, ...)  \n#> get_n_frac_range : function (object, x, log_vals = FALSE, frac = c(1/10, 5/10), ...)  \n#> get_p : function (object, x, log_vals = FALSE, ...)  \n#> get_rbf_range : function (object, x, seed = sample.int(10^5, 1), ...)\n\nThese functions generally take a data frame of predictors (in an argument called x) and add the range of the parameter object. Using the formula in the earth package, we might use:\n\nget_initial_mars_terms <- function(object, x) {\n  upper_bound <- min(200, max(20, 2 * ncol(x))) + 1\n  upper_bound <- as.integer(upper_bound)\n  bounds <- range_get(object)\n  bounds$upper <- upper_bound\n  range_set(object, bounds)\n}\n\n# Use the mtcars are the finalize the upper bound: \nnum_initial_terms() %>% get_initial_mars_terms(x = mtcars[, -1])\n#> # Initial MARS Terms (quantitative)\n#> Range: [1, 21]\n\nOnce we add this function to the object, the general finalize() method can be used:\n\nnum_initial_terms <- function(range = c(1L, unknown()), trans = NULL) {\n  new_quant_param(\n    type = \"integer\",\n    range = range,\n    inclusive = c(TRUE, TRUE),\n    trans = trans,\n    label = c(num_initial_terms = \"# Initial MARS Terms\"),\n    finalize = get_initial_mars_terms\n  )\n}\n\nnum_initial_terms() %>% finalize(x = mtcars[, -1])\n#> # Initial MARS Terms (quantitative)\n#> Range: [1, 21]"
  },
  {
    "objectID": "content/learn/develop/parameters/index.html#qualitative-parameters",
    "href": "content/learn/develop/parameters/index.html#qualitative-parameters",
    "title": "How to create a tuning parameter function",
    "section": "Qualitative parameters",
    "text": "Qualitative parameters\nNow let’s look at an example of a qualitative parameter. If a model includes a data aggregation step, we want to allow users to tune how our parameters are aggregated. For example, in embedding methods, possible values might be min, max, mean, sum, or to not aggregate at all (“none”). Since these cannot be put on a numeric scale, they are possible values of a qualitative parameter. We’ll take “character” input (not “logical”), and we must specify the allowed values. By default we won’t aggregate.\n\naggregation <- function(values = c(\"none\", \"min\", \"max\", \"mean\", \"sum\")) {\n  new_qual_param(\n    type = \"character\",\n    values = values,\n    # By default, the first value is selected as default. We'll specify that to\n    # make it clear.\n    default = \"none\",\n    label = c(aggregation = \"Aggregation Method\")\n  )\n}\n\nWithin the dials package, the convention is to have the values contained in a separate vector whose name starts with values_. For example:\n\nvalues_aggregation <- c(\"none\", \"min\", \"max\", \"mean\", \"sum\")\naggregation <- function(values = values_aggregation) {\n  new_qual_param(\n    type = \"character\",\n    values = values,\n    # By default, the first value is selected as default. We'll specify that to\n    # make it clear.\n    default = \"none\",\n    label = c(aggregation = \"Aggregation Method\")\n  )\n}\n\nThis step may not make sense if you are using the function in a script and not keeping it within a package.\nWe can use our aggregation parameters with dials functions.\n\naggregation()\n#> Warning: The `default` argument of `new_qual_param()` is deprecated as of\n#> dials 1.0.1.\n#> Aggregation Method  (qualitative)\n#> 5 possible values include:\n#> 'none', 'min', 'max', 'mean' and 'sum'\naggregation() %>% value_sample(3)\n#> [1] \"min\"  \"sum\"  \"mean\""
  },
  {
    "objectID": "content/learn/develop/parameters/index.html#session-information",
    "href": "content/learn/develop/parameters/index.html#session-information",
    "title": "How to create a tuning parameter function",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  scales     * 1.2.1      2022-08-20 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/develop/broom/index.html",
    "href": "content/learn/develop/broom/index.html",
    "title": "Create your own broom tidier methods",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: generics, tidymodels, tidyverse, and usethis.\nThe broom package provides tools to summarize key information about models in tidy tibble()s. The package provides three verbs, or “tidiers,” to help make model objects easier to work with:\n\ntidy() summarizes information about model components\nglance() reports information about the entire model\naugment() adds information about observations to a dataset\n\nEach of the three verbs above are generic, in that they do not define a procedure to tidy a given model object, but instead redirect to the relevant method implemented to tidy a specific type of model object. The broom package provides methods for model objects from over 100 modeling packages along with nearly all of the model objects in the stats package that comes with base R. However, for maintainability purposes, the broom package authors now ask that requests for new methods be first directed to the parent package (i.e. the package that supplies the model object) rather than to broom. New methods will generally only be integrated into broom in the case that the requester has already asked the maintainers of the model-owning package to implement tidier methods in the parent package.\nWe’d like to make implementing external tidier methods as painless as possible. The general process for doing so is:\n\nre-export the tidier generics\nimplement tidying methods\ndocument the new methods\n\nIn this article, we’ll walk through each of the above steps in detail, giving examples and pointing out helpful functions when possible."
  },
  {
    "objectID": "content/learn/develop/broom/index.html#re-export-the-tidier-generics",
    "href": "content/learn/develop/broom/index.html#re-export-the-tidier-generics",
    "title": "Create your own broom tidier methods",
    "section": "Re-export the tidier generics",
    "text": "Re-export the tidier generics\nThe first step is to re-export the generic functions for tidy(), glance(), and/or augment(). You could do so from broom itself, but we’ve provided an alternative, much lighter dependency called generics.\nFirst you’ll need to add the generics package to Imports. We recommend using the usethis package for this:\n\nusethis::use_package(\"generics\", \"Imports\")\n\nNext, you’ll need to re-export the appropriate tidying methods. If you plan to implement a glance() method, for example, you can re-export the glance() generic by adding the following somewhere inside the /R folder of your package:\n\n#' @importFrom generics glance\n#' @export\ngenerics::glance\n\nOftentimes it doesn’t make sense to define one or more of these methods for a particular model. In this case, only implement the methods that do make sense.\n\n\n\n\n\n\nWarning\n\n\n\nPlease do not define tidy(), glance(), or augment() generics in your package. This will result in namespace conflicts whenever your package is used along other packages that also export tidying methods."
  },
  {
    "objectID": "content/learn/develop/broom/index.html#implement-tidying-methods",
    "href": "content/learn/develop/broom/index.html#implement-tidying-methods",
    "title": "Create your own broom tidier methods",
    "section": "Implement tidying methods",
    "text": "Implement tidying methods\nYou’ll now need to implement specific tidying methods for each of the generics you’ve re-exported in the above step. For each of tidy(), glance(), and augment(), we’ll walk through the big picture, an example, and helpful resources.\nIn this article, we’ll use the base R dataset trees, giving the tree girth (in inches), height (in feet), and volume (in cubic feet), to fit an example linear model using the base R lm() function.\n\n# load in the trees dataset\ndata(trees)\n\n# take a look!\nstr(trees)\n#> 'data.frame':    31 obs. of  3 variables:\n#>  $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n#>  $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n#>  $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\n\n# fit the timber volume as a function of girth and height\ntrees_model <- lm(Volume ~ Girth + Height, data = trees)\n\nLet’s take a look at the summary() of our trees_model fit.\n\nsummary(trees_model)\n#> \n#> Call:\n#> lm(formula = Volume ~ Girth + Height, data = trees)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -6.4065 -2.6493 -0.2876  2.2003  8.4847 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\n#> Girth         4.7082     0.2643  17.816  < 2e-16 ***\n#> Height        0.3393     0.1302   2.607   0.0145 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.882 on 28 degrees of freedom\n#> Multiple R-squared:  0.948,  Adjusted R-squared:  0.9442 \n#> F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\nThis output gives some summary statistics on the residuals (which would be described more fully in an augment() output), model coefficients (which, in this case, make up the tidy() output), and some model-level summarizations such as RSE, \\(R^2\\), etc. (which make up the glance() output.)\n\nImplementing the tidy() method\nThe tidy(x, ...) method will return a tibble where each row contains information about a component of the model. The x input is a model object, and the dots (...) are an optional argument to supply additional information to any calls inside your method. New tidy() methods can take additional arguments, but must include the x and ... arguments to be compatible with the generic function. (For a glossary of currently acceptable additional arguments, see the end of this article.) Examples of model components include regression coefficients (for regression models), clusters (for classification/clustering models), etc. These tidy() methods are useful for inspecting model details and creating custom model visualizations.\nReturning to the example of our linear model on timber volume, we’d like to extract information on the model components. In this example, the components are the regression coefficients. After taking a look at the model object and its summary(), you might notice that you can extract the regression coefficients as follows:\n\nsummary(trees_model)$coefficients\n#>                Estimate Std. Error   t value     Pr(>|t|)\n#> (Intercept) -57.9876589  8.6382259 -6.712913 2.749507e-07\n#> Girth         4.7081605  0.2642646 17.816084 8.223304e-17\n#> Height        0.3392512  0.1301512  2.606594 1.449097e-02\n\nThis object contains the model coefficients as a table, where the information giving which coefficient is being described in each row is given in the row names. Converting to a tibble where the row names are contained in a column, you might write:\n\ntrees_model_tidy <- summary(trees_model)$coefficients %>% \n  as_tibble(rownames = \"term\")\n\ntrees_model_tidy\n#> # A tibble: 3 × 5\n#>   term        Estimate `Std. Error` `t value` `Pr(>|t|)`\n#>   <chr>          <dbl>        <dbl>     <dbl>      <dbl>\n#> 1 (Intercept)  -58.0          8.64      -6.71   2.75e- 7\n#> 2 Girth          4.71         0.264     17.8    8.22e-17\n#> 3 Height         0.339        0.130      2.61   1.45e- 2\n\nThe broom package standardizes common column names used to describe coefficients. In this case, the column names are:\n\ncolnames(trees_model_tidy) <- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n\nA glossary giving the currently acceptable column names outputted by tidy() methods can be found at the end of this article. As a rule of thumb, column names resulting from tidy() methods should be all lowercase and contain only alphanumerics or periods (though there are plenty of exceptions).\nFinally, it is common for tidy() methods to include an option to calculate confidence/credible intervals for each component based on the model, when possible. In this example, the confint() function can be used to calculate confidence intervals from a model object resulting from lm():\n\nconfint(trees_model)\n#>                    2.5 %      97.5 %\n#> (Intercept) -75.68226247 -40.2930554\n#> Girth         4.16683899   5.2494820\n#> Height        0.07264863   0.6058538\n\nWith these considerations in mind, a reasonable tidy() method for lm() might look something like:\n\ntidy.lm <- function(x, conf.int = FALSE, conf.level = 0.95, ...) {\n  \n  result <- summary(x)$coefficients %>%\n    tibble::as_tibble(rownames = \"term\") %>%\n    dplyr::rename(estimate = Estimate,\n                  std.error = `Std. Error`,\n                  statistic = `t value`,\n                  p.value = `Pr(>|t|)`)\n  \n  if (conf.int) {\n    ci <- confint(x, level = conf.level)\n    result <- dplyr::left_join(result, ci, by = \"term\")\n  }\n  \n  result\n}\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re interested, the actual tidy.lm() source can be found here! It’s not too different from the version above except for some argument checking and additional columns.\n\n\nWith this method exported, then, if a user calls tidy(fit), where fit is an output from lm(), the tidy() generic would “redirect” the call to the tidy.lm() function above.\nSome things to keep in mind while writing your tidy() method:\n\nSometimes a model will have several different types of components. For example, in mixed models, there is different information associated with fixed effects and random effects. Since this information doesn’t have the same interpretation, it doesn’t make sense to summarize the fixed and random effects in the same table. In cases like this you should add an argument that allows the user to specify which type of information they want. For example, you might implement an interface along the lines of:\n\n\nmodel <- mixed_model(...)\ntidy(model, effects = \"fixed\")\ntidy(model, effects = \"random\")\n\n\nHow are missing values encoded in the model object and its summary()? Ensure that rows are included even when the associated model component is missing or rank deficient.\nAre there other measures specific to each component that could reasonably be expected to be included in their summarizations? Some common arguments to tidy() methods include:\n\nconf.int: A logical indicating whether or not to calculate confidence/credible intervals. This should default to FALSE.\nconf.level: The confidence level to use for the interval when conf.int = TRUE. Typically defaults to .95.\nexponentiate: A logical indicating whether or not model terms should be presented on an exponential scale (typical for logistic regression).\n\n\n\n\nImplementing the glance() method\nglance() returns a one-row tibble providing model-level summarizations (e.g. goodness of fit measures and related statistics). This is useful to check for model misspecification and to compare many models. Again, the x input is a model object, and the ... is an optional argument to supply additional information to any calls inside your method. New glance() methods can also take additional arguments and must include the x and ... arguments. (For a glossary of currently acceptable additional arguments, see the end of this article.)\nReturning to the trees_model example, we could pull out the \\(R^2\\) value with the following code:\n\nsummary(trees_model)$r.squared\n#> [1] 0.94795\n\nSimilarly, for the adjusted \\(R^2\\):\n\nsummary(trees_model)$adj.r.squared\n#> [1] 0.9442322\n\nUnfortunately, for many model objects, the extraction of model-level information is largely a manual process. You will likely need to build a tibble() element-by-element by subsetting the summary() object repeatedly. The with() function, however, can help make this process a bit less tedious by evaluating expressions inside of the summary(trees_model) environment. To grab those those same two model elements from above using with():\n\nwith(summary(trees_model),\n     tibble::tibble(r.squared = r.squared,\n                    adj.r.squared = adj.r.squared))\n#> # A tibble: 1 × 2\n#>   r.squared adj.r.squared\n#>       <dbl>         <dbl>\n#> 1     0.948         0.944\n\nA reasonable glance() method for lm(), then, might look something like:\n\nglance.lm <- function(x, ...) {\n  with(\n    summary(x),\n    tibble::tibble(\n      r.squared = r.squared,\n      adj.r.squared = adj.r.squared,\n      sigma = sigma,\n      statistic = fstatistic[\"value\"],\n      p.value = pf(\n        fstatistic[\"value\"],\n        fstatistic[\"numdf\"],\n        fstatistic[\"dendf\"],\n        lower.tail = FALSE\n      ),\n      df = fstatistic[\"numdf\"],\n      logLik = as.numeric(stats::logLik(x)),\n      AIC = stats::AIC(x),\n      BIC = stats::BIC(x),\n      deviance = stats::deviance(x),\n      df.residual = df.residual(x),\n      nobs = stats::nobs(x)\n    )\n  )\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThis is the actual definition of glance.lm() provided by broom!\n\n\nSome things to keep in mind while writing glance() methods: * Output should not include the name of the modeling function or any arguments given to the modeling function. * In some cases, you may wish to provide model-level diagnostics not returned by the original object. For example, the above glance.lm() calculates AIC and BIC from the model fit. If these are easy to compute, feel free to add them. However, tidier methods are generally not an appropriate place to implement complex or time consuming calculations. * The glance method should always return the same columns in the same order when given an object of a given model class. If a summary metric (such as AIC) is not defined in certain circumstances, use NA.\n\n\nImplementing the augment() method\naugment() methods add columns to a dataset containing information such as fitted values, residuals or cluster assignments. All columns added to a dataset have a . prefix to prevent existing columns from being overwritten. (Currently acceptable column names are given in the glossary.) The x and ... arguments share their meaning with the two functions described above. augment methods also optionally accept a data argument that is a data.frame (or tibble) to add observation-level information to, returning a tibble object with the same number of rows as data. Many augment() methods also accept a newdata argument, following the same conventions as the data argument, except with the underlying assumption that the model has not “seen” the data yet. As a result, newdata arguments need not contain the response columns in data. Only one of data or newdata should be supplied. A full glossary of acceptable arguments to augment() methods can be found at the end of this article.\nIf a data argument is not specified, augment() should try to reconstruct the original data as much as possible from the model object. This may not always be possible, and often it will not be possible to recover columns not used by the model.\nWith this is mind, we can look back to our trees_model example. For one, the model element inside of the trees_model object will allow us to recover the original data:\n\ntrees_model$model\n#>    Volume Girth Height\n#> 1    10.3   8.3     70\n#> 2    10.3   8.6     65\n#> 3    10.2   8.8     63\n#> 4    16.4  10.5     72\n#> 5    18.8  10.7     81\n#> 6    19.7  10.8     83\n#> 7    15.6  11.0     66\n#> 8    18.2  11.0     75\n#> 9    22.6  11.1     80\n#> 10   19.9  11.2     75\n#> 11   24.2  11.3     79\n#> 12   21.0  11.4     76\n#> 13   21.4  11.4     76\n#> 14   21.3  11.7     69\n#> 15   19.1  12.0     75\n#> 16   22.2  12.9     74\n#> 17   33.8  12.9     85\n#> 18   27.4  13.3     86\n#> 19   25.7  13.7     71\n#> 20   24.9  13.8     64\n#> 21   34.5  14.0     78\n#> 22   31.7  14.2     80\n#> 23   36.3  14.5     74\n#> 24   38.3  16.0     72\n#> 25   42.6  16.3     77\n#> 26   55.4  17.3     81\n#> 27   55.7  17.5     82\n#> 28   58.3  17.9     80\n#> 29   51.5  18.0     80\n#> 30   51.0  18.0     80\n#> 31   77.0  20.6     87\n\nSimilarly, the fitted values and residuals can be accessed with the following code:\n\nhead(trees_model$fitted.values)\n#>         1         2         3         4         5         6 \n#>  4.837660  4.553852  4.816981 15.874115 19.869008 21.018327\nhead(trees_model$residuals)\n#>          1          2          3          4          5          6 \n#>  5.4623403  5.7461484  5.3830187  0.5258848 -1.0690084 -1.3183270\n\nAs with glance() methods, it’s fine (and encouraged!) to include common metrics associated with observations if they are not computationally intensive to compute. A common metric associated with linear models, for example, is the standard error of fitted values:\n\nse.fit <- predict(trees_model, newdata = trees, se.fit = TRUE)$se.fit %>%\n  unname()\n\nhead(se.fit)\n#> [1] 1.3211285 1.4893775 1.6325024 0.9444212 1.3484251 1.5319772\n\nThus, a reasonable augment() method for lm might look something like this:\n\naugment.lm <- function(x, data = x$model, newdata = NULL, ...) {\n  if (is.null(newdata)) {\n    dplyr::bind_cols(tibble::as_tibble(data),\n                     tibble::tibble(.fitted = x$fitted.values,\n                                    .se.fit = predict(x, \n                                                      newdata = data, \n                                                      se.fit = TRUE)$se.fit,\n                                   .resid =  x$residuals))\n  } else {\n    predictions <- predict(x, newdata = newdata, se.fit = TRUE)\n    dplyr::bind_cols(tibble::as_tibble(newdata),\n                     tibble::tibble(.fitted = predictions$fit,\n                                    .se.fit = predictions$se.fit))\n  }\n}\n\nSome other things to keep in mind while writing augment() methods: * The newdata argument should default to NULL. Users should only ever specify one of data or newdata. Providing both data and newdata should result in an error. The newdata argument should accept both data.frames and tibbles. * Data given to the data argument must have both the original predictors and the original response. Data given to the newdata argument only needs to have the original predictors. This is important because there may be important information associated with training data that is not associated with test data. This means that the original_data object in augment(model, data = original_data) should provide .fitted and .resid columns (in most cases), whereas test_data in augment(model, data = test_data) only needs a .fitted column, even if the response is present in test_data. * If the data or newdata is specified as a data.frame with rownames, augment should return them in a column called .rownames. * For observations where no fitted values or summaries are available (where there’s missing data, for example), return NA. * The augment() method should always return as many rows as were in data or newdata, depending on which is supplied\n\n\n\n\n\n\nNote\n\n\n\nThe recommended interface and functionality for augment() methods may change soon."
  },
  {
    "objectID": "content/learn/develop/broom/index.html#document-the-new-methods",
    "href": "content/learn/develop/broom/index.html#document-the-new-methods",
    "title": "Create your own broom tidier methods",
    "section": "Document the new methods",
    "text": "Document the new methods\nThe only remaining step is to integrate the new methods into the parent package! To do so, just drop the methods into a .R file inside of the /R folder and document them using roxygen2. If you’re unfamiliar with the process of documenting objects, you can read more about it here. Here’s an example of how our tidy.lm() method might be documented:\n\n#' Tidy a(n) lm object\n#'\n#' @param x A `lm` object.\n#' @param conf.int Logical indicating whether or not to include \n#'   a confidence interval in the tidied output. Defaults to FALSE.\n#' @param conf.level The confidence level to use for the confidence \n#'   interval if conf.int = TRUE. Must be strictly greater than 0 \n#'   and less than 1. Defaults to 0.95, which corresponds to a \n#'   95 percent confidence interval.\n#' @param ... Unused, included for generic consistency only.\n#' @return A tidy [tibble::tibble()] summarizing component-level\n#'   information about the model\n#'\n#' @examples\n#' # load the trees dataset\n#' data(trees)\n#' \n#' # fit a linear model on timber volume\n#' trees_model <- lm(Volume ~ Girth + Height, data = trees)\n#'\n#' # summarize model coefficients in a tidy tibble!\n#' tidy(trees_model)\n#'\n#' @export\ntidy.lm <- function(x, conf.int = FALSE, conf.level = 0.95, ...) {\n\n  # ... the rest of the function definition goes here!\n\nOnce you’ve documented each of your new methods and executed devtools::document(), you’re done! Congrats on implementing your own broom tidier methods for a new model object!"
  },
  {
    "objectID": "content/learn/develop/broom/index.html#glossary",
    "href": "content/learn/develop/broom/index.html#glossary",
    "title": "Create your own broom tidier methods",
    "section": "Glossaries: and column names",
    "text": "Glossaries: and column names\n\nArguments\nTidier methods have a standardized set of acceptable argument and output column names. The currently acceptable argument names by tidier method are:\n\n\n\n\n\n\n\n\n\nColumn Names\nThe currently acceptable column names by tidier method are:\n\n\n\n\n\n\n\nThe alexpghayes/modeltests package provides unit testing infrastructure to check your new tidier methods. Please file an issue there to request new arguments/columns to be added to the glossaries!"
  },
  {
    "objectID": "content/learn/develop/broom/index.html#session-information",
    "href": "content/learn/develop/broom/index.html#session-information",
    "title": "Create your own broom tidier methods",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  generics   * 0.1.3      2022-07-05 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tidyverse  * 1.3.2      2022-07-18 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/models/time-series/index.html",
    "href": "content/learn/models/time-series/index.html",
    "title": "Modeling time series with tidy resampling",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: forecast, sweep, tidymodels, timetk, and zoo.\n“Demo Week: Tidy Forecasting with sweep” is an excellent article that uses tidy methods with time series. This article uses their analysis with rsample to find performance estimates for future observations using rolling forecast origin resampling."
  },
  {
    "objectID": "content/learn/models/time-series/index.html#example-data",
    "href": "content/learn/models/time-series/index.html#example-data",
    "title": "Modeling time series with tidy resampling",
    "section": "Example data",
    "text": "Example data\nThe data for this article are sales of alcoholic beverages originally from the Federal Reserve Bank of St. Louis website.\n\nlibrary(tidymodels)\nlibrary(modeldata)\ndata(\"drinks\")\nglimpse(drinks)\n#> Rows: 309\n#> Columns: 2\n#> $ date           <date> 1992-01-01, 1992-02-01, 1992-03-01, 1992-04-01, 1992-0…\n#> $ S4248SM144NCEN <dbl> 3459, 3458, 4002, 4564, 4221, 4529, 4466, 4137, 4126, 4…\n\nEach row represents one month of sales (in millions of US dollars)."
  },
  {
    "objectID": "content/learn/models/time-series/index.html#time-series-resampling",
    "href": "content/learn/models/time-series/index.html#time-series-resampling",
    "title": "Modeling time series with tidy resampling",
    "section": "Time series resampling",
    "text": "Time series resampling\nSuppose that we need predictions for one year ahead and our model should use the most recent data from the last 20 years. To set up this resampling scheme:\n\nroll_rs <- rolling_origin(\n  drinks, \n  initial = 12 * 20, \n  assess = 12,\n  cumulative = FALSE\n  )\n\nnrow(roll_rs)\n#> [1] 58\n\nroll_rs\n#> # Rolling origin forecast resampling \n#> # A tibble: 58 × 2\n#>    splits           id     \n#>    <list>           <chr>  \n#>  1 <split [240/12]> Slice01\n#>  2 <split [240/12]> Slice02\n#>  3 <split [240/12]> Slice03\n#>  4 <split [240/12]> Slice04\n#>  5 <split [240/12]> Slice05\n#>  6 <split [240/12]> Slice06\n#>  7 <split [240/12]> Slice07\n#>  8 <split [240/12]> Slice08\n#>  9 <split [240/12]> Slice09\n#> 10 <split [240/12]> Slice10\n#> # … with 48 more rows\n\nEach split element contains the information about that resample:\n\nroll_rs$splits[[1]]\n#> <Analysis/Assess/Total>\n#> <240/12/309>\n\nFor plotting, let’s index each split by the first day of the assessment set:\n\nget_date <- function(x) {\n  min(assessment(x)$date)\n}\n\nstart_date <- map(roll_rs$splits, get_date)\nroll_rs$start_date <- do.call(\"c\", start_date)\nhead(roll_rs$start_date)\n#> [1] \"2012-01-01\" \"2012-02-01\" \"2012-03-01\" \"2012-04-01\" \"2012-05-01\"\n#> [6] \"2012-06-01\"\n\nThis resampling scheme has 58 splits of the data so that there will be 58 ARIMA models that are fit. To create the models, we use the auto.arima() function from the forecast package. The rsample functions analysis() and assessment() return a data frame, so another step converts the data to a ts object called mod_dat using a function in the timetk package.\n\nlibrary(forecast)  # for `auto.arima`\nlibrary(timetk)    # for `tk_ts`\nlibrary(zoo)       # for `as.yearmon`\n\nfit_model <- function(x, ...) {\n  # suggested by Matt Dancho:\n  x %>%\n    analysis() %>%\n    # Since the first day changes over resamples, adjust it\n    # based on the first date value in the data frame \n    tk_ts(start = .$date[[1]] %>% as.yearmon(), \n          frequency = 12, \n          silent = TRUE) %>%\n    auto.arima(...)\n}\n\nSave each model in a new column:\n\nroll_rs$arima <- map(roll_rs$splits, fit_model)\n\n# For example:\nroll_rs$arima[[1]]\n#> Series: . \n#> ARIMA(4,1,1)(0,1,2)[12] \n#> \n#> Coefficients:\n#>           ar1      ar2     ar3      ar4      ma1    sma1     sma2\n#>       -0.1852  -0.0238  0.3577  -0.1517  -0.8311  -0.193  -0.3244\n#> s.e.   0.1466   0.1656  0.1440   0.0809   0.1377   0.067   0.0640\n#> \n#> sigma^2 = 72198:  log likelihood = -1591.15\n#> AIC=3198.3   AICc=3198.97   BIC=3225.7\n\n(There are some warnings produced by these regarding extra columns in the data that can be ignored.)"
  },
  {
    "objectID": "content/learn/models/time-series/index.html#model-performance",
    "href": "content/learn/models/time-series/index.html#model-performance",
    "title": "Modeling time series with tidy resampling",
    "section": "Model performance",
    "text": "Model performance\nUsing the model fits, let’s measure performance in two ways:\n\nInterpolation error will measure how well the model fits to the data that were used to create the model. This is most likely optimistic since no holdout method is used.\nExtrapolation or forecast error evaluates the performance of the model on the data from the following year (that were not used in the model fit).\n\nIn each case, the mean absolute percent error (MAPE) is the statistic used to characterize the model fits. The interpolation error can be computed from the Arima object. To make things easy, let’s use the sweep package’s sw_glance() function:\n\nlibrary(sweep)\n\nroll_rs$interpolation <- map_dbl(\n  roll_rs$arima,\n  function(x) \n    sw_glance(x)[[\"MAPE\"]]\n  )\n\nsummary(roll_rs$interpolation)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   2.841   2.921   2.950   2.947   2.969   3.135\n\nFor the extrapolation error, the model and split objects are required. Using these:\n\nget_extrap <- function(split, mod) {\n  n <- nrow(assessment(split))\n  # Get assessment data\n  pred_dat <- assessment(split) %>%\n    mutate(\n      pred = as.vector(forecast(mod, h = n)$mean),\n      pct_error = ( S4248SM144NCEN - pred ) / S4248SM144NCEN * 100\n    )\n  mean(abs(pred_dat$pct_error))\n}\n\nroll_rs$extrapolation <- \n  map2_dbl(roll_rs$splits, roll_rs$arima, get_extrap)\n\nsummary(roll_rs$extrapolation)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   2.371   3.231   3.629   3.654   4.113   5.453\n\nWhat do these error estimates look like over time?\n\nroll_rs %>%\n  select(interpolation, extrapolation, start_date) %>%\n  pivot_longer(cols = matches(\"ation\"), names_to = \"error\", values_to = \"MAPE\") %>%\n  ggplot(aes(x = start_date, y = MAPE, col = error)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\nIt is likely that the interpolation error is an underestimate to some degree, as mentioned above.\nIt is also worth noting that rolling_origin() can be used over calendar periods, rather than just over a fixed window size. This is especially useful for irregular series where a fixed window size might not make sense because of missing data points, or because of calendar features like different months having a different number of days.\nThe example below demonstrates this idea by splitting drinks into a nested set of 26 years, and rolling over years rather than months. Note that the end result accomplishes a different task than the original example; in this new case, each slice moves forward an entire year, rather than just one month.\n\n# The idea is to nest by the period to roll over,\n# which in this case is the year.\nroll_rs_annual <- drinks %>%\n  mutate(year = as.POSIXlt(date)$year + 1900) %>%\n  nest(data = c(date, S4248SM144NCEN)) %>%\n  rolling_origin(\n    initial = 20, \n    assess = 1, \n    cumulative = FALSE\n  )\n\nanalysis(roll_rs_annual$splits[[1]])\n#> # A tibble: 20 × 2\n#>     year data             \n#>    <dbl> <list>           \n#>  1  1992 <tibble [12 × 2]>\n#>  2  1993 <tibble [12 × 2]>\n#>  3  1994 <tibble [12 × 2]>\n#>  4  1995 <tibble [12 × 2]>\n#>  5  1996 <tibble [12 × 2]>\n#>  6  1997 <tibble [12 × 2]>\n#>  7  1998 <tibble [12 × 2]>\n#>  8  1999 <tibble [12 × 2]>\n#>  9  2000 <tibble [12 × 2]>\n#> 10  2001 <tibble [12 × 2]>\n#> 11  2002 <tibble [12 × 2]>\n#> 12  2003 <tibble [12 × 2]>\n#> 13  2004 <tibble [12 × 2]>\n#> 14  2005 <tibble [12 × 2]>\n#> 15  2006 <tibble [12 × 2]>\n#> 16  2007 <tibble [12 × 2]>\n#> 17  2008 <tibble [12 × 2]>\n#> 18  2009 <tibble [12 × 2]>\n#> 19  2010 <tibble [12 × 2]>\n#> 20  2011 <tibble [12 × 2]>\n\nThe workflow to access these calendar slices is to use bind_rows() to join each analysis set together.\n\nmutate(\n  roll_rs_annual,\n  extracted_slice = map(splits, ~ bind_rows(analysis(.x)$data))\n)\n#> # Rolling origin forecast resampling \n#> # A tibble: 6 × 3\n#>   splits         id     extracted_slice   \n#>   <list>         <chr>  <list>            \n#> 1 <split [20/1]> Slice1 <tibble [240 × 2]>\n#> 2 <split [20/1]> Slice2 <tibble [240 × 2]>\n#> 3 <split [20/1]> Slice3 <tibble [240 × 2]>\n#> 4 <split [20/1]> Slice4 <tibble [240 × 2]>\n#> 5 <split [20/1]> Slice5 <tibble [240 × 2]>\n#> 6 <split [20/1]> Slice6 <tibble [240 × 2]>"
  },
  {
    "objectID": "content/learn/models/time-series/index.html#session-information",
    "href": "content/learn/models/time-series/index.html#session-information",
    "title": "Modeling time series with tidy resampling",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  forecast   * 8.19       2022-11-20 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  sweep      * 0.2.3      2020-07-10 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  timetk     * 2.8.2      2022-11-17 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#>  zoo        * 1.8-11     2022-09-17 [1] CRAN (R 4.2.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/models/parsnip-nnet/index.html",
    "href": "content/learn/models/parsnip-nnet/index.html",
    "title": "Classification models using a neural network",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: AppliedPredictiveModeling, brulee, and tidymodels. You will also need the python torch library installed (see ?torch::install_torch()).\nWe can create classification models with the tidymodels package parsnip to predict categorical quantities or class labels. Here, let’s fit a single classification model using a neural network and evaluate using a validation set. While the tune package has functionality to also do this, the parsnip package is the center of attention in this article so that we can better understand its usage."
  },
  {
    "objectID": "content/learn/models/parsnip-nnet/index.html#fitting-a-neural-network",
    "href": "content/learn/models/parsnip-nnet/index.html#fitting-a-neural-network",
    "title": "Classification models using a neural network",
    "section": "Fitting a neural network",
    "text": "Fitting a neural network\nLet’s fit a model to a small, two predictor classification data set. The data are in the modeldata package (part of tidymodels) and have been split into training, validation, and test data sets. In this analysis, the test set is left untouched; this article tries to emulate a good data usage methodology where the test set would only be evaluated once at the end after a variety of models have been considered.\n\nlibrary(AppliedPredictiveModeling)\n\nset.seed(321)\ncls_train <- quadBoundaryFunc(2000) %>% select(A = X1, B = X2, class)\ncls_val   <- quadBoundaryFunc( 500) %>% select(A = X1, B = X2, class)\ncls_test  <- quadBoundaryFunc( 500) %>% select(A = X1, B = X2, class)\n\nA plot of the data shows two right-skewed predictors:\n\nggplot(cls_train, aes(x = A, y = B, col = class)) + \n  geom_point(alpha = 1 / 4, cex = 3) + \n  coord_fixed()\n\n\n\n\n\n\n\n\nLet’s use a single hidden layer neural network to predict the outcome. To do this, we transform the predictor columns to be more symmetric (via the step_BoxCox() function) and on a common scale (using step_normalize()). We can use recipes to do so:\n\nbiv_rec <- \n  recipe(class ~ ., data = cls_train) %>%\n  step_normalize(all_predictors())\n\nThis recipe is not directly executed; the steps will be estimated when the model is fit.\nWe can use the brulee package to fit a model with 5 hidden units and a 10% dropout rate, to regularize the model:\n\nnnet_spec <- \n  mlp(epochs = 1000, hidden_units = 10, penalty = 0.01, learn_rate = 0.1) %>% \n  set_engine(\"brulee\", validation = 0) %>% \n  set_mode(\"classification\")\n\nnnet_wflow <- \n  biv_rec %>% \n  workflow(nnet_spec)\n\nset.seed(987)\nnnet_fit <- fit(nnet_wflow, cls_train)\nnnet_fit %>% extract_fit_engine()\n#> Multilayer perceptron\n#> \n#> relu activation\n#> 10 hidden units,  52 model parameters\n#> 2,000 samples, 2 features, 2 classes \n#> class weights Class1=1, Class2=1 \n#> weight decay: 0.01 \n#> dropout proportion: 0 \n#> batch size: 2000 \n#> learn rate: 0.1 \n#> training set loss after 1000 epochs: 0.375"
  },
  {
    "objectID": "content/learn/models/parsnip-nnet/index.html#model-performance",
    "href": "content/learn/models/parsnip-nnet/index.html#model-performance",
    "title": "Classification models using a neural network",
    "section": "Model performance",
    "text": "Model performance\nIn parsnip, the predict() function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data:\n\nval_results <- \n  cls_val %>%\n  bind_cols(\n    predict(nnet_fit, new_data = cls_val),\n    predict(nnet_fit, new_data = cls_val, type = \"prob\")\n  )\nval_results %>% slice(1:5)\n#>           A           B  class .pred_class .pred_Class1 .pred_Class2\n#> 1 0.7632082 -0.04012164 Class2      Class2   0.06255509   0.93744493\n#> 2 0.9823745 -0.16911637 Class2      Class2   0.05721300   0.94278705\n#> 3 1.0558147  0.52817699 Class2      Class2   0.10368267   0.89631736\n#> 4 1.2424507  1.10902951 Class2      Class2   0.34966809   0.65033191\n#> 5 1.5889815  2.71047720 Class1      Class1   0.97951710   0.02048291\n\nval_results %>% roc_auc(truth = class, .pred_Class1)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.957\n\nval_results %>% accuracy(truth = class, .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary          0.91\n\nval_results %>% conf_mat(truth = class, .pred_class)\n#>           Truth\n#> Prediction Class1 Class2\n#>     Class1    175     18\n#>     Class2     27    280\n\nLet’s also create a grid to get a visual sense of the class boundary for the test set.\n\na_rng <- range(cls_train$A)\nb_rng <- range(cls_train$B)\nx_grid <-\n  expand.grid(A = seq(a_rng[1], a_rng[2], length.out = 100),\n              B = seq(b_rng[1], b_rng[2], length.out = 100))\n\n\n# Make predictions using the transformed predictors but \n# attach them to the predictors in the original units: \nx_grid <- \n  x_grid %>% \n  bind_cols(predict(nnet_fit, x_grid, type = \"prob\"))\n\nggplot(x_grid, aes(x = A, y = B)) + \n  geom_point(data = cls_test, aes(col = class), alpha = 1 / 2, cex = 3) +\n  geom_contour(aes(z = .pred_Class1), breaks = .5, col = \"black\", linewidth = 1) + \n  coord_fixed()"
  },
  {
    "objectID": "content/learn/models/parsnip-nnet/index.html#session-information",
    "href": "content/learn/models/parsnip-nnet/index.html#session-information",
    "title": "Classification models using a neural network",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package                   * version    date (UTC) lib source\n#>  AppliedPredictiveModeling * 1.1-7      2018-05-22 [1] CRAN (R 4.2.0)\n#>  broom                     * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  brulee                      0.2.0      2022-09-19 [1] CRAN (R 4.2.0)\n#>  dials                     * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr                     * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2                   * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer                     * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip                   * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr                     * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes                   * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang                       1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample                   * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble                    * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels                * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune                      * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows                 * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick                 * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/models/coefficients/index.html",
    "href": "content/learn/models/coefficients/index.html",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients.\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. To use code in this article, you will need to install the following packages: glmnet and tidymodels."
  },
  {
    "objectID": "content/learn/models/coefficients/index.html#linear-regression",
    "href": "content/learn/models/coefficients/index.html#linear-regression",
    "title": "Working with model coefficients",
    "section": "Linear regression",
    "text": "Linear regression\nLet’s start with a linear regression model:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p\\]\nThe \\(\\beta\\) values are the coefficients and the \\(x_j\\) are model predictors, or features.\nLet’s use the Chicago train data where we predict the ridership at the Clark and Lake station (column name: ridership) with the previous ridership data 14 days prior at three of the stations.\nThe data are in the modeldata package:\n\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago <- Chicago %>% select(ridership, Clark_Lake, Austin, Harlem)\n\n\nA single model\nLet’s start by fitting only a single parsnip model object. We’ll create a model specification using linear_reg().\n{{% note %}} The default engine is \"lm\" so no call to set_engine() is required. {{%/ note %}}\nThe fit() function estimates the model coefficients, given a formula and data set.\n\nlm_spec <- linear_reg()\nlm_fit <- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> stats::lm(formula = ridership ~ ., data = data)\n#> \n#> Coefficients:\n#> (Intercept)   Clark_Lake       Austin       Harlem  \n#>      1.6778       0.9035       0.6123      -0.5550\n\nThe best way to retrieve the fitted parameters is to use the tidy() method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names:\n\ntidy(lm_fit)\n#> # A tibble: 4 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)    1.68     0.156      10.7  1.11e- 26\n#> 2 Clark_Lake     0.904    0.0280     32.3  5.14e-210\n#> 3 Austin         0.612    0.320       1.91 5.59e-  2\n#> 4 Harlem        -0.555    0.165      -3.36 7.85e-  4\n\nWe’ll use this function in subsequent sections.\n\n\nResampled or tuned models\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the bootstrap to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.\nWe’ll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\nset.seed(123)\nbt <- bootstraps(Chicago, times = 5)\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function fit_resamples() is the recommended approach for doing so.\n{{% warning %}} The fit_resamples() function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to fit_resamples() that can save the model object or any other aspect of the fit. {{%/ warning %}}\nThis function takes a single argument that represents the fitted workflow object (even if you don’t give fit_resamples() a workflow).\nFrom this, we can extract the model fit. There are two “levels” of model objects that are available:\n\nThe parsnip model object, which wraps the underlying model object. We retrieve this using the extract_fit_parsnip() function.\nThe underlying model object (a.k.a. the engine fit) via the extract_fit_engine().\n\nWe’ll use the latter option and then tidy this model object as we did in the previous section. Let’s add this to the control function so that we can re-use it.\n\nget_lm_coefs <- function(x) {\n  x %>% \n    # get the lm model object\n    extract_fit_engine() %>% \n    # transform its format\n    tidy()\n}\ntidy_ctrl <- control_grid(extract = get_lm_coefs)\n\nThis argument is then passed to fit_resamples():\n\nlm_res <- \n  lm_spec %>% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n#> # Resampling results\n#> # Bootstrap sampling \n#> # A tibble: 5 × 5\n#>   splits              id         .metrics         .notes           .extracts\n#>   <list>              <chr>      <list>           <list>           <list>   \n#> 1 <split [5698/2076]> Bootstrap1 <tibble [2 × 4]> <tibble [0 × 3]> <tibble> \n#> 2 <split [5698/2098]> Bootstrap2 <tibble [2 × 4]> <tibble [0 × 3]> <tibble> \n#> 3 <split [5698/2064]> Bootstrap3 <tibble [2 × 4]> <tibble [0 × 3]> <tibble> \n#> 4 <split [5698/2082]> Bootstrap4 <tibble [2 × 4]> <tibble [0 × 3]> <tibble> \n#> 5 <split [5698/2088]> Bootstrap5 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>\n\nNote that there is a .extracts column in our resampling results. This object contains the output of our get_lm_coefs() function for each resample. The structure of the elements of this column is a little complex. Let’s start by looking at the first element (which corresponds to the first resample):\n\nlm_res$.extracts[[1]]\n#> # A tibble: 1 × 2\n#>   .extracts        .config             \n#>   <list>           <chr>               \n#> 1 <tibble [4 × 5]> Preprocessor1_Model1\n\nThere is another column in this element called .extracts that has the results of the tidy() function call:\n\nlm_res$.extracts[[1]]$.extracts[[1]]\n#> # A tibble: 4 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)    1.40     0.157       8.90 7.23e- 19\n#> 2 Clark_Lake     0.842    0.0280     30.1  2.39e-184\n#> 3 Austin         1.46     0.320       4.54 5.70e-  6\n#> 4 Harlem        -0.637    0.163      -3.92 9.01e-  5\n\nThese nested columns can be flattened via the purrr unnest() function:\n\nlm_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) \n#> # A tibble: 5 × 3\n#>   id         .extracts        .config             \n#>   <chr>      <list>           <chr>               \n#> 1 Bootstrap1 <tibble [4 × 5]> Preprocessor1_Model1\n#> 2 Bootstrap2 <tibble [4 × 5]> Preprocessor1_Model1\n#> 3 Bootstrap3 <tibble [4 × 5]> Preprocessor1_Model1\n#> 4 Bootstrap4 <tibble [4 × 5]> Preprocessor1_Model1\n#> 5 Bootstrap5 <tibble [4 × 5]> Preprocessor1_Model1\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format:\n\nlm_coefs <- \n  lm_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  unnest(.extracts)\n\nlm_coefs %>% select(id, term, estimate, p.value)\n#> # A tibble: 20 × 4\n#>    id         term        estimate   p.value\n#>    <chr>      <chr>          <dbl>     <dbl>\n#>  1 Bootstrap1 (Intercept)    1.40  7.23e- 19\n#>  2 Bootstrap1 Clark_Lake     0.842 2.39e-184\n#>  3 Bootstrap1 Austin         1.46  5.70e-  6\n#>  4 Bootstrap1 Harlem        -0.637 9.01e-  5\n#>  5 Bootstrap2 (Intercept)    1.69  2.87e- 28\n#>  6 Bootstrap2 Clark_Lake     0.911 1.06e-219\n#>  7 Bootstrap2 Austin         0.595 5.93e-  2\n#>  8 Bootstrap2 Harlem        -0.580 3.88e-  4\n#>  9 Bootstrap3 (Intercept)    1.27  3.43e- 16\n#> 10 Bootstrap3 Clark_Lake     0.859 5.03e-194\n#> 11 Bootstrap3 Austin         1.09  6.77e-  4\n#> 12 Bootstrap3 Harlem        -0.470 4.34e-  3\n#> 13 Bootstrap4 (Intercept)    1.95  2.91e- 34\n#> 14 Bootstrap4 Clark_Lake     0.974 1.47e-233\n#> 15 Bootstrap4 Austin        -0.116 7.21e-  1\n#> 16 Bootstrap4 Harlem        -0.620 2.11e-  4\n#> 17 Bootstrap5 (Intercept)    1.87  1.98e- 33\n#> 18 Bootstrap5 Clark_Lake     0.901 1.16e-210\n#> 19 Bootstrap5 Austin         0.494 1.15e-  1\n#> 20 Bootstrap5 Harlem        -0.512 1.73e-  3\n\nThat’s better! Now, let’s plot the model coefficients for each resample:\n\nlm_coefs %>%\n  filter(term != \"(Intercept)\") %>% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two.\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases."
  },
  {
    "objectID": "content/learn/models/coefficients/index.html#more-complex-a-glmnet-model",
    "href": "content/learn/models/coefficients/index.html#more-complex-a-glmnet-model",
    "title": "Working with model coefficients",
    "section": "More complex: a glmnet model",
    "text": "More complex: a glmnet model\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set.\nThere are two types of penalization that this model uses:\n\nLasso (a.k.a. \\(L_1\\)) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model).\nWeight decay (a.k.a ridge regression or \\(L_2\\)) uses a different type of penalty that is most useful for highly correlated predictors.\n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\nglmnet_spec <- \n  linear_reg(penalty = 0.1, mixture = 0.95) %>% \n  set_engine(\"glmnet\")\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high).\n{{% note %}} Models with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet automatically centers and scales the data. You can use recipes to center and scale your data yourself. {{%/ note %}}\nLet’s combine the model specification with a formula in a model workflow() and then fit the model to the data:\n\nglmnet_wflow <- \n  workflow() %>% \n  add_model(glmnet_spec) %>% \n  add_formula(ridership ~ .)\n\nglmnet_fit <- fit(glmnet_wflow, Chicago)\nglmnet_fit\n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> ridership ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0.95) \n#> \n#>    Df  %Dev Lambda\n#> 1   0  0.00 6.1040\n#> 2   1 12.75 5.5620\n#> 3   1 23.45 5.0680\n#> 4   1 32.43 4.6180\n#> 5   1 39.95 4.2070\n#> 6   1 46.25 3.8340\n#> 7   1 51.53 3.4930\n#> 8   1 55.94 3.1830\n#> 9   1 59.62 2.9000\n#> 10  1 62.70 2.6420\n#> 11  2 65.28 2.4080\n#> 12  2 67.44 2.1940\n#> 13  2 69.23 1.9990\n#> 14  2 70.72 1.8210\n#> 15  2 71.96 1.6600\n#> 16  2 73.00 1.5120\n#> 17  2 73.86 1.3780\n#> 18  2 74.57 1.2550\n#> 19  2 75.17 1.1440\n#> 20  2 75.66 1.0420\n#> 21  2 76.07 0.9496\n#> 22  2 76.42 0.8653\n#> 23  2 76.70 0.7884\n#> 24  2 76.94 0.7184\n#> 25  2 77.13 0.6545\n#> 26  2 77.30 0.5964\n#> 27  2 77.43 0.5434\n#> 28  2 77.55 0.4951\n#> 29  2 77.64 0.4512\n#> 30  2 77.72 0.4111\n#> 31  2 77.78 0.3746\n#> 32  2 77.84 0.3413\n#> 33  2 77.88 0.3110\n#> 34  2 77.92 0.2833\n#> 35  2 77.95 0.2582\n#> 36  2 77.98 0.2352\n#> 37  2 78.00 0.2143\n#> 38  2 78.01 0.1953\n#> 39  2 78.03 0.1779\n#> 40  2 78.04 0.1621\n#> 41  2 78.05 0.1477\n#> 42  2 78.06 0.1346\n#> 43  2 78.07 0.1226\n#> 44  2 78.07 0.1118\n#> 45  2 78.08 0.1018\n#> 46  2 78.08 0.0928\n#> \n#> ...\n#> and 9 more lines.\n\nIn this output, the term lambda is used to represent the penalty.\nNote that the output shows many values of the penalty despite our specification of penalty = 0.1. It turns out that this model fits a “path” of penalty values. Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object.\nLet’s look at two different approaches to obtaining the coefficients. Both will use the tidy() method. One will tidy a glmnet object and the other will tidy a tidymodels object.\n\nUsing glmnet penalty values\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are 55 penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy:\n\nglmnet_fit %>% \n  extract_fit_engine() %>% \n  tidy() %>% \n  rename(penalty = lambda) %>%   # <- for consistent naming\n  filter(term != \"(Intercept)\")\n#> # A tibble: 99 × 5\n#>    term        step estimate penalty dev.ratio\n#>    <chr>      <dbl>    <dbl>   <dbl>     <dbl>\n#>  1 Clark_Lake     2   0.0753    5.56     0.127\n#>  2 Clark_Lake     3   0.145     5.07     0.234\n#>  3 Clark_Lake     4   0.208     4.62     0.324\n#>  4 Clark_Lake     5   0.266     4.21     0.400\n#>  5 Clark_Lake     6   0.319     3.83     0.463\n#>  6 Clark_Lake     7   0.368     3.49     0.515\n#>  7 Clark_Lake     8   0.413     3.18     0.559\n#>  8 Clark_Lake     9   0.454     2.90     0.596\n#>  9 Clark_Lake    10   0.491     2.64     0.627\n#> 10 Clark_Lake    11   0.526     2.41     0.653\n#> # … with 89 more rows\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the tidy() method for glmnet objects does not use it.\n\n\nUsing specific penalty values\nIf we run the tidy() method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified:\n\ntidy(glmnet_fit)\n#> # A tibble: 4 × 3\n#>   term        estimate penalty\n#>   <chr>          <dbl>   <dbl>\n#> 1 (Intercept)    1.69      0.1\n#> 2 Clark_Lake     0.846     0.1\n#> 3 Austin         0.271     0.1\n#> 4 Harlem         0         0.1\n\nFor any another (single) penalty, we can use an additional argument:\n\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n#> # A tibble: 4 × 3\n#>   term        estimate penalty\n#>   <chr>          <dbl>   <dbl>\n#> 1 (Intercept)  12.6       5.56\n#> 2 Clark_Lake    0.0753    5.56\n#> 3 Austin        0         5.56\n#> 4 Harlem        0         5.56\n\nThe reason for having two tidy() methods is that, with tidymodels, the focus is on using a specific penalty value.\n\n\nTuning a glmnet model\nIf we know a priori acceptable values for penalty and mixture, we can use the fit_resamples() function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels tune_*() functions.\nLet’s tune our glmnet model over both parameters with this grid:\n\npen_vals <- 10^seq(-3, 0, length.out = 10)\ngrid <- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of mixture will probably produce a different set of penalty values contained in the model object. How can we look at the coefficients at the specific penalty values that we are using to tune?\nThe approach that we suggest is to use the special path_values option for glmnet. Details are described in the technical documentation about glmnet and tidymodels but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture).\nWe can pass these as an engine argument and then update our previous workflow object:\n\nglmnet_tune_spec <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow <- \n  glmnet_wflow %>% \n  update_model(glmnet_tune_spec)\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty:\n\nget_glmnet_coefs <- function(x) {\n  x %>% \n    extract_fit_engine() %>% \n    tidy(return_zeros = TRUE) %>% \n    rename(penalty = lambda)\n}\nparsnip_ctrl <- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res <- \n  glmnet_wflow %>% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n#> # Tuning results\n#> # Bootstrap sampling \n#> # A tibble: 5 × 5\n#>   splits              id         .metrics          .notes           .extracts\n#>   <list>              <chr>      <list>            <list>           <list>   \n#> 1 <split [5698/2076]> Bootstrap1 <tibble [40 × 6]> <tibble [0 × 3]> <tibble> \n#> 2 <split [5698/2098]> Bootstrap2 <tibble [40 × 6]> <tibble [0 × 3]> <tibble> \n#> 3 <split [5698/2064]> Bootstrap3 <tibble [40 × 6]> <tibble [0 × 3]> <tibble> \n#> 4 <split [5698/2082]> Bootstrap4 <tibble [40 × 6]> <tibble [0 × 3]> <tibble> \n#> 5 <split [5698/2088]> Bootstrap5 <tibble [40 × 6]> <tibble [0 × 3]> <tibble>\n\nAs noted before, the elements of the main .extracts column have an embedded list column with the results of get_glmnet_coefs():\n\nglmnet_res$.extracts[[1]] %>% head()\n#> # A tibble: 6 × 4\n#>   penalty mixture .extracts         .config              \n#>     <dbl>   <dbl> <list>            <chr>                \n#> 1       1     0.1 <tibble [40 × 5]> Preprocessor1_Model01\n#> 2       1     0.1 <tibble [40 × 5]> Preprocessor1_Model02\n#> 3       1     0.1 <tibble [40 × 5]> Preprocessor1_Model03\n#> 4       1     0.1 <tibble [40 × 5]> Preprocessor1_Model04\n#> 5       1     0.1 <tibble [40 × 5]> Preprocessor1_Model05\n#> 6       1     0.1 <tibble [40 × 5]> Preprocessor1_Model06\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %>% head()\n#> # A tibble: 6 × 5\n#>   term         step estimate penalty dev.ratio\n#>   <chr>       <dbl>    <dbl>   <dbl>     <dbl>\n#> 1 (Intercept)     1    0.568  1          0.769\n#> 2 (Intercept)     2    0.432  0.464      0.775\n#> 3 (Intercept)     3    0.607  0.215      0.779\n#> 4 (Intercept)     4    0.846  0.1        0.781\n#> 5 (Intercept)     5    1.06   0.0464     0.782\n#> 6 (Intercept)     6    1.22   0.0215     0.783\n\nAs before, we’ll have to use a double unnest(). Since the penalty value is in both the top-level and lower-level .extracts, we’ll use select() to get rid of the first version (but keep mixture):\n\nglmnet_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  select(id, mixture, .extracts) %>%  # <- removes the first penalty column\n  unnest(.extracts)\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of mixture, the results are the same:\n\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n#> [1] TRUE\n\nFor this reason, we’ll add a slice(1) when grouping by id and mixture. This will get rid of the replicated results.\n\nglmnet_coefs <- \n  glmnet_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  select(id, mixture, .extracts) %>% \n  group_by(id, mixture) %>%          # ┐\n  slice(1) %>%                       # │ Remove the redundant results\n  ungroup() %>%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %>% \n  select(id, penalty, mixture, term, estimate) %>% \n  filter(term != \"(Intercept)\")\n#> # A tibble: 300 × 5\n#>    id         penalty mixture term       estimate\n#>    <chr>        <dbl>   <dbl> <chr>         <dbl>\n#>  1 Bootstrap1 1           0.1 Clark_Lake    0.391\n#>  2 Bootstrap1 0.464       0.1 Clark_Lake    0.485\n#>  3 Bootstrap1 0.215       0.1 Clark_Lake    0.590\n#>  4 Bootstrap1 0.1         0.1 Clark_Lake    0.680\n#>  5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746\n#>  6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793\n#>  7 Bootstrap1 0.01        0.1 Clark_Lake    0.817\n#>  8 Bootstrap1 0.00464     0.1 Clark_Lake    0.828\n#>  9 Bootstrap1 0.00215     0.1 Clark_Lake    0.834\n#> 10 Bootstrap1 0.001       0.1 Clark_Lake    0.837\n#> # … with 290 more rows\n\nNow we have the coefficients. Let’s look at how they behave as more regularization is used:\n\nglmnet_coefs %>% \n  filter(term != \"(Intercept)\") %>% \n  mutate(mixture = format(mixture)) %>% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nNotice a couple of things:\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases.\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive."
  },
  {
    "objectID": "content/learn/models/coefficients/index.html#session-information",
    "href": "content/learn/models/coefficients/index.html#session-information",
    "title": "Working with model coefficients",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  glmnet     * 4.1-4      2022-04-15 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/models/parsnip-ranger-glmnet/index.html",
    "href": "content/learn/models/parsnip-ranger-glmnet/index.html",
    "title": "Regression models two ways",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: glmnet, randomForest, ranger, and tidymodels.\nWe can create regression models with the tidymodels package parsnip to predict continuous or numeric quantities. Here, let’s first fit a random forest model, which does not require all numeric input (see discussion here) and discuss how to use fit() and fit_xy(), as well as data descriptors.\nSecond, let’s fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip."
  },
  {
    "objectID": "content/learn/models/parsnip-ranger-glmnet/index.html#the-ames-housing-data",
    "href": "content/learn/models/parsnip-ranger-glmnet/index.html#the-ames-housing-data",
    "title": "Regression models two ways",
    "section": "The Ames housing data",
    "text": "The Ames housing data\nWe’ll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:\n\nlibrary(tidymodels)\n\ndata(ames)\n\nset.seed(4595)\ndata_split <- initial_split(ames, strata = \"Sale_Price\", prop = 0.75)\n\names_train <- training(data_split)\names_test  <- testing(data_split)\n\nThe use of the test set here is only for illustration; normally in a data analysis these data would be saved to the very end after many models have been evaluated."
  },
  {
    "objectID": "content/learn/models/parsnip-ranger-glmnet/index.html#random-forest",
    "href": "content/learn/models/parsnip-ranger-glmnet/index.html#random-forest",
    "title": "Regression models two ways",
    "section": "Random forest",
    "text": "Random forest\nWe’ll start by fitting a random forest model to a small set of parameters. Let’s create a model with the predictors Longitude, Latitude, Lot_Area, Neighborhood, and Year_Sold. A simple random forest model can be specified via:\n\nrf_defaults <- rand_forest(mode = \"regression\")\nrf_defaults\n#> Random Forest Model Specification (regression)\n#> \n#> Computational engine: ranger\n\nThe model will be fit with the ranger package by default. Since we didn’t add any extra arguments to fit, many of the arguments will be set to their defaults from the function ranger::ranger(). The help pages for the model function describe the default parameters and you can also use the translate() function to check out such details.\nThe parsnip package provides two different interfaces to fit a model:\n\nthe formula interface (fit()), and\nthe non-formula interface (fit_xy()).\n\nLet’s start with the non-formula interface:\n\npreds <- c(\"Longitude\", \"Latitude\", \"Lot_Area\", \"Neighborhood\", \"Year_Sold\")\n\nrf_xy_fit <- \n  rf_defaults %>%\n  set_engine(\"ranger\") %>%\n  fit_xy(\n    x = ames_train[, preds],\n    y = log10(ames_train$Sale_Price)\n  )\n\nrf_xy_fit\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  500 \n#> Sample size:                      2197 \n#> Number of independent variables:  5 \n#> Mtry:                             2 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       0.008500188 \n#> R squared (OOB):                  0.7239116\n\nThe non-formula interface doesn’t do anything to the predictors before passing them to the underlying model function. This particular model does not require indicator variables (sometimes called “dummy variables”) to be created prior to fitting the model. Note that the output shows “Number of independent variables: 5”.\nFor regression models, we can use the basic predict() method, which returns a tibble with a column named .pred:\n\ntest_results <- \n  ames_test %>%\n  select(Sale_Price) %>%\n  mutate(Sale_Price = log10(Sale_Price)) %>%\n  bind_cols(\n    predict(rf_xy_fit, new_data = ames_test[, preds])\n  )\ntest_results %>% slice(1:5)\n#> # A tibble: 5 × 2\n#>   Sale_Price .pred\n#>        <dbl> <dbl>\n#> 1       5.39  5.25\n#> 2       5.28  5.29\n#> 3       5.23  5.26\n#> 4       5.21  5.30\n#> 5       5.60  5.51\n\n# summarize performance\ntest_results %>% metrics(truth = Sale_Price, estimate = .pred) \n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      0.0945\n#> 2 rsq     standard      0.733 \n#> 3 mae     standard      0.0629\n\nNote that:\n\nIf the model required indicator variables, we would have to create them manually prior to using fit() (perhaps using the recipes package).\nWe had to manually log the outcome prior to modeling.\n\nNow, for illustration, let’s use the formula method using some new parameter values:\n\nrand_forest(mode = \"regression\", mtry = 3, trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3,      x), num.trees = ~1000, num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  1000 \n#> Sample size:                      2197 \n#> Number of independent variables:  5 \n#> Mtry:                             3 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       0.008402569 \n#> R squared (OOB):                  0.7270823\n\nSuppose that we would like to use the randomForest package instead of ranger. To do so, the only part of the syntax that needs to change is the set_engine() argument:\n\nrand_forest(mode = \"regression\", mtry = 3, trees = 1000) %>%\n  set_engine(\"randomForest\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n#> parsnip model object\n#> \n#> \n#> Call:\n#>  randomForest(x = maybe_data_frame(x), y = y, ntree = ~1000, mtry = min_cols(~3,      x)) \n#>                Type of random forest: regression\n#>                      Number of trees: 1000\n#> No. of variables tried at each split: 3\n#> \n#>           Mean of squared residuals: 0.008472074\n#>                     % Var explained: 72.47\n\nLook at the formula code that was printed out; one function uses the argument name ntree and the other uses num.trees. The parsnip models don’t require you to know the specific names of the main arguments.\nNow suppose that we want to modify the value of mtry based on the number of predictors in the data. Usually, a good default value is floor(sqrt(num_predictors)) but a pure bagging model requires an mtry value equal to the total number of parameters. There may be cases where you may not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or a variable filter) so this might be difficult to know exactly ahead of time when you write your code.\nWhen the model it being fit by parsnip, data descriptors are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using rand_forest()), the values of the arguments that you give it are immediately evaluated unless you delay them. To delay the evaluation of any argument, you can used rlang::expr() to make an expression.\nTwo relevant data descriptors for our example model are:\n\n.preds(): the number of predictor variables in the data set that are associated with the predictors prior to dummy variable creation.\n.cols(): the number of predictor columns after dummy variables (or other encodings) are created.\n\nSince ranger won’t create indicator values, .preds() would be appropriate for mtry for a bagging model.\nFor example, let’s use an expression with the .preds() descriptor to fit a bagging model:\n\nrand_forest(mode = \"regression\", mtry = .preds(), trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~.preds(),      x), num.trees = ~1000, num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  1000 \n#> Sample size:                      2197 \n#> Number of independent variables:  5 \n#> Mtry:                             5 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       0.00867085 \n#> R squared (OOB):                  0.7183685"
  },
  {
    "objectID": "content/learn/models/parsnip-ranger-glmnet/index.html#regularized-regression",
    "href": "content/learn/models/parsnip-ranger-glmnet/index.html#regularized-regression",
    "title": "Regression models two ways",
    "section": "Regularized regression",
    "text": "Regularized regression\nA linear model might work for this data set as well. We can use the linear_reg() parsnip model. There are two engines that can perform regularization/penalization, the glmnet and sparklyr packages. Let’s use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used.\nWhen regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won’t do that automatically so we will need to do this ourselves. We’ll use the recipes package for these steps.\n\nnorm_recipe <- \n  recipe(\n    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, \n    data = ames_train\n  ) %>%\n  step_other(Neighborhood) %>% \n  step_dummy(all_nominal()) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors()) %>%\n  step_log(Sale_Price, base = 10) %>% \n  # estimate the means and standard deviations\n  prep(training = ames_train, retain = TRUE)\n\n# Now let's fit the model using the processed version of the data\n\nglmn_fit <- \n  linear_reg(penalty = 0.001, mixture = 0.5) %>% \n  set_engine(\"glmnet\") %>%\n  fit(Sale_Price ~ ., data = bake(norm_recipe, new_data = NULL))\nglmn_fit\n#> parsnip model object\n#> \n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0.5) \n#> \n#>    Df  %Dev   Lambda\n#> 1   0  0.00 0.138300\n#> 2   1  1.96 0.126000\n#> 3   1  3.72 0.114800\n#> 4   1  5.28 0.104600\n#> 5   2  7.07 0.095320\n#> 6   3  9.64 0.086850\n#> 7   4 12.58 0.079140\n#> 8   5 15.45 0.072110\n#> 9   5 17.93 0.065700\n#> 10  7 20.81 0.059860\n#> 11  7 23.51 0.054550\n#> 12  7 25.82 0.049700\n#> 13  8 28.20 0.045290\n#> 14  8 30.31 0.041260\n#> 15  8 32.12 0.037600\n#> 16  8 33.66 0.034260\n#> 17  8 34.97 0.031210\n#> 18  8 36.08 0.028440\n#> 19  8 37.02 0.025910\n#> 20  9 37.90 0.023610\n#> 21  9 38.65 0.021510\n#> 22  9 39.29 0.019600\n#> 23  9 39.83 0.017860\n#> 24  9 40.28 0.016270\n#> 25 10 40.68 0.014830\n#> 26 11 41.06 0.013510\n#> 27 11 41.38 0.012310\n#> 28 11 41.65 0.011220\n#> 29 11 41.88 0.010220\n#> 30 12 42.09 0.009313\n#> 31 12 42.27 0.008486\n#> 32 12 42.43 0.007732\n#> 33 12 42.56 0.007045\n#> 34 12 42.66 0.006419\n#> 35 12 42.75 0.005849\n#> 36 12 42.83 0.005329\n#> 37 12 42.90 0.004856\n#> 38 12 42.95 0.004424\n#> 39 12 42.99 0.004031\n#> 40 12 43.03 0.003673\n#> 41 12 43.06 0.003347\n#> 42 12 43.09 0.003050\n#> 43 12 43.11 0.002779\n#> 44 12 43.13 0.002532\n#> 45 12 43.15 0.002307\n#> 46 12 43.16 0.002102\n#> 47 12 43.17 0.001915\n#> 48 12 43.18 0.001745\n#> 49 12 43.19 0.001590\n#> 50 12 43.19 0.001449\n#> 51 12 43.20 0.001320\n#> 52 12 43.20 0.001203\n#> 53 12 43.21 0.001096\n#> 54 12 43.21 0.000999\n#> 55 12 43.21 0.000910\n#> 56 12 43.21 0.000829\n#> 57 12 43.22 0.000755\n#> 58 12 43.22 0.000688\n#> 59 12 43.22 0.000627\n#> 60 12 43.22 0.000571\n#> 61 12 43.22 0.000521\n#> 62 12 43.22 0.000474\n#> 63 12 43.22 0.000432\n#> 64 12 43.22 0.000394\n#> 65 12 43.22 0.000359\n\nIf penalty were not specified, all of the lambda values would be computed.\nTo get the predictions for this specific value of lambda (aka penalty):\n\n# First, get the processed version of the test set predictors:\ntest_normalized <- bake(norm_recipe, new_data = ames_test, all_predictors())\n\ntest_results <- \n  test_results %>%\n  rename(`random forest` = .pred) %>%\n  bind_cols(\n    predict(glmn_fit, new_data = test_normalized) %>%\n      rename(glmnet = .pred)\n  )\ntest_results\n#> # A tibble: 733 × 3\n#>    Sale_Price `random forest` glmnet\n#>         <dbl>           <dbl>  <dbl>\n#>  1       5.39            5.25   5.16\n#>  2       5.28            5.29   5.27\n#>  3       5.23            5.26   5.24\n#>  4       5.21            5.30   5.24\n#>  5       5.60            5.51   5.24\n#>  6       5.32            5.29   5.26\n#>  7       5.17            5.14   5.18\n#>  8       5.06            5.13   5.17\n#>  9       4.98            5.01   5.18\n#> 10       5.11            5.14   5.19\n#> # … with 723 more rows\n\ntest_results %>% metrics(truth = Sale_Price, estimate = glmnet) \n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      0.142 \n#> 2 rsq     standard      0.391 \n#> 3 mae     standard      0.0979\n\ntest_results %>% \n  gather(model, prediction, -Sale_Price) %>% \n  ggplot(aes(x = prediction, y = Sale_Price)) + \n  geom_abline(col = \"green\", lty = 2) + \n  geom_point(alpha = .4) + \n  facet_wrap(~model) + \n  coord_fixed()\n\n\n\n\n\n\n\n\nThis final plot compares the performance of the random forest and regularized regression models."
  },
  {
    "objectID": "content/learn/models/parsnip-ranger-glmnet/index.html#session-information",
    "href": "content/learn/models/parsnip-ranger-glmnet/index.html#session-information",
    "title": "Regression models two ways",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  broom        * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials        * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr        * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2      * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  glmnet       * 4.1-4      2022-04-15 [1] CRAN (R 4.2.0)\n#>  infer        * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip      * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr        * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  randomForest * 4.7-1.1    2022-05-23 [1] CRAN (R 4.2.0)\n#>  ranger       * 0.14.1     2022-06-18 [1] CRAN (R 4.2.0)\n#>  recipes      * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang          1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample      * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble       * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels   * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune         * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows    * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick    * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/models/pls/index.html",
    "href": "content/learn/models/pls/index.html",
    "title": "Multivariate analysis using partial least squares",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: modeldata, pls, and tidymodels.\n“Multivariate analysis” usually refers to multiple outcomes being modeled, analyzed, and/or predicted. There are multivariate versions of many common statistical tools. For example, suppose there was a data set with columns y1 and y2 representing two outcomes to be predicted. The lm() function would look something like:\n\nlm(cbind(y1, y2) ~ ., data = dat)\n\nThis cbind() call is pretty awkward and is a consequence of how the traditional formula infrastructure works. The recipes package is a lot easier to work with! This article demonstrates how to model multiple outcomes.\nThe data that we’ll use has three outcomes. From ?modeldata::meats:\n\n“These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.\n\n\n“For each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.”\n\nThe goal is to predict the proportion of the three substances using the chemistry test. There can often be a high degree of between-variable correlations in predictors, and that is certainly the case here.\nTo start, let’s take the two data matrices (called endpoints and absorp) and bind them together in a data frame:\n\nlibrary(modeldata)\ndata(meats)\n\nThe three outcomes have fairly high correlations also."
  },
  {
    "objectID": "content/learn/models/pls/index.html#preprocessing-the-data",
    "href": "content/learn/models/pls/index.html#preprocessing-the-data",
    "title": "Multivariate analysis using partial least squares",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nIf the outcomes can be predicted using a linear model, partial least squares (PLS) is an ideal method. PLS models the data as a function of a set of unobserved latent variables that are derived in a manner similar to principal component analysis (PCA).\nPLS, unlike PCA, also incorporates the outcome data when creating the PLS components. Like PCA, it tries to maximize the variance of the predictors that are explained by the components but it also tries to simultaneously maximize the correlation between those components and the outcomes. In this way, PLS chases variation of the predictors and outcomes.\nSince we are working with variances and covariances, we need to standardize the data. The recipe will center and scale all of the variables.\nMany base R functions that deal with multivariate outcomes using a formula require the use of cbind() on the left-hand side of the formula to work with the traditional formula methods. In tidymodels, recipes do not; the outcomes can be symbolically “added” together on the left-hand side:\n\nnorm_rec <- \n  recipe(water + fat + protein ~ ., data = meats) %>%\n  step_normalize(everything()) \n\nBefore we can finalize the PLS model, the number of PLS components to retain must be determined. This can be done using performance metrics such as the root mean squared error. However, we can also calculate the proportion of variance explained by the components for the predictors and each of the outcomes. This allows an informed choice to be made based on the level of evidence that the situation requires.\nSince the data set isn’t large, let’s use resampling to measure these proportions. With ten repeats of 10-fold cross-validation, we build the PLS model on 90% of the data and evaluate on the heldout 10%. For each of the 100 models, we extract and save the proportions.\nThe folds can be created using the rsample package and the recipe can be estimated for each resample using the prepper() function:\n\nset.seed(57343)\nfolds <- vfold_cv(meats, repeats = 10)\n\nfolds <- \n  folds %>%\n  mutate(recipes = map(splits, prepper, recipe = norm_rec))"
  },
  {
    "objectID": "content/learn/models/pls/index.html#partial-least-squares",
    "href": "content/learn/models/pls/index.html#partial-least-squares",
    "title": "Multivariate analysis using partial least squares",
    "section": "Partial least squares",
    "text": "Partial least squares\nThe complicated parts for moving forward are:\n\nFormatting the predictors and outcomes into the format that the pls package requires, and\nEstimating the proportions.\n\nFor the first part, the standardized outcomes and predictors need to be formatted into two separate matrices. Since we used retain = TRUE when prepping the recipes, we can bake() with new_data = NULl to get the processed data back out. To save the data as a matrix, the option composition = \"matrix\" will avoid saving the data as tibbles and use the required format.\nThe pls package expects a simple formula to specify the model, but each side of the formula should represent a matrix. In other words, we need a data set with two columns where each column is a matrix. The secret to doing this is to “protect” the two matrices using I() when adding them to the data frame.\nThe calculation for the proportion of variance explained is straightforward for the predictors; the function pls::explvar() will compute that. For the outcomes, the process is more complicated. A ready-made function to compute these is not obvious but there is some code inside of the summary function to do the computation (see below).\nThe function get_var_explained() shown here will do all these computations and return a data frame with columns components, source (for the predictors, water, etc), and the proportion of variance that is explained by the components.\n\nlibrary(pls)\n\nget_var_explained <- function(recipe, ...) {\n  \n  # Extract the predictors and outcomes into their own matrices\n  y_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_outcomes())\n  x_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_predictors())\n  \n  # The pls package prefers the data in a data frame where the outcome\n  # and predictors are in _matrices_. To make sure this is formatted\n  # properly, use the `I()` function to inhibit `data.frame()` from making\n  # all the individual columns. `pls_format` should have two columns.\n  pls_format <- data.frame(\n    endpoints = I(y_mat),\n    measurements = I(x_mat)\n  )\n  # Fit the model\n  mod <- plsr(endpoints ~ measurements, data = pls_format)\n  \n  # Get the proportion of the predictor variance that is explained\n  # by the model for different number of components. \n  xve <- explvar(mod)/100 \n\n  # To do the same for the outcome, it is more complex. This code \n  # was extracted from pls:::summary.mvr. \n  explained <- \n    drop(pls::R2(mod, estimate = \"train\", intercept = FALSE)$val) %>% \n    # transpose so that components are in rows\n    t() %>% \n    as_tibble() %>%\n    # Add the predictor proportions\n    mutate(predictors = cumsum(xve) %>% as.vector(),\n           components = seq_along(xve)) %>%\n    # Put into a tidy format that is tall\n    pivot_longer(\n      cols = c(-components),\n      names_to = \"source\",\n      values_to = \"proportion\"\n    )\n}\n\nWe compute this data frame for each resample and save the results in the different columns.\n\nfolds <- \n  folds %>%\n  mutate(var = map(recipes, get_var_explained),\n         var = unname(var))\n\nTo extract and aggregate these data, simple row binding can be used to stack the data vertically. Most of the action happens in the first 15 components so let’s filter the data and compute the average proportion.\n\nvariance_data <- \n  bind_rows(folds[[\"var\"]]) %>%\n  filter(components <= 15) %>%\n  group_by(components, source) %>%\n  summarize(proportion = mean(proportion))\n#> `summarise()` has grouped output by 'components'. You can override\n#> using the `.groups` argument.\n\nThe plot below shows that, if the protein measurement is important, you might require 10 or so components to achieve a good representation of that outcome. Note that the predictor variance is captured extremely well using a single component. This is due to the high degree of correlation in those data.\n\nggplot(variance_data, aes(x = components, y = proportion, col = source)) + \n  geom_line(alpha = 0.5, size = 1.2) + \n  geom_point() \n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "content/learn/models/pls/index.html#session-information",
    "href": "content/learn/models/pls/index.html#session-information",
    "title": "Multivariate analysis using partial least squares",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  modeldata  * 1.0.1.9000 2023-01-04 [1] local\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  pls        * 2.8-1      2022-07-16 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/models/sub-sampling/index.html",
    "href": "content/learn/models/sub-sampling/index.html",
    "title": "Subsampling for class imbalances",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: discrim, klaR, readr, ROSE, themis, and tidymodels.\nSubsampling a training set, either undersampling or oversampling the appropriate class or classes, can be a helpful approach to dealing with classification data where one or more classes occur very infrequently. In such a situation (without compensating for it), most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance.\nThis article describes subsampling for dealing with class imbalances. For better understanding, some knowledge of classification metrics like sensitivity, specificity, and receiver operating characteristic curves is required. See Section 3.2.2 in Kuhn and Johnson (2019) for more information on these metrics."
  },
  {
    "objectID": "content/learn/models/sub-sampling/index.html#simulated-data",
    "href": "content/learn/models/sub-sampling/index.html#simulated-data",
    "title": "Subsampling for class imbalances",
    "section": "Simulated data",
    "text": "Simulated data\nConsider a two-class problem where the first class has a very low rate of occurrence. The data were simulated and can be imported into R using the code below:\n\nimbal_data <- \n  readr::read_csv(\"https://bit.ly/imbal_data\") %>% \n  mutate(Class = factor(Class))\ndim(imbal_data)\n#> [1] 1200   16\ntable(imbal_data$Class)\n#> \n#> Class1 Class2 \n#>     60   1140\n\nIf “Class1” is the event of interest, it is very likely that a classification model would be able to achieve very good specificity since almost all of the data are of the second class. Sensitivity, however, would likely be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.\nOne result of class imbalance when there are two classes is that the default probability cutoff of 50% is inappropriate; a different cutoff that is more extreme might be able to achieve good performance."
  },
  {
    "objectID": "content/learn/models/sub-sampling/index.html#subsampling-the-data",
    "href": "content/learn/models/sub-sampling/index.html#subsampling-the-data",
    "title": "Subsampling for class imbalances",
    "section": "Subsampling the data",
    "text": "Subsampling the data\nOne way to alleviate this issue is to subsample the data. There are a number of ways to do this but the most simple one is to sample down (undersample) the majority class data until it occurs with the same frequency as the minority class. While it may seem counterintuitive, throwing out a large percentage of your data can be effective at producing a useful model that can recognize both the majority and minority classes. In some cases, this even means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are better calibrated, meaning that the distributions of the class probabilities are more well behaved. As a result, the default 50% cutoff is much more likely to produce better sensitivity and specificity values than they would otherwise.\nLet’s explore subsampling using themis::step_rose() in a recipe for the simulated data. It uses the ROSE (random over sampling examples) method from Menardi, G. and Torelli, N. (2014). This is an example of an oversampling strategy, rather than undersampling.\nIn terms of workflow:\n\nIt is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce poor estimates of model performance.\nThe subsampling process should only be applied to the analysis set. The assessment set should reflect the event rates seen “in the wild” and, for this reason, the skip argument to step_downsample() and other subsampling recipes steps has a default of TRUE.\n\nHere is a simple recipe implementing oversampling:\n\nlibrary(tidymodels)\nlibrary(themis)\nimbal_rec <- \n  recipe(Class ~ ., data = imbal_data) %>%\n  step_rose(Class)\n\nFor a model, let’s use a quadratic discriminant analysis (QDA) model. From the discrim package, this model can be specified using:\n\nlibrary(discrim)\nqda_mod <- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) %>% \n  set_engine(\"klaR\")\n\nTo keep these objects bound together, they can be combined in a workflow:\n\nqda_rose_wflw <- \n  workflow() %>% \n  add_model(qda_mod) %>% \n  add_recipe(imbal_rec)\nqda_rose_wflw\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: discrim_regularized()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> 1 Recipe Step\n#> \n#> • step_rose()\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Regularized Discriminant Model Specification (classification)\n#> \n#> Main Arguments:\n#>   frac_common_cov = 0\n#>   frac_identity = 0\n#> \n#> Computational engine: klaR"
  },
  {
    "objectID": "content/learn/models/sub-sampling/index.html#model-performance",
    "href": "content/learn/models/sub-sampling/index.html#model-performance",
    "title": "Subsampling for class imbalances",
    "section": "Model performance",
    "text": "Model performance\nStratified, repeated 10-fold cross-validation is used to resample the model:\n\nset.seed(5732)\ncv_folds <- vfold_cv(imbal_data, strata = \"Class\", repeats = 5)\n\nTo measure model performance, let’s use two metrics:\n\nThe area under the ROC curve is an overall assessment of performance across all cutoffs. Values near one indicate very good results while values near 0.5 would imply that the model is very poor.\nThe J index (a.k.a. Youden’s J statistic) is sensitivity + specificity - 1. Values near one are once again best.\n\nIf a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities. The yardstick package will be used to compute these metrics.\n\ncls_metrics <- metric_set(roc_auc, j_index)\n\nNow, we train the models and generate the results using tune::fit_resamples():\n\nset.seed(2180)\nqda_rose_res <- fit_resamples(\n  qda_rose_wflw, \n  resamples = cv_folds, \n  metrics = cls_metrics\n)\n\ncollect_metrics(qda_rose_res)\n#> # A tibble: 2 × 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 j_index binary     0.813    50 0.0179  Preprocessor1_Model1\n#> 2 roc_auc binary     0.954    50 0.00461 Preprocessor1_Model1\n\nWhat do the results look like without using ROSE? We can create another workflow and fit the QDA model along the same resamples:\n\nqda_wflw <- \n  workflow() %>% \n  add_model(qda_mod) %>% \n  add_formula(Class ~ .)\n\nset.seed(2180)\nqda_only_res <- fit_resamples(qda_wflw, resamples = cv_folds, metrics = cls_metrics)\ncollect_metrics(qda_only_res)\n#> # A tibble: 2 × 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 j_index binary     0.250    50 0.0288  Preprocessor1_Model1\n#> 2 roc_auc binary     0.953    50 0.00479 Preprocessor1_Model1\n\nIt looks like ROSE helped a lot, especially with the J-index. Class imbalance sampling methods tend to greatly improve metrics based on the hard class predictions (i.e., the categorical predictions) because the default cutoff tends to be a better balance of sensitivity and specificity.\nLet’s plot the metrics for each resample to see how the individual results changed.\n\nno_sampling <- \n  qda_only_res %>% \n  collect_metrics(summarize = FALSE) %>% \n  dplyr::select(-.estimator) %>% \n  mutate(sampling = \"no_sampling\")\n\nwith_sampling <- \n  qda_rose_res %>% \n  collect_metrics(summarize = FALSE) %>% \n  dplyr::select(-.estimator) %>% \n  mutate(sampling = \"rose\")\n\nbind_rows(no_sampling, with_sampling) %>% \n  mutate(label = paste(id2, id)) %>%  \n  ggplot(aes(x = sampling, y = .estimate, group = label)) + \n  geom_line(alpha = .4) + \n  facet_wrap(~ .metric, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThis visually demonstrates that the subsampling mostly affects metrics that use the hard class predictions."
  },
  {
    "objectID": "content/learn/models/sub-sampling/index.html#session-information",
    "href": "content/learn/models/sub-sampling/index.html#session-information",
    "title": "Subsampling for class imbalances",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  discrim    * 1.0.0.9000 2022-08-16 [1] Github (tidymodels/discrim@6060ce1)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  klaR       * 1.7-1      2022-06-27 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  readr      * 2.1.3      2022-10-01 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  ROSE       * 0.0-4      2021-06-14 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  themis     * 1.0.0      2022-07-02 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/index.html",
    "href": "content/learn/index.html",
    "title": "Learn",
    "section": "",
    "text": "After you know what you need to get started with tidymodels, you can learn more and go further. Find articles here to help you solve specific problems using the tidymodels framework. Articles are organized into four categories:"
  },
  {
    "objectID": "content/learn/index.html#perform-statistical-analyses",
    "href": "content/learn/index.html#perform-statistical-analyses",
    "title": "Learn",
    "section": "Perform Statistical Analyses",
    "text": "Perform Statistical Analyses\n\nCorrelation and regression fundamentals with tidy data principles: Analyze the results of correlation tests and simple regression models for many data sets at once\nK-means clustering with tidy data principles: Summarize clustering characteristics and estimate the best number of clusters for a data set.\nBootstrap resampling and tidy regression models: Apply bootstrap resampling to estimate uncertainty in model parameters.\nHypothesis testing using resampling and tidy data: Perform common hypothesis tests for statistical inference using flexible functions.\nStatistical analysis of contingency tables: Use tests of independence and goodness of fit to analyze tables of counts."
  },
  {
    "objectID": "content/learn/index.html#create-robust-models",
    "href": "content/learn/index.html#create-robust-models",
    "title": "Learn",
    "section": "Create Robust Models",
    "text": "Create Robust Models\n\nRegression models two ways: Create and train different kinds of regression models with different computational engines.\nClassification models using a neural network: Train a classification model and evaluate its performance.\nSubsampling for class imbalances: Improve model performance in imbalanced data sets through undersampling or oversampling.\nModeling time series with tidy resampling: Calculate performance estimates for time series forecasts using resampling.\nWorking with model coefficients: Create models that use coefficients, extract them from fitted models, and visualize them."
  },
  {
    "objectID": "content/learn/index.html#tune-compare-and-work-with-your-models",
    "href": "content/learn/index.html#tune-compare-and-work-with-your-models",
    "title": "Learn",
    "section": "Tune, Compare, and Work with Your Models",
    "text": "Tune, Compare, and Work with Your Models\n\nModel tuning via grid search: Choose hyperparameters for a model by training on a grid of many possible parameter values.\nNested resampling: Estimate the best hyperparameters for a model using nested resampling.\nIterative Bayesian optimization of a classification model: Identify the best hyperparameters for a model using Bayesian optimization of iterative search.\nTuning text models: Prepare text data for predictive modeling and tune with both grid and iterative search.\nCreating case weights based on time: Create models that use coefficients, extract them from fitted models, and visualize them."
  },
  {
    "objectID": "content/learn/index.html#create-custom-modeling-tools",
    "href": "content/learn/index.html#create-custom-modeling-tools",
    "title": "Learn",
    "section": "Create Custom Modeling Tools",
    "text": "Create Custom Modeling Tools\n\nCreate your own recipe step function: Write a new recipe step for data preprocessing.\nHow to build a parsnip model: Create a parsnip model function from an existing model implementation.\nCustom performance metrics: Create a new performance metric and integrate it with yardstick functions.\nHow to create a tuning parameter function: Build functions to use in tuning both quantitative and qualitative parameters.\nCreate your own broom tidier methods: Write tidy(), glance(), and augment() methods for new model objects."
  },
  {
    "objectID": "content/learn/work/bayes-opt/index.html",
    "href": "content/learn/work/bayes-opt/index.html",
    "title": "Iterative Bayesian optimization of a classification model",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: kernlab, modeldata, themis, and tidymodels.\nMany of the examples for model tuning focus on grid search. For that method, all the candidate tuning parameter combinations are defined prior to evaluation. Alternatively, iterative search can be used to analyze the existing tuning parameter results and then predict which tuning parameters to try next.\nThere are a variety of methods for iterative search and the focus in this article is on Bayesian optimization. For more information on this method, these resources might be helpful:\n\nPractical bayesian optimization of machine learning algorithms (2012). J Snoek, H Larochelle, and RP Adams. Advances in neural information.\nA Tutorial on Bayesian Optimization for Machine Learning (2018). R Adams.\nGaussian Processes for Machine Learning (2006). C E Rasmussen and C Williams.\nOther articles!"
  },
  {
    "objectID": "content/learn/work/bayes-opt/index.html#cell-segmenting-revisited",
    "href": "content/learn/work/bayes-opt/index.html#cell-segmenting-revisited",
    "title": "Iterative Bayesian optimization of a classification model",
    "section": "Cell segmenting revisited",
    "text": "Cell segmenting revisited\nTo demonstrate this approach to tuning models, let’s return to the cell segmentation data from the Getting Started article on resampling:\n\nlibrary(tidymodels)\nlibrary(modeldata)\n\n# Load data\ndata(cells)\n\nset.seed(2369)\ntr_te_split <- initial_split(cells %>% select(-case), prop = 3/4)\ncell_train <- training(tr_te_split)\ncell_test  <- testing(tr_te_split)\n\nset.seed(1697)\nfolds <- vfold_cv(cell_train, v = 10)"
  },
  {
    "objectID": "content/learn/work/bayes-opt/index.html#the-tuning-scheme",
    "href": "content/learn/work/bayes-opt/index.html#the-tuning-scheme",
    "title": "Iterative Bayesian optimization of a classification model",
    "section": "The tuning scheme",
    "text": "The tuning scheme\nSince the predictors are highly correlated, we can used a recipe to convert the original predictors to principal component scores. There is also slight class imbalance in these data; about 64% of the data are poorly segmented. To mitigate this, the data will be down-sampled at the end of the pre-processing so that the number of poorly and well segmented cells occur with equal frequency. We can use a recipe for all this pre-processing, but the number of principal components will need to be tuned so that we have enough (but not too many) representations of the data.\n\nlibrary(themis)\n\ncell_pre_proc <-\n  recipe(class ~ ., data = cell_train) %>%\n  step_YeoJohnson(all_predictors()) %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), num_comp = tune()) %>%\n  step_downsample(class)\n\nIn this analysis, we will use a support vector machine to model the data. Let’s use a radial basis function (RBF) kernel and tune its main parameter (\\(\\sigma\\)). Additionally, the main SVM parameter, the cost value, also needs optimization.\n\nsvm_mod <-\n  svm_rbf(mode = \"classification\", cost = tune(), rbf_sigma = tune()) %>%\n  set_engine(\"kernlab\")\n\nThese two objects (the recipe and model) will be combined into a single object via the workflow() function from the workflows package; this object will be used in the optimization process.\n\nsvm_wflow <-\n  workflow() %>%\n  add_model(svm_mod) %>%\n  add_recipe(cell_pre_proc)\n\nFrom this object, we can derive information about what parameters are slated to be tuned. A parameter set is derived by:\n\nsvm_set <- extract_parameter_set_dials(svm_wflow)\nsvm_set\n#> Collection of 3 parameters for tuning\n#> \n#>  identifier      type    object\n#>        cost      cost nparam[+]\n#>   rbf_sigma rbf_sigma nparam[+]\n#>    num_comp  num_comp nparam[+]\n\nThe default range for the number of PCA components is rather small for this data set. A member of the parameter set can be modified using the update() function. Let’s constrain the search to one to twenty components by updating the num_comp parameter. Additionally, the lower bound of this parameter is set to zero which specifies that the original predictor set should also be evaluated (i.e., with no PCA step at all):\n\nsvm_set <- \n  svm_set %>% \n  update(num_comp = num_comp(c(0L, 20L)))"
  },
  {
    "objectID": "content/learn/work/bayes-opt/index.html#sequential-tuning",
    "href": "content/learn/work/bayes-opt/index.html#sequential-tuning",
    "title": "Iterative Bayesian optimization of a classification model",
    "section": "Sequential tuning",
    "text": "Sequential tuning\nBayesian optimization is a sequential method that uses a model to predict new candidate parameters for assessment. When scoring potential parameter value, the mean and variance of performance are predicted. The strategy used to define how these two statistical quantities are used is defined by an acquisition function.\nFor example, one approach for scoring new candidates is to use a confidence bound. Suppose accuracy is being optimized. For a metric that we want to maximize, a lower confidence bound can be used. The multiplier on the standard error (denoted as \\(\\kappa\\)) is a value that can be used to make trade-offs between exploration and exploitation.\n\nExploration means that the search will consider candidates in untested space.\nExploitation focuses in areas where the previous best results occurred.\n\nThe variance predicted by the Bayesian model is mostly spatial variation; the value will be large for candidate values that are not close to values that have already been evaluated. If the standard error multiplier is high, the search process will be more likely to avoid areas without candidate values in the vicinity.\nWe’ll use another acquisition function, expected improvement, that determines which candidates are likely to be helpful relative to the current best results. This is the default acquisition function. More information on these functions can be found in the package vignette for acquisition functions.\n\nset.seed(12)\nsearch_res <-\n  svm_wflow %>% \n  tune_bayes(\n    resamples = folds,\n    # To use non-default parameter ranges\n    param_info = svm_set,\n    # Generate five at semi-random to start\n    initial = 5,\n    iter = 50,\n    # How to measure performance?\n    metrics = metric_set(roc_auc),\n    control = control_bayes(no_improve = 30, verbose = TRUE)\n  )\n#> \n#> ❯  Generating a set of 5 initial parameter results\n#> ✓ Initialization complete\n#> \n#> \n#> ── Iteration 1 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8794 (@iter 0)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=29.2, rbf_sigma=0.707, num_comp=17\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.7883 (+/-0.0111)\n#> \n#> ── Iteration 2 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8794 (@iter 0)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=30.4, rbf_sigma=0.0087, num_comp=13\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:    roc_auc=0.8954 (+/-0.0101)\n#> \n#> ── Iteration 3 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8954 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.0374, rbf_sigma=0.00425, num_comp=11\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8749 (+/-0.0123)\n#> \n#> ── Iteration 4 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8954 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=28.8, rbf_sigma=0.00386, num_comp=4\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8738 (+/-0.012)\n#> \n#> ── Iteration 5 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8954 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=21.5, rbf_sigma=0.0738, num_comp=11\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8523 (+/-0.0115)\n#> \n#> ── Iteration 6 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8954 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=24.1, rbf_sigma=0.0111, num_comp=18\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8829 (+/-0.0101)\n#> \n#> ── Iteration 7 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8954 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=1.48, rbf_sigma=0.00629, num_comp=13\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8801 (+/-0.0118)\n#> \n#> ── Iteration 8 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8954 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=25.3, rbf_sigma=0.011, num_comp=11\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:    roc_auc=0.8985 (+/-0.0102)\n#> \n#> ── Iteration 9 ───────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8985 (@iter 8)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=14.8, rbf_sigma=0.628, num_comp=0\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.7852 (+/-0.0173)\n#> \n#> ── Iteration 10 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8985 (@iter 8)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=30.1, rbf_sigma=0.0102, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:    roc_auc=0.899 (+/-0.00955)\n#> \n#> ── Iteration 11 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.899 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=25.1, rbf_sigma=0.0111, num_comp=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:    roc_auc=0.8994 (+/-0.00996)\n#> \n#> ── Iteration 12 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8994 (@iter 11)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=28.3, rbf_sigma=0.0118, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8989 (+/-0.00928)\n#> \n#> ── Iteration 13 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8994 (@iter 11)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=28.5, rbf_sigma=0.0026, num_comp=19\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8879 (+/-0.00951)\n#> \n#> ── Iteration 14 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8994 (@iter 11)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=30.4, rbf_sigma=0.00245, num_comp=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.886 (+/-0.0111)\n#> \n#> ── Iteration 15 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8994 (@iter 11)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=31.5, rbf_sigma=0.0179, num_comp=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8964 (+/-0.00966)\n#> \n#> ── Iteration 16 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8994 (@iter 11)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=31.1, rbf_sigma=0.00933, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8994 (+/-0.00968)\n#> \n#> ── Iteration 17 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8994 (@iter 11)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=27.3, rbf_sigma=0.00829, num_comp=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:    roc_auc=0.8996 (+/-0.00997)\n#> \n#> ── Iteration 18 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=31.7, rbf_sigma=0.00363, num_comp=12\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8909 (+/-0.00974)\n#> \n#> ── Iteration 19 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=29.6, rbf_sigma=0.0119, num_comp=8\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8964 (+/-0.0104)\n#> \n#> ── Iteration 20 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=23.6, rbf_sigma=0.0121, num_comp=0\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8636 (+/-0.0122)\n#> \n#> ── Iteration 21 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=27.6, rbf_sigma=0.00824, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8993 (+/-0.00961)\n#> \n#> ── Iteration 22 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=27.6, rbf_sigma=0.00901, num_comp=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8993 (+/-0.01)\n#> \n#> ── Iteration 23 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=24.1, rbf_sigma=0.0133, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8988 (+/-0.00935)\n#> \n#> ── Iteration 24 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=18.8, rbf_sigma=0.00058, num_comp=20\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8784 (+/-0.0112)\n#> \n#> ── Iteration 25 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=29.3, rbf_sigma=0.00958, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8994 (+/-0.00959)\n#> \n#> ── Iteration 26 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=30.6, rbf_sigma=0.00841, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8993 (+/-0.00949)\n#> \n#> ── Iteration 27 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00169, rbf_sigma=0.0201, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8794 (+/-0.0119)\n#> \n#> ── Iteration 28 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.0012, rbf_sigma=0.000867, num_comp=20\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.3452 (+/-0.116)\n#> \n#> ── Iteration 29 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00128, rbf_sigma=0.0138, num_comp=4\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8671 (+/-0.0132)\n#> \n#> ── Iteration 30 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.0319, rbf_sigma=0.0279, num_comp=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8805 (+/-0.0121)\n#> \n#> ── Iteration 31 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.0011, rbf_sigma=0.00787, num_comp=8\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8762 (+/-0.0121)\n#> \n#> ── Iteration 32 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=7.06, rbf_sigma=0.00645, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.89 (+/-0.0102)\n#> \n#> ── Iteration 33 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.000998, rbf_sigma=0.305, num_comp=7\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8757 (+/-0.0126)\n#> \n#> ── Iteration 34 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00615, rbf_sigma=0.0134, num_comp=8\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.877 (+/-0.0122)\n#> \n#> ── Iteration 35 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.208, rbf_sigma=0.00946, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8793 (+/-0.0122)\n#> \n#> ── Iteration 36 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=31.6, rbf_sigma=0.00481, num_comp=15\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8961 (+/-0.00885)\n#> \n#> ── Iteration 37 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00108, rbf_sigma=0.653, num_comp=11\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.3649 (+/-0.106)\n#> \n#> ── Iteration 38 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00156, rbf_sigma=0.13, num_comp=5\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8816 (+/-0.0121)\n#> \n#> ── Iteration 39 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=7.03, rbf_sigma=0.235, num_comp=16\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8572 (+/-0.0117)\n#> \n#> ── Iteration 40 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00466, rbf_sigma=0.211, num_comp=1\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.7714 (+/-0.0105)\n#> \n#> ── Iteration 41 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.0357, rbf_sigma=0.00126, num_comp=1\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.7725 (+/-0.0106)\n#> \n#> ── Iteration 42 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=23.1, rbf_sigma=0.0332, num_comp=16\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8612 (+/-0.0137)\n#> \n#> ── Iteration 43 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=3.56, rbf_sigma=0.0294, num_comp=3\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8724 (+/-0.0126)\n#> \n#> ── Iteration 44 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00168, rbf_sigma=0.0337, num_comp=7\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8791 (+/-0.0123)\n#> \n#> ── Iteration 45 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00128, rbf_sigma=0.00258, num_comp=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.3444 (+/-0.114)\n#> \n#> ── Iteration 46 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.506, rbf_sigma=0.000548, num_comp=4\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.8653 (+/-0.0131)\n#> \n#> ── Iteration 47 ──────────────────────────────────────────────────────\n#> \n#> i Current best:      roc_auc=0.8996 (@iter 17)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i cost=0.00142, rbf_sigma=0.204, num_comp=18\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:    roc_auc=0.3586 (+/-0.113)\n#> ! No improvement for 30 iterations; returning current results.\n\nThe resulting tibble is a stacked set of rows of the rsample object with an additional column for the iteration number:\n\nsearch_res\n#> # Tuning results\n#> # 10-fold cross-validation \n#> # A tibble: 480 × 5\n#>    splits             id     .metrics         .notes           .iter\n#>    <list>             <chr>  <list>           <list>           <int>\n#>  1 <split [1362/152]> Fold01 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  2 <split [1362/152]> Fold02 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  3 <split [1362/152]> Fold03 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  4 <split [1362/152]> Fold04 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  5 <split [1363/151]> Fold05 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  6 <split [1363/151]> Fold06 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  7 <split [1363/151]> Fold07 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  8 <split [1363/151]> Fold08 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  9 <split [1363/151]> Fold09 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#> 10 <split [1363/151]> Fold10 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#> # … with 470 more rows\n\nAs with grid search, we can summarize the results over resamples:\n\nestimates <- \n  collect_metrics(search_res) %>% \n  arrange(.iter)\n\nestimates\n#> # A tibble: 52 × 10\n#>        cost  rbf_sigma num_c…¹ .metric .esti…²  mean     n std_err .config .iter\n#>       <dbl>      <dbl>   <int> <chr>   <chr>   <dbl> <int>   <dbl> <chr>   <int>\n#>  1  0.00383    2.72e-6      17 roc_auc binary  0.348    10  0.114  Prepro…     0\n#>  2  0.250      1.55e-2       7 roc_auc binary  0.879    10  0.0122 Prepro…     0\n#>  3  0.0372     1.02e-9       3 roc_auc binary  0.242    10  0.0574 Prepro…     0\n#>  4  1.28       8.13e-8       8 roc_auc binary  0.344    10  0.114  Prepro…     0\n#>  5 10.3        1.37e-3      14 roc_auc binary  0.877    10  0.0117 Prepro…     0\n#>  6 29.2        7.07e-1      17 roc_auc binary  0.788    10  0.0111 Iter1       1\n#>  7 30.4        8.70e-3      13 roc_auc binary  0.895    10  0.0101 Iter2       2\n#>  8  0.0374     4.25e-3      11 roc_auc binary  0.875    10  0.0123 Iter3       3\n#>  9 28.8        3.86e-3       4 roc_auc binary  0.874    10  0.0120 Iter4       4\n#> 10 21.5        7.38e-2      11 roc_auc binary  0.852    10  0.0115 Iter5       5\n#> # … with 42 more rows, and abbreviated variable names ¹​num_comp, ²​.estimator\n\nThe best performance of the initial set of candidate values was AUC = 0.8793995. The best results were achieved at iteration 17 with a corresponding AUC value of 0.8995534. The five best results are:\n\nshow_best(search_res, metric = \"roc_auc\")\n#> # A tibble: 5 × 10\n#>    cost rbf_sigma num_comp .metric .estimator  mean     n std_err .config .iter\n#>   <dbl>     <dbl>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>   <int>\n#> 1  27.3   0.00829        9 roc_auc binary     0.900    10 0.00997 Iter17     17\n#> 2  29.3   0.00958       10 roc_auc binary     0.899    10 0.00959 Iter25     25\n#> 3  25.1   0.0111         9 roc_auc binary     0.899    10 0.00996 Iter11     11\n#> 4  31.1   0.00933       10 roc_auc binary     0.899    10 0.00968 Iter16     16\n#> 5  27.6   0.00901        9 roc_auc binary     0.899    10 0.0100  Iter22     22\n\nA plot of the search iterations can be created via:\n\nautoplot(search_res, type = \"performance\")\n\n\n\n\n\n\n\n\nThere are many parameter combinations have roughly equivalent results.\nHow did the parameters change over iterations?\n\nautoplot(search_res, type = \"parameters\") + \n  labs(x = \"Iterations\", y = NULL)"
  },
  {
    "objectID": "content/learn/work/bayes-opt/index.html#session-information",
    "href": "content/learn/work/bayes-opt/index.html#session-information",
    "title": "Iterative Bayesian optimization of a classification model",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  kernlab    * 0.9-31     2022-06-09 [1] CRAN (R 4.2.0)\n#>  modeldata  * 1.0.1.9000 2023-01-04 [1] local\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang      * 1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  themis     * 1.0.0      2022-07-02 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html",
    "href": "content/learn/work/tune-svm/index.html",
    "title": "Model tuning via grid search",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: kernlab, mlbench, and tidymodels.\nThis article demonstrates how to tune a model using grid search. Many models have hyperparameters that can’t be learned directly from a single data set when training the model. Instead, we can train many models in a grid of possible hyperparameter values and see which ones turn out best."
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#example-data",
    "href": "content/learn/work/tune-svm/index.html#example-data",
    "title": "Model tuning via grid search",
    "section": "Example data",
    "text": "Example data\nTo demonstrate model tuning, we’ll use the Ionosphere data in the mlbench package:\n\nlibrary(tidymodels)\nlibrary(mlbench)\ndata(Ionosphere)\n\nFrom ?Ionosphere:\n\nThis radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. “good” radar returns are those showing evidence of some type of structure in the ionosphere. “bad” returns are those that do not; their signals pass through the ionosphere.\n\n\nReceived signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal. See cited below for more details.\n\nThere are 43 predictors and a factor outcome. Two of the predictors are factors (V1 and V2) and the rest are numeric variables that have been scaled to a range of -1 to 1. Note that the two factor predictors have sparse distributions:\n\ntable(Ionosphere$V1)\n#> \n#>   0   1 \n#>  38 313\ntable(Ionosphere$V2)\n#> \n#>   0 \n#> 351\n\nThere’s no point of putting V2 into any model since is is a zero-variance predictor. V1 is not but it could be if the resampling process ends up sampling all of the same value. Is this an issue? It might be since the standard R formula infrastructure fails when there is only a single observed value:\n\nglm(Class ~ ., data = Ionosphere, family = binomial)\n\n# Surprisingly, this doesn't help: \n\nglm(Class ~ . - V2, data = Ionosphere, family = binomial)\n\nLet’s remove these two problematic variables:\n\nIonosphere <- Ionosphere %>% select(-V1, -V2)"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#inputs-for-the-search",
    "href": "content/learn/work/tune-svm/index.html#inputs-for-the-search",
    "title": "Model tuning via grid search",
    "section": "Inputs for the search",
    "text": "Inputs for the search\nTo demonstrate, we’ll fit a radial basis function support vector machine to these data and tune the SVM cost parameter and the \\(\\sigma\\) parameter in the kernel function:\n\nsvm_mod <-\n  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\nIn this article, tuning will be demonstrated in two ways, using:\n\na standard R formula, and\na recipe.\n\nLet’s create a simple recipe here:\n\niono_rec <-\n  recipe(Class ~ ., data = Ionosphere)  %>%\n  # remove any zero variance predictors\n  step_zv(all_predictors()) %>% \n  # remove any linear combinations\n  step_lincomb(all_numeric())\n\nThe only other required item for tuning is a resampling strategy as defined by an rsample object. Let’s demonstrate using basic bootstrapping:\n\nset.seed(4943)\niono_rs <- bootstraps(Ionosphere, times = 30)"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#optional-inputs",
    "href": "content/learn/work/tune-svm/index.html#optional-inputs",
    "title": "Model tuning via grid search",
    "section": "Optional inputs",
    "text": "Optional inputs\nAn optional step for model tuning is to specify which metrics should be computed using the out-of-sample predictions. For classification, the default is to calculate the log-likelihood statistic and overall accuracy. Instead of the defaults, the area under the ROC curve will be used. To do this, a yardstick package function can be used to create a metric set:\n\nroc_vals <- metric_set(roc_auc)\n\nIf no grid or parameters are provided, a set of 10 hyperparameters are created using a space-filling design (via a Latin hypercube). A grid can be given in a data frame where the parameters are in columns and parameter combinations are in rows. Here, the default will be used.\nAlso, a control object can be passed that specifies different aspects of the search. Here, the verbose option is turned off and the option to save the out-of-sample predictions is turned on.\n\nctrl <- control_grid(verbose = FALSE, save_pred = TRUE)"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#executing-with-a-formula",
    "href": "content/learn/work/tune-svm/index.html#executing-with-a-formula",
    "title": "Model tuning via grid search",
    "section": "Executing with a formula",
    "text": "Executing with a formula\nFirst, we can use the formula interface:\n\nset.seed(35)\nformula_res <-\n  svm_mod %>% \n  tune_grid(\n    Class ~ .,\n    resamples = iono_rs,\n    metrics = roc_vals,\n    control = ctrl\n  )\nformula_res\n#> # Tuning results\n#> # Bootstrap sampling \n#> # A tibble: 30 × 5\n#>    splits            id          .metrics          .notes           .predictions\n#>    <list>            <chr>       <list>            <list>           <list>      \n#>  1 <split [351/120]> Bootstrap01 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  2 <split [351/130]> Bootstrap02 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  3 <split [351/137]> Bootstrap03 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  4 <split [351/141]> Bootstrap04 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  5 <split [351/131]> Bootstrap05 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  6 <split [351/131]> Bootstrap06 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  7 <split [351/127]> Bootstrap07 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  8 <split [351/123]> Bootstrap08 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  9 <split [351/131]> Bootstrap09 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#> 10 <split [351/117]> Bootstrap10 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#> # … with 20 more rows\n\nThe .metrics column contains tibbles of the performance metrics for each tuning parameter combination:\n\nformula_res %>% \n  select(.metrics) %>% \n  slice(1) %>% \n  pull(1)\n#> [[1]]\n#> # A tibble: 10 × 6\n#>        cost rbf_sigma .metric .estimator .estimate .config              \n#>       <dbl>     <dbl> <chr>   <chr>          <dbl> <chr>                \n#>  1  0.00849  1.11e-10 roc_auc binary         0.815 Preprocessor1_Model01\n#>  2  0.176    7.28e- 8 roc_auc binary         0.839 Preprocessor1_Model02\n#>  3 14.9      3.93e- 4 roc_auc binary         0.870 Preprocessor1_Model03\n#>  4  5.51     2.10e- 3 roc_auc binary         0.919 Preprocessor1_Model04\n#>  5  1.87     3.53e- 7 roc_auc binary         0.838 Preprocessor1_Model05\n#>  6  0.00719  1.45e- 5 roc_auc binary         0.832 Preprocessor1_Model06\n#>  7  0.00114  8.41e- 2 roc_auc binary         0.969 Preprocessor1_Model07\n#>  8  0.950    1.74e- 1 roc_auc binary         0.984 Preprocessor1_Model08\n#>  9  0.189    3.13e- 6 roc_auc binary         0.832 Preprocessor1_Model09\n#> 10  0.0364   4.96e- 9 roc_auc binary         0.839 Preprocessor1_Model10\n\nTo get the final resampling estimates, the collect_metrics() function can be used on the grid object:\n\nestimates <- collect_metrics(formula_res)\nestimates\n#> # A tibble: 10 × 8\n#>        cost rbf_sigma .metric .estimator  mean     n std_err .config            \n#>       <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n#>  1  0.00849  1.11e-10 roc_auc binary     0.822    30 0.00718 Preprocessor1_Mode…\n#>  2  0.176    7.28e- 8 roc_auc binary     0.871    30 0.00525 Preprocessor1_Mode…\n#>  3 14.9      3.93e- 4 roc_auc binary     0.916    30 0.00497 Preprocessor1_Mode…\n#>  4  5.51     2.10e- 3 roc_auc binary     0.960    30 0.00378 Preprocessor1_Mode…\n#>  5  1.87     3.53e- 7 roc_auc binary     0.871    30 0.00524 Preprocessor1_Mode…\n#>  6  0.00719  1.45e- 5 roc_auc binary     0.871    30 0.00534 Preprocessor1_Mode…\n#>  7  0.00114  8.41e- 2 roc_auc binary     0.966    30 0.00301 Preprocessor1_Mode…\n#>  8  0.950    1.74e- 1 roc_auc binary     0.979    30 0.00204 Preprocessor1_Mode…\n#>  9  0.189    3.13e- 6 roc_auc binary     0.871    30 0.00536 Preprocessor1_Mode…\n#> 10  0.0364   4.96e- 9 roc_auc binary     0.871    30 0.00537 Preprocessor1_Mode…\n\nThe top combinations are:\n\nshow_best(formula_res, metric = \"roc_auc\")\n#> # A tibble: 5 × 8\n#>       cost rbf_sigma .metric .estimator  mean     n std_err .config             \n#>      <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1  0.950   0.174     roc_auc binary     0.979    30 0.00204 Preprocessor1_Model…\n#> 2  0.00114 0.0841    roc_auc binary     0.966    30 0.00301 Preprocessor1_Model…\n#> 3  5.51    0.00210   roc_auc binary     0.960    30 0.00378 Preprocessor1_Model…\n#> 4 14.9     0.000393  roc_auc binary     0.916    30 0.00497 Preprocessor1_Model…\n#> 5  0.00719 0.0000145 roc_auc binary     0.871    30 0.00534 Preprocessor1_Model…"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#executing-with-a-recipe",
    "href": "content/learn/work/tune-svm/index.html#executing-with-a-recipe",
    "title": "Model tuning via grid search",
    "section": "Executing with a recipe",
    "text": "Executing with a recipe\nNext, we can use the same syntax but pass a recipe in as the pre-processor argument:\n\nset.seed(325)\nrecipe_res <-\n  svm_mod %>% \n  tune_grid(\n    iono_rec,\n    resamples = iono_rs,\n    metrics = roc_vals,\n    control = ctrl\n  )\nrecipe_res\n#> # Tuning results\n#> # Bootstrap sampling \n#> # A tibble: 30 × 5\n#>    splits            id          .metrics          .notes           .predictions\n#>    <list>            <chr>       <list>            <list>           <list>      \n#>  1 <split [351/120]> Bootstrap01 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  2 <split [351/130]> Bootstrap02 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  3 <split [351/137]> Bootstrap03 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  4 <split [351/141]> Bootstrap04 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  5 <split [351/131]> Bootstrap05 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  6 <split [351/131]> Bootstrap06 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  7 <split [351/127]> Bootstrap07 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  8 <split [351/123]> Bootstrap08 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#>  9 <split [351/131]> Bootstrap09 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#> 10 <split [351/117]> Bootstrap10 <tibble [10 × 6]> <tibble [0 × 3]> <tibble>    \n#> # … with 20 more rows\n\nThe best setting here is:\n\nshow_best(recipe_res, metric = \"roc_auc\")\n#> # A tibble: 5 × 8\n#>      cost rbf_sigma .metric .estimator  mean     n std_err .config              \n#>     <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 15.6    0.182     roc_auc binary     0.981    30 0.00213 Preprocessor1_Model04\n#> 2  0.385  0.0276    roc_auc binary     0.978    30 0.00222 Preprocessor1_Model03\n#> 3  0.143  0.00243   roc_auc binary     0.930    30 0.00443 Preprocessor1_Model06\n#> 4  0.841  0.000691  roc_auc binary     0.892    30 0.00504 Preprocessor1_Model07\n#> 5  0.0499 0.0000335 roc_auc binary     0.872    30 0.00521 Preprocessor1_Model08"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#out-of-sample-predictions",
    "href": "content/learn/work/tune-svm/index.html#out-of-sample-predictions",
    "title": "Model tuning via grid search",
    "section": "Out-of-sample predictions",
    "text": "Out-of-sample predictions\nIf we used save_pred = TRUE to keep the out-of-sample predictions for each resample during tuning, we can obtain those predictions, along with the tuning parameters and resample identifier, using collect_predictions():\n\ncollect_predictions(recipe_res)\n#> # A tibble: 38,740 × 8\n#>    id          .pred_bad .pred_good  .row    cost  rbf_sigma Class .config      \n#>    <chr>           <dbl>      <dbl> <int>   <dbl>      <dbl> <fct> <chr>        \n#>  1 Bootstrap01     0.333      0.667     1 0.00296 0.00000383 good  Preprocessor…\n#>  2 Bootstrap01     0.333      0.667     9 0.00296 0.00000383 good  Preprocessor…\n#>  3 Bootstrap01     0.333      0.667    10 0.00296 0.00000383 bad   Preprocessor…\n#>  4 Bootstrap01     0.333      0.667    12 0.00296 0.00000383 bad   Preprocessor…\n#>  5 Bootstrap01     0.333      0.667    14 0.00296 0.00000383 bad   Preprocessor…\n#>  6 Bootstrap01     0.333      0.667    15 0.00296 0.00000383 good  Preprocessor…\n#>  7 Bootstrap01     0.333      0.667    16 0.00296 0.00000383 bad   Preprocessor…\n#>  8 Bootstrap01     0.334      0.666    22 0.00296 0.00000383 bad   Preprocessor…\n#>  9 Bootstrap01     0.333      0.667    23 0.00296 0.00000383 good  Preprocessor…\n#> 10 Bootstrap01     0.334      0.666    24 0.00296 0.00000383 bad   Preprocessor…\n#> # … with 38,730 more rows\n\nWe can obtain the hold-out sets for all the resamples augmented with the predictions using augment(), which provides opportunities for flexible visualization of model results:\n\naugment(recipe_res) %>%\n  ggplot(aes(V3, .pred_good, color = Class)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~Class)"
  },
  {
    "objectID": "content/learn/work/tune-svm/index.html#session-information",
    "href": "content/learn/work/tune-svm/index.html#session-information",
    "title": "Model tuning via grid search",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  kernlab    * 0.9-31     2022-06-09 [1] CRAN (R 4.2.0)\n#>  mlbench    * 2.1-3      2021-01-29 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/work/nested-resampling/index.html",
    "href": "content/learn/work/nested-resampling/index.html",
    "title": "Nested resampling",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: furrr, kernlab, mlbench, scales, and tidymodels.\nIn this article, we discuss an alternative method for evaluating and tuning models, called nested resampling. While it is more computationally taxing and challenging to implement than other resampling methods, it has the potential to produce better estimates of model performance."
  },
  {
    "objectID": "content/learn/work/nested-resampling/index.html#resampling-models",
    "href": "content/learn/work/nested-resampling/index.html#resampling-models",
    "title": "Nested resampling",
    "section": "Resampling models",
    "text": "Resampling models\nA typical scheme for splitting the data when developing a predictive model is to create an initial split of the data into a training and test set. If resampling is used, it is executed on the training set. A series of binary splits is created. In rsample, we use the term analysis set for the data that are used to fit the model and the term assessment set for the set used to compute performance:\n\n\n\n\n\n\n\n\n\nA common method for tuning models is grid search where a candidate set of tuning parameters is created. The full set of models for every combination of the tuning parameter grid and the resamples is fitted. Each time, the assessment data are used to measure performance and the average value is determined for each tuning parameter.\nThe potential problem is that once we pick the tuning parameter associated with the best performance, this performance value is usually quoted as the performance of the model. There is serious potential for optimization bias since we use the same data to tune the model and to assess performance. This would result in an optimistic estimate of performance.\nNested resampling uses an additional layer of resampling that separates the tuning activities from the process used to estimate the efficacy of the model. An outer resampling scheme is used and, for every split in the outer resample, another full set of resampling splits are created on the original analysis set. For example, if 10-fold cross-validation is used on the outside and 5-fold cross-validation on the inside, a total of 500 models will be fit. The parameter tuning will be conducted 10 times and the best parameters are determined from the average of the 5 assessment sets. This process occurs 10 times.\nOnce the tuning results are complete, a model is fit to each of the outer resampling splits using the best parameter associated with that resample. The average of the outer method’s assessment sets are a unbiased estimate of the model.\nWe will simulate some regression data to illustrate the methods. The mlbench package has a function mlbench::mlbench.friedman1() that can simulate a complex regression data structure from the original MARS publication. A training set size of 100 data points are generated as well as a large set that will be used to characterize how well the resampling procedure performed.\n\nlibrary(mlbench)\nsim_data <- function(n) {\n  tmp <- mlbench.friedman1(n, sd = 1)\n  tmp <- cbind(tmp$x, tmp$y)\n  tmp <- as.data.frame(tmp)\n  names(tmp)[ncol(tmp)] <- \"y\"\n  tmp\n}\n\nset.seed(9815)\ntrain_dat <- sim_data(100)\nlarge_dat <- sim_data(10^5)"
  },
  {
    "objectID": "content/learn/work/nested-resampling/index.html#nested-resampling",
    "href": "content/learn/work/nested-resampling/index.html#nested-resampling",
    "title": "Nested resampling",
    "section": "Nested resampling",
    "text": "Nested resampling\nTo get started, the types of resampling methods need to be specified. This isn’t a large data set, so 5 repeats of 10-fold cross validation will be used as the outer resampling method for generating the estimate of overall performance. To tune the model, it would be good to have precise estimates for each of the values of the tuning parameter so let’s use 25 iterations of the bootstrap. This means that there will eventually be 5 * 10 * 25 = 1250 models that are fit to the data per tuning parameter. These models will be discarded once the performance of the model has been quantified.\nTo create the tibble with the resampling specifications:\n\nlibrary(tidymodels)\nresults <- nested_cv(train_dat, \n                     outside = vfold_cv(repeats = 5), \n                     inside = bootstraps(times = 25))\nresults\n#> # Nested resampling:\n#> #  outer: 10-fold cross-validation repeated 5 times\n#> #  inner: Bootstrap sampling\n#> # A tibble: 50 × 4\n#>    splits          id      id2    inner_resamples\n#>    <list>          <chr>   <chr>  <list>         \n#>  1 <split [90/10]> Repeat1 Fold01 <boot [25 × 2]>\n#>  2 <split [90/10]> Repeat1 Fold02 <boot [25 × 2]>\n#>  3 <split [90/10]> Repeat1 Fold03 <boot [25 × 2]>\n#>  4 <split [90/10]> Repeat1 Fold04 <boot [25 × 2]>\n#>  5 <split [90/10]> Repeat1 Fold05 <boot [25 × 2]>\n#>  6 <split [90/10]> Repeat1 Fold06 <boot [25 × 2]>\n#>  7 <split [90/10]> Repeat1 Fold07 <boot [25 × 2]>\n#>  8 <split [90/10]> Repeat1 Fold08 <boot [25 × 2]>\n#>  9 <split [90/10]> Repeat1 Fold09 <boot [25 × 2]>\n#> 10 <split [90/10]> Repeat1 Fold10 <boot [25 × 2]>\n#> # … with 40 more rows\n\nThe splitting information for each resample is contained in the split objects. Focusing on the second fold of the first repeat:\n\nresults$splits[[2]]\n#> <Analysis/Assess/Total>\n#> <90/10/100>\n\n<90/10/100> indicates the number of observations in the analysis set, assessment set, and the original data.\nEach element of inner_resamples has its own tibble with the bootstrapping splits.\n\nresults$inner_resamples[[5]]\n#> # Bootstrap sampling \n#> # A tibble: 25 × 2\n#>    splits          id         \n#>    <list>          <chr>      \n#>  1 <split [90/31]> Bootstrap01\n#>  2 <split [90/33]> Bootstrap02\n#>  3 <split [90/37]> Bootstrap03\n#>  4 <split [90/31]> Bootstrap04\n#>  5 <split [90/32]> Bootstrap05\n#>  6 <split [90/32]> Bootstrap06\n#>  7 <split [90/36]> Bootstrap07\n#>  8 <split [90/34]> Bootstrap08\n#>  9 <split [90/29]> Bootstrap09\n#> 10 <split [90/31]> Bootstrap10\n#> # … with 15 more rows\n\nThese are self-contained, meaning that the bootstrap sample is aware that it is a sample of a specific 90% of the data:\n\nresults$inner_resamples[[5]]$splits[[1]]\n#> <Analysis/Assess/Total>\n#> <90/31/90>\n\nTo start, we need to define how the model will be created and measured. Let’s use a radial basis support vector machine model via the function kernlab::ksvm. This model is generally considered to have two tuning parameters: the SVM cost value and the kernel parameter sigma. For illustration purposes here, only the cost value will be tuned and the function kernlab::sigest will be used to estimate sigma during each model fit. This is automatically done by ksvm.\nAfter the model is fit to the analysis set, the root-mean squared error (RMSE) is computed on the assessment set. One important note: for this model, it is critical to center and scale the predictors before computing dot products. We don’t do this operation here because mlbench.friedman1 simulates all of the predictors to be standardized uniform random variables.\nOur function to fit the model and compute the RMSE is:\n\nlibrary(kernlab)\n\n# `object` will be an `rsplit` object from our `results` tibble\n# `cost` is the tuning parameter\nsvm_rmse <- function(object, cost = 1) {\n  y_col <- ncol(object$data)\n  mod <- \n    svm_rbf(mode = \"regression\", cost = cost) %>% \n    set_engine(\"kernlab\") %>% \n    fit(y ~ ., data = analysis(object))\n  \n  holdout_pred <- \n    predict(mod, assessment(object) %>% dplyr::select(-y)) %>% \n    bind_cols(assessment(object) %>% dplyr::select(y))\n  rmse(holdout_pred, truth = y, estimate = .pred)$.estimate\n}\n\n# In some case, we want to parameterize the function over the tuning parameter:\nrmse_wrapper <- function(cost, object) svm_rmse(object, cost)\n\nFor the nested resampling, a model needs to be fit for each tuning parameter and each bootstrap split. To do this, create a wrapper:\n\n# `object` will be an `rsplit` object for the bootstrap samples\ntune_over_cost <- function(object) {\n  tibble(cost = 2 ^ seq(-2, 8, by = 1)) %>% \n    mutate(RMSE = map_dbl(cost, rmse_wrapper, object = object))\n}\n\nSince this will be called across the set of outer cross-validation splits, another wrapper is required:\n\n# `object` is an `rsplit` object in `results$inner_resamples` \nsummarize_tune_results <- function(object) {\n  # Return row-bound tibble that has the 25 bootstrap results\n  map_df(object$splits, tune_over_cost) %>%\n    # For each value of the tuning parameter, compute the \n    # average RMSE which is the inner bootstrap estimate. \n    group_by(cost) %>%\n    summarize(mean_RMSE = mean(RMSE, na.rm = TRUE),\n              n = length(RMSE),\n              .groups = \"drop\")\n}\n\nNow that those functions are defined, we can execute all the inner resampling loops:\n\ntuning_results <- map(results$inner_resamples, summarize_tune_results) \n\nAlternatively, since these computations can be run in parallel, we can use the furrr package. Instead of using map(), the function future_map() parallelizes the iterations using the future package. The multisession plan uses the local cores to process the inner resampling loop. The end results are the same as the sequential computations.\n\nlibrary(furrr)\nplan(multisession)\n\ntuning_results <- future_map(results$inner_resamples, summarize_tune_results) \n\nThe object tuning_results is a list of data frames for each of the 50 outer resamples.\nLet’s make a plot of the averaged results to see what the relationship is between the RMSE and the tuning parameters for each of the inner bootstrapping operations:\n\nlibrary(scales)\n\npooled_inner <- tuning_results %>% bind_rows\n\nbest_cost <- function(dat) dat[which.min(dat$mean_RMSE),]\n\np <- \n  ggplot(pooled_inner, aes(x = cost, y = mean_RMSE)) + \n  scale_x_continuous(trans = 'log2') +\n  xlab(\"SVM Cost\") + ylab(\"Inner RMSE\")\n\nfor (i in 1:length(tuning_results))\n  p <- p  +\n  geom_line(data = tuning_results[[i]], alpha = .2) +\n  geom_point(data = best_cost(tuning_results[[i]]), pch = 16, alpha = 3/4)\n\np <- p + geom_smooth(data = pooled_inner, se = FALSE)\np\n\n\n\n\n\n\n\n\nEach gray line is a separate bootstrap resampling curve created from a different 90% of the data. The blue line is a LOESS smooth of all the results pooled together.\nTo determine the best parameter estimate for each of the outer resampling iterations:\n\ncost_vals <- \n  tuning_results %>% \n  map_df(best_cost) %>% \n  select(cost)\n\nresults <- \n  bind_cols(results, cost_vals) %>% \n  mutate(cost = factor(cost, levels = paste(2 ^ seq(-2, 8, by = 1))))\n\nggplot(results, aes(x = cost)) + \n  geom_bar() + \n  xlab(\"SVM Cost\") + \n  scale_x_discrete(drop = FALSE)\n\n\n\n\n\n\n\n\nMost of the resamples produced an optimal cost value of 2.0, but the distribution is right-skewed due to the flat trend in the resampling profile once the cost value becomes 10 or larger.\nNow that we have these estimates, we can compute the outer resampling results for each of the 50 splits using the corresponding tuning parameter value:\n\nresults <- \n  results %>% \n  mutate(RMSE = map2_dbl(splits, cost, svm_rmse))\n\nsummary(results$RMSE)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.673   2.095   2.651   2.689   3.258   4.297\n\nThe estimated RMSE for the model tuning process is 2.69.\nWhat is the RMSE estimate for the non-nested procedure when only the outer resampling method is used? For each cost value in the tuning grid, 50 SVM models are fit and their RMSE values are averaged. The table of cost values and mean RMSE estimates is used to determine the best cost value. The associated RMSE is the biased estimate.\n\nnot_nested <- \n  map(results$splits, tune_over_cost) %>%\n  bind_rows\n\nouter_summary <- not_nested %>% \n  group_by(cost) %>% \n  summarize(outer_RMSE = mean(RMSE), n = length(RMSE))\n\nouter_summary\n#> # A tibble: 11 × 3\n#>      cost outer_RMSE     n\n#>     <dbl>      <dbl> <int>\n#>  1   0.25       3.54    50\n#>  2   0.5        3.11    50\n#>  3   1          2.77    50\n#>  4   2          2.62    50\n#>  5   4          2.65    50\n#>  6   8          2.75    50\n#>  7  16          2.82    50\n#>  8  32          2.82    50\n#>  9  64          2.83    50\n#> 10 128          2.83    50\n#> 11 256          2.82    50\n\nggplot(outer_summary, aes(x = cost, y = outer_RMSE)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(trans = 'log2') +\n  xlab(\"SVM Cost\") + ylab(\"RMSE\")\n\n\n\n\n\n\n\n\nThe non-nested procedure estimates the RMSE to be 2.62. Both estimates are fairly close.\nThe approximately true RMSE for an SVM model with a cost value of 2.0 can be approximated with the large sample that was simulated at the beginning.\n\nfinalModel <- ksvm(y ~ ., data = train_dat, C = 2)\nlarge_pred <- predict(finalModel, large_dat[, -ncol(large_dat)])\nsqrt(mean((large_dat$y - large_pred) ^ 2, na.rm = TRUE))\n#> [1] 2.712059\n\nThe nested procedure produces a closer estimate to the approximate truth but the non-nested estimate is very similar."
  },
  {
    "objectID": "content/learn/work/nested-resampling/index.html#session-information",
    "href": "content/learn/work/nested-resampling/index.html#session-information",
    "title": "Nested resampling",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  furrr      * 0.3.1      2022-08-15 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  kernlab    * 0.9-31     2022-06-09 [1] CRAN (R 4.2.0)\n#>  mlbench    * 2.1-3      2021-01-29 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  scales     * 1.2.1      2022-08-20 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/work/tune-text/index.html",
    "href": "content/learn/work/tune-text/index.html",
    "title": "Tuning text models",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: stopwords, textfeatures, textrecipes, and tidymodels.\nThis article demonstrates an advanced example for training and tuning models for text data. Text data must be processed and transformed to a numeric representation to be ready for computation in modeling; in tidymodels, we use a recipe for this preprocessing. This article also shows how to extract information from each model fit during tuning to use later on."
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#text-as-data",
    "href": "content/learn/work/tune-text/index.html#text-as-data",
    "title": "Tuning text models",
    "section": "Text as data",
    "text": "Text as data\nThe text data we’ll use in this article are from Amazon:\n\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review.\n\nThis article uses a small subset of the total reviews available at the original source. We sampled a single review from 5,000 random products and allocated 80% of these data to the training set, with the remaining 1,000 reviews held out for the test set.\nThere is a column for the product, a column for the text of the review, and a factor column for the outcome variable. The outcome is whether the reviewer gave the product a five-star rating or not.\n\nlibrary(tidymodels)\n\ndata(\"small_fine_foods\")\ntraining_data\n#> # A tibble: 4,000 × 3\n#>    product    review                                                       score\n#>    <chr>      <chr>                                                        <fct>\n#>  1 B000J0LSBG \"this stuff is  not stuffing  its  not good at all  save yo… other\n#>  2 B000EYLDYE \"I absolutely LOVE this dried fruit.  LOVE IT.  Whenever I … great\n#>  3 B0026LIO9A \"GREAT DEAL, CONVENIENT TOO.  Much cheaper than WalMart and… great\n#>  4 B00473P8SK \"Great flavor, we go through a ton of this sauce! I discove… great\n#>  5 B001SAWTNM \"This is excellent salsa/hot sauce, but you can get it for … great\n#>  6 B000FAG90U \"Again, this is the best dogfood out there.  One suggestion… great\n#>  7 B006BXTCEK \"The box I received was filled with teas, hot chocolates, a… other\n#>  8 B002GWH5OY \"This is delicious coffee which compares favorably with muc… great\n#>  9 B003R0MFYY \"Don't let these little tiny cans fool you.  They pack a lo… great\n#> 10 B001EO5ZXI \"One of the nicest, smoothest cup of chai I've made. Nice m… great\n#> # … with 3,990 more rows\n\nOur modeling goal is to create modeling features from the text of the reviews to predict whether the review was five-star or not."
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#inputs-for-the-search",
    "href": "content/learn/work/tune-text/index.html#inputs-for-the-search",
    "title": "Tuning text models",
    "section": "Inputs for the search",
    "text": "Inputs for the search\nText, perhaps more so than tabular data we often deal with, must be heavily processed to be used as predictor data for modeling. There are multiple ways to process and prepare text for modeling; let’s add several steps together to create different kinds of features:\n\nCreate an initial set of count-based features, such as the number of words, spaces, lower- or uppercase characters, URLs, and so on; we can use the textfeatures package for this.\nTokenize the text (i.e. break the text into smaller components such as words).\nRemove stop words such as “the”, “an”, “of”, etc.\nStem tokens to a common root where possible.\nConvert tokens to dummy variables via a signed, binary hash function.\nOptionally transform non-token features (the count-based features like number of lowercase characters) to a more symmetric state using a Yeo-Johnson transformation.\nRemove predictors with a single distinct value.\nCenter and scale all predictors.\n\n\n\n\n\n\n\nNote\n\n\n\nWe will end up with two kinds of features:\n\ndummy/indicator variables for the count-based features like number of digits or punctuation characters\nhash features for the tokens like “salsa” or “delicious”.\n\n\n\nSome of these preprocessing steps (such as stemming) may or may not be good ideas but a full discussion of their effects is beyond the scope of this article. In this preprocessing approach, the main tuning parameter is the number of hashing features to use.\nBefore we start building our preprocessing recipe, we need some helper objects. For example, for the Yeo-Johnson transformation, we need to know the set of count-based text features:\n\nlibrary(textfeatures)\n\nbasics <- names(textfeatures:::count_functions)\nhead(basics)\n#> [1] \"n_words\"    \"n_uq_words\" \"n_charS\"    \"n_uq_charS\" \"n_digits\"  \n#> [6] \"n_hashtags\"\n\nAlso, the implementation of feature hashes does not produce the binary values we need. This small function will help convert the scores to values of -1, 0, or 1:\n\nbinary_hash <- function(x) {\n  x <- ifelse(x < 0, -1, x)\n  x <- ifelse(x > 0,  1, x)\n  x\n}\n\nNow, let’s put this all together in one recipe:\n\nlibrary(textrecipes)\n\npre_proc <-\n  recipe(score ~ product + review, data = training_data) %>%\n  # Do not use the product ID as a predictor\n  update_role(product, new_role = \"id\") %>%\n  # Make a copy of the raw text\n  step_mutate(review_raw = review) %>%\n  # Compute the initial features. This removes the `review_raw` column\n  step_textfeature(review_raw) %>%\n  # Make the feature names shorter\n  step_rename_at(\n    starts_with(\"textfeature_\"),\n    fn = ~ gsub(\"textfeature_review_raw_\", \"\", .)\n  ) %>%\n  step_tokenize(review)  %>%\n  step_stopwords(review) %>%\n  step_stem(review) %>%\n  # Here is where the tuning parameter is declared\n  step_texthash(review, signed = TRUE, num_terms = tune()) %>%\n  # Simplify these names\n  step_rename_at(starts_with(\"review_hash\"), fn = ~ gsub(\"review_\", \"\", .)) %>%\n  # Convert the features from counts to values of -1, 0, or 1\n  step_mutate_at(starts_with(\"hash\"), fn = binary_hash) %>%\n  # Transform the initial feature set\n  step_YeoJohnson(one_of(!!basics)) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that, when objects from the global environment are used, they are injected into the step objects via !!. For some parallel processing technologies, these objects may not be found by the worker processes.\n\n\nThe preprocessing recipe is long and complex (often typical for working with text data) but the model we’ll use is more straightforward. Let’s stick with a regularized logistic regression model:\n\nlr_mod <-\n  logistic_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine(\"glmnet\")\n\nThere are three tuning parameters for this data analysis:\n\nnum_terms, the number of feature hash variables to create\npenalty, the amount of regularization for the model\nmixture, the proportion of L1 regularization"
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#resampling",
    "href": "content/learn/work/tune-text/index.html#resampling",
    "title": "Tuning text models",
    "section": "Resampling",
    "text": "Resampling\nThere are enough data here so that 10-fold resampling would hold out 400 reviews at a time to estimate performance. Performance estimates using this many observations have sufficiently low noise to measure and tune models.\n\nset.seed(8935)\nfolds <- vfold_cv(training_data)\nfolds\n#> #  10-fold cross-validation \n#> # A tibble: 10 × 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [3600/400]> Fold01\n#>  2 <split [3600/400]> Fold02\n#>  3 <split [3600/400]> Fold03\n#>  4 <split [3600/400]> Fold04\n#>  5 <split [3600/400]> Fold05\n#>  6 <split [3600/400]> Fold06\n#>  7 <split [3600/400]> Fold07\n#>  8 <split [3600/400]> Fold08\n#>  9 <split [3600/400]> Fold09\n#> 10 <split [3600/400]> Fold10"
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#grid-search",
    "href": "content/learn/work/tune-text/index.html#grid-search",
    "title": "Tuning text models",
    "section": "Grid search",
    "text": "Grid search\nLet’s begin our tuning with grid search and a regular grid. For glmnet models, evaluating penalty values is fairly cheap because of the use of the “submodel-trick”. The grid will use 20 penalty values, 5 mixture values, and 3 values for the number of hash features.\n\nfive_star_grid <- \n  crossing(\n    penalty = 10^seq(-3, 0, length = 20),\n    mixture = c(0.01, 0.25, 0.50, 0.75, 1),\n    num_terms = 2^c(8, 10, 12)\n  )\nfive_star_grid\n#> # A tibble: 300 × 3\n#>    penalty mixture num_terms\n#>      <dbl>   <dbl>     <dbl>\n#>  1   0.001    0.01       256\n#>  2   0.001    0.01      1024\n#>  3   0.001    0.01      4096\n#>  4   0.001    0.25       256\n#>  5   0.001    0.25      1024\n#>  6   0.001    0.25      4096\n#>  7   0.001    0.5        256\n#>  8   0.001    0.5       1024\n#>  9   0.001    0.5       4096\n#> 10   0.001    0.75       256\n#> # … with 290 more rows\n\nNote that, for each resample, the (computationally expensive) text preprocessing recipe is only prepped 6 times. This increases the efficiency of the analysis by avoiding redundant work.\nLet’s save information on the number of predictors by penalty value for each glmnet model. This can help us understand how many features were used across the penalty values. Use an extraction function to do this:\n\nglmnet_vars <- function(x) {\n  # `x` will be a workflow object\n  mod <- extract_model(x)\n  # `df` is the number of model terms for each penalty value\n  tibble(penalty = mod$lambda, num_vars = mod$df)\n}\n\nctrl <- control_grid(extract = glmnet_vars, verbose = TRUE)\n\nFinally, let’s run the grid search:\n\nroc_scores <- metric_set(roc_auc)\n\nset.seed(1559)\nfive_star_glmnet <- \n  tune_grid(\n    lr_mod, \n    pre_proc, \n    resamples = folds, \n    grid = five_star_grid, \n    metrics = roc_scores, \n    control = ctrl\n  )\n\nfive_star_glmnet\n#> # Tuning results\n#> # 10-fold cross-validation \n#> # A tibble: 10 × 5\n#>    splits             id     .metrics           .notes           .extracts\n#>    <list>             <chr>  <list>             <list>           <list>   \n#>  1 <split [3600/400]> Fold01 <tibble [300 × 7]> <tibble [1 × 3]> <tibble> \n#>  2 <split [3600/400]> Fold02 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  3 <split [3600/400]> Fold03 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  4 <split [3600/400]> Fold04 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  5 <split [3600/400]> Fold05 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  6 <split [3600/400]> Fold06 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  7 <split [3600/400]> Fold07 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  8 <split [3600/400]> Fold08 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#>  9 <split [3600/400]> Fold09 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#> 10 <split [3600/400]> Fold10 <tibble [300 × 7]> <tibble [0 × 3]> <tibble> \n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x1: `extract_model()` was deprecated in tune 0.1.6. ℹ Please use `ext...\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n\nThis took a while to complete! What do the results look like? Let’s get the resampling estimates of the area under the ROC curve for each tuning parameter:\n\ngrid_roc <- \n  collect_metrics(five_star_glmnet) %>% \n  arrange(desc(mean))\ngrid_roc\n#> # A tibble: 300 × 9\n#>    penalty mixture num_terms .metric .estimator  mean     n std_err .config     \n#>      <dbl>   <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n#>  1 0.695      0.01      4096 roc_auc binary     0.811    10 0.00799 Preprocesso…\n#>  2 0.483      0.01      4096 roc_auc binary     0.811    10 0.00797 Preprocesso…\n#>  3 0.0379     0.25      4096 roc_auc binary     0.809    10 0.00755 Preprocesso…\n#>  4 0.0183     0.5       4096 roc_auc binary     0.807    10 0.00776 Preprocesso…\n#>  5 0.0264     0.25      4096 roc_auc binary     0.807    10 0.00792 Preprocesso…\n#>  6 0.0127     0.75      4096 roc_auc binary     0.807    10 0.00773 Preprocesso…\n#>  7 0.336      0.01      4096 roc_auc binary     0.806    10 0.00781 Preprocesso…\n#>  8 0.00886    1         4096 roc_auc binary     0.806    10 0.00783 Preprocesso…\n#>  9 1          0.01      4096 roc_auc binary     0.806    10 0.00801 Preprocesso…\n#> 10 0.0546     0.25      4096 roc_auc binary     0.805    10 0.00783 Preprocesso…\n#> # … with 290 more rows\n\nThe best results have a fairly high penalty value and focus on the ridge penalty (i.e. no feature selection via the lasso’s L1 penalty). The best solutions also use the largest number of hashing features.\nWhat is the relationship between performance and the tuning parameters?\n\nautoplot(five_star_glmnet, metric = \"roc_auc\")\n\n\n\n\n\n\n\n\n\nWe can definitely see that performance improves with the number of features included. In this article, we’ve used a small sample of the overall data set available. When more data are used, an even larger feature set is optimal.\nThe profiles with larger mixture values (greater than 0.01) have steep drop-offs in performance. What’s that about? Those are cases where the lasso penalty is removing too many (and perhaps all) features from the model.\n\nThe panel with at least 4096 features shows that there are several parameter combinations that have about the same performance; there isn’t much difference between the best performance for the different mixture values. A case could be made that we should choose a larger mixture value and a smaller penalty to select a simpler model that contains fewer predictors.\nIf more experimentation were conducted, a larger set of features (more than 4096) should also be considered.\n\nWe’ll come back to the extracted glmnet components at the end of this article."
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#directed-search",
    "href": "content/learn/work/tune-text/index.html#directed-search",
    "title": "Tuning text models",
    "section": "Directed search",
    "text": "Directed search\nWhat if we had started with Bayesian optimization? Would a good set of conditions have been found more efficiently?\nLet’s pretend that we haven’t seen the grid search results. We’ll initialize the Gaussian process model with five tuning parameter combinations chosen with a space-filling design.\nIt might be good to use a custom dials object for the number of hash terms. The default object, num_terms(), uses a linear range and tries to set the upper bound of the parameter using the data. Instead, let’s create a parameter set, change the scale to be log2, and define the same range as was used in grid search.\n\nhash_range <- num_terms(c(8, 12), trans = log2_trans())\nhash_range\n#> # Model Terms (quantitative)\n#> Transformer: log-2 [1e-100, Inf]\n#> Range (transformed scale): [8, 12]\n\nTo use this, we have to merge the recipe and parsnip model object into a workflow:\n\nfive_star_wflow <-\n  workflow() %>%\n  add_recipe(pre_proc) %>%\n  add_model(lr_mod)\n\nThen we can extract and manipulate the corresponding parameter set:\n\nfive_star_set <-\n  five_star_wflow %>%\n  parameters() %>%\n  update(\n    num_terms = hash_range, \n    penalty = penalty(c(-3, 0)),\n    mixture = mixture(c(0.05, 1.00))\n  )\n#> Warning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\n#> ℹ Please use `hardhat::extract_parameter_set_dials()` instead.\n\nThis is passed to the search function via the param_info argument.\nThe initial rounds of search can be biased more towards exploration of the parameter space (as opposed to staying near the current best results). If expected improvement is used as the acquisition function, the trade-off value can be slowly moved from exploration to exploitation over iterations (see the tune vignette on acquisition functions for more details). The tune package has a built-in function called expo_decay() that can help accomplish this:\n\ntrade_off_decay <- function(iter) {\n  expo_decay(iter, start_val = .01, limit_val = 0, slope = 1/4)\n}\n\nUsing these values, let’s run the search:\n\nset.seed(12)\nfive_star_search <-\n  tune_bayes(\n    five_star_wflow, \n    resamples = folds,\n    param_info = five_star_set,\n    initial = 5,\n    iter = 30,\n    metrics = roc_scores,\n    objective = exp_improve(trade_off_decay),\n    control = control_bayes(verbose_iter = TRUE)\n  )\n#> Optimizing roc_auc using the expected improvement with variable trade-off\n#> values.\n#> ! No improvement for 10 iterations; returning current results.\n\nfive_star_search\n#> # Tuning results\n#> # 10-fold cross-validation \n#> # A tibble: 290 × 5\n#>    splits             id     .metrics         .notes           .iter\n#>    <list>             <chr>  <list>           <list>           <int>\n#>  1 <split [3600/400]> Fold01 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  2 <split [3600/400]> Fold02 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  3 <split [3600/400]> Fold03 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  4 <split [3600/400]> Fold04 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  5 <split [3600/400]> Fold05 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  6 <split [3600/400]> Fold06 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  7 <split [3600/400]> Fold07 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  8 <split [3600/400]> Fold08 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#>  9 <split [3600/400]> Fold09 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#> 10 <split [3600/400]> Fold10 <tibble [5 × 7]> <tibble [0 × 3]>     0\n#> # … with 280 more rows\n\nThese results show some improvement over the initial set. One issue is that so many settings are sub-optimal (as shown in the plot above for grid search) so there are poor results periodically. There are regions where the penalty parameter becomes too large and all of the predictors are removed from the model. These regions are also dependent on the number of terms. There is a fairly narrow ridge (sorry, pun intended!) where good performance can be achieved. Using more iterations would probably result in the search finding better results. Let’s look at a plot of model performance versus the search iterations:\n\nautoplot(five_star_search, type = \"performance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat would we do if we knew about the grid search results and wanted to try directed, iterative search? We would restrict the range for the number of hash features to be larger (especially with more data). We might also restrict the penalty and mixture parameters to have a lower upper bound."
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#extracted-results",
    "href": "content/learn/work/tune-text/index.html#extracted-results",
    "title": "Tuning text models",
    "section": "Extracted results",
    "text": "Extracted results\nLet’s return to the grid search results and examine the results of our extract function. For each fitted model, a tibble was saved that contains the relationship between the number of predictors and the penalty value. Let’s look at these results for the best model:\n\nparams <- select_best(five_star_glmnet, metric = \"roc_auc\")\nparams\n#> # A tibble: 1 × 4\n#>   penalty mixture num_terms .config               \n#>     <dbl>   <dbl>     <dbl> <chr>                 \n#> 1   0.695    0.01      4096 Preprocessor3_Model019\n\nRecall that we saved the glmnet results in a tibble. The column five_star_glmnet$.extracts is a list of tibbles. As an example, the first element of the list is:\n\nfive_star_glmnet$.extracts[[1]]\n#> # A tibble: 300 × 5\n#>    num_terms penalty mixture .extracts          .config               \n#>        <dbl>   <dbl>   <dbl> <list>             <chr>                 \n#>  1       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model001\n#>  2       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model002\n#>  3       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model003\n#>  4       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model004\n#>  5       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model005\n#>  6       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model006\n#>  7       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model007\n#>  8       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model008\n#>  9       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model009\n#> 10       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model010\n#> # … with 290 more rows\n\nMore nested tibbles! Let’s unnest() the five_star_glmnet$.extracts column:\n\nlibrary(tidyr)\nextracted <- \n  five_star_glmnet %>% \n  dplyr::select(id, .extracts) %>% \n  unnest(cols = .extracts)\nextracted\n#> # A tibble: 3,000 × 6\n#>    id     num_terms penalty mixture .extracts          .config               \n#>    <chr>      <dbl>   <dbl>   <dbl> <list>             <chr>                 \n#>  1 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model001\n#>  2 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model002\n#>  3 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model003\n#>  4 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model004\n#>  5 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model005\n#>  6 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model006\n#>  7 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model007\n#>  8 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model008\n#>  9 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model009\n#> 10 Fold01       256       1    0.01 <tibble [100 × 2]> Preprocessor1_Model010\n#> # … with 2,990 more rows\n\nOne thing to realize here is that tune_grid() may not fit all of the models that are evaluated. In this case, for each value of mixture and num_terms, the model is fit over all penalty values (this is a feature of this particular model and is not generally true for other engines). To select the best parameter set, we can exclude the penalty column in extracted:\n\nextracted <- \n  extracted %>% \n  dplyr::select(-penalty) %>% \n  inner_join(params, by = c(\"num_terms\", \"mixture\")) %>% \n  # Now remove it from the final results\n  dplyr::select(-penalty)\nextracted\n#> # A tibble: 200 × 6\n#>    id     num_terms mixture .extracts          .config.x              .config.y \n#>    <chr>      <dbl>   <dbl> <list>             <chr>                  <chr>     \n#>  1 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model001 Preproces…\n#>  2 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model002 Preproces…\n#>  3 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model003 Preproces…\n#>  4 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model004 Preproces…\n#>  5 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model005 Preproces…\n#>  6 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model006 Preproces…\n#>  7 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model007 Preproces…\n#>  8 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model008 Preproces…\n#>  9 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model009 Preproces…\n#> 10 Fold01      4096    0.01 <tibble [100 × 2]> Preprocessor3_Model010 Preproces…\n#> # … with 190 more rows\n\nNow we can get at the results that we want using another unnest():\n\nextracted <- \n  extracted %>% \n  unnest(col = .extracts) # <- these contain a `penalty` column\nextracted\n#> # A tibble: 20,000 × 7\n#>    id     num_terms mixture penalty num_vars .config.x              .config.y   \n#>    <chr>      <dbl>   <dbl>   <dbl>    <int> <chr>                  <chr>       \n#>  1 Fold01      4096    0.01    8.60        0 Preprocessor3_Model001 Preprocesso…\n#>  2 Fold01      4096    0.01    8.21        2 Preprocessor3_Model001 Preprocesso…\n#>  3 Fold01      4096    0.01    7.84        2 Preprocessor3_Model001 Preprocesso…\n#>  4 Fold01      4096    0.01    7.48        3 Preprocessor3_Model001 Preprocesso…\n#>  5 Fold01      4096    0.01    7.14        3 Preprocessor3_Model001 Preprocesso…\n#>  6 Fold01      4096    0.01    6.82        3 Preprocessor3_Model001 Preprocesso…\n#>  7 Fold01      4096    0.01    6.51        4 Preprocessor3_Model001 Preprocesso…\n#>  8 Fold01      4096    0.01    6.21        6 Preprocessor3_Model001 Preprocesso…\n#>  9 Fold01      4096    0.01    5.93        7 Preprocessor3_Model001 Preprocesso…\n#> 10 Fold01      4096    0.01    5.66        7 Preprocessor3_Model001 Preprocesso…\n#> # … with 19,990 more rows\n\nLet’s look at a plot of these results (per resample):\n\nggplot(extracted, aes(x = penalty, y = num_vars)) + \n  geom_line(aes(group = id, col = id), alpha = .5) + \n  ylab(\"Number of retained predictors\") + \n  scale_x_log10()  + \n  ggtitle(paste(\"mixture = \", params$mixture, \"and\", params$num_terms, \"features\")) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThese results might help guide the choice of the penalty range if more optimization was conducted."
  },
  {
    "objectID": "content/learn/work/tune-text/index.html#session-information",
    "href": "content/learn/work/tune-text/index.html#session-information",
    "title": "Tuning text models",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  broom        * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials        * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr        * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2      * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer        * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip      * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr        * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes      * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang          1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample      * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  stopwords    * 2.3        2021-10-28 [1] CRAN (R 4.2.0)\n#>  textfeatures * 0.3.3      2019-09-03 [1] CRAN (R 4.2.0)\n#>  textrecipes  * 1.0.1      2022-10-06 [1] CRAN (R 4.2.0)\n#>  tibble       * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels   * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune         * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows    * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick    * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/learn/work/case-weights/index.html",
    "href": "content/learn/work/case-weights/index.html",
    "title": "Creating case weights based on time",
    "section": "",
    "text": "To use code in this article, you will need to install the following packages: tidymodels.\nThis article demonstrates how to create and use importance weights in a predictive model. Using importance weights is a way to have our model care more about some observations than others."
  },
  {
    "objectID": "content/learn/work/case-weights/index.html#example-data",
    "href": "content/learn/work/case-weights/index.html#example-data",
    "title": "Creating case weights based on time",
    "section": "Example Data",
    "text": "Example Data\nTo demonstrate we will use the Chicago data from the modeldata package.\n\nlibrary(tidymodels)\ndata(Chicago)\n\nChicago <- Chicago %>%\n  select(ridership, date, one_of(stations))\n\nFrom ?Chicago\n\nThese data are from Kuhn and Johnson (2020) and contain an abbreviated training set for modeling the number of people (in thousands) who enter the Clark and Lake L station.\n\n\nThe date column corresponds to the current date. The columns with station names (Austin through California) are a sample of the columns used in the original analysis (for filesize reasons). These are 14 day lag variables (i.e. date - 14 days). There are columns related to weather and sports team schedules.\n\nFor simplicity, we have limited our view to the date and station variables."
  },
  {
    "objectID": "content/learn/work/case-weights/index.html#creating-weights",
    "href": "content/learn/work/case-weights/index.html#creating-weights",
    "title": "Creating case weights based on time",
    "section": "Creating weights",
    "text": "Creating weights\nThis data set contains daily information from 2001-01-22 to 2016-08-28. We will pretend that it is January 1st, 2016 and we want to predict the ridership for the remainder of 2016 using the date and station variables as predictors. Without any weighting, all the previous observations would have the same influence on the model. This may not be ideal since some observations appear a long time ago and not be as representative of the future as more recent observations.\nWe could just use recent observations to fit the model, ensuring that the training data stays as close to the testing data as possible. While a tempting idea, it would throw out a lot of informative data. Instead let us assign a weight to each observation, related to how long ago the observation was taken. This way we are not completely throwing away any observation; we are only giving less weight to data farther in the past.\nWe need to decide on a way to calculate the case weights. The main thing constraint is that the weight cannot be negative, and it would be nice if today was weighted as 1. So we need a function that is 1 when x = 0 and decreasing otherwise. There are many kinds of functions like that, and we will be using this exponential decay function\n\\[ weight = base ^ x \\]\nwhere base is some constant and x is the number of days. To make sure that we select a reasonable base, we need to do some manual testing, starting with looking at how old the oldest observation is.\n\ndifftime(\"2016-01-01\", min(Chicago$date))\n#> Time difference of 5457.208 days\n\nUsing this information we can visualize the weight curve, to see if we like the value of base.\n\ntibble_days <- tibble(days = 0:5457)\n\ntibble_days %>%\n  ggplot(aes(days)) +\n  geom_function(fun = ~ 0.99 ^ .x)\n\n\n\n\n\n\n\n\nsetting base to 0.99 appears to be down weighted too much. Any observation more than a year old would have no influence.\nLet us try a few more values to find\n\nmap_dfr(\n  c(0.99, 0.999, 0.9999),\n  ~ tibble_days %>% mutate(base = factor(.x), value = .x ^ days)\n) %>%\n  ggplot(aes(days, value, group = base, color = base)) +\n  geom_line()\n\n\n\n\n\n\n\n\nFrom this, we could pick something around 0.999 since it gives a better balance. Let’s create a small function to help us encode this weight.\n\nweights_from_dates <- function(x, ref) {\n  if_else(\n    condition = x >= ref,\n    true = 1,     # <- Notice that I'm setting any future weight to 1.\n    false = 0.999 ^ as.numeric(difftime(ref, x, units = \"days\"))\n  )\n}\n\nWe then modify Chicago to add a weight column, explicitly making it an importance weight with importance_weight().\n\nChicago <- Chicago %>%\n  mutate(weight = weights_from_dates(date, \"2016-01-01\"),\n         weight = importance_weights(weight))\n\nThis approach to creating importance weights from dates is not limited to cases where we have daily observations. You are free to create similar weights if you have gaps or repeated observations within the same day. Likewise, you don’t need to use days as the unit. Seconds, weeks, or years could be used as well."
  },
  {
    "objectID": "content/learn/work/case-weights/index.html#modeling",
    "href": "content/learn/work/case-weights/index.html#modeling",
    "title": "Creating case weights based on time",
    "section": "Modeling",
    "text": "Modeling\nWe start by splitting up our data into a training and testing set based on the day \"2016-01-01\". We added weights to the data set before splitting it so each set has weights.\n\nChicago_train <- Chicago %>% filter(date < \"2016-01-01\")\nChicago_test <- Chicago %>% filter(date >= \"2016-01-01\")\n\nNext, we are going to create a recipe. The weights won’t have any influence on the preprocessing since none of these operations are supervised and we are using importance weights.\n\nbase_recipe <-\n  recipe(ridership ~ ., data = Chicago_train) %>%\n  # Create date features\n  step_date(date) %>%\n  step_holiday(date, keep_original_cols = FALSE) %>%\n  # Remove any columns with a single unique value\n  step_zv(all_predictors()) %>%\n  # Normalize all the numerical features\n  step_normalize(all_numeric_predictors()) %>%\n  # Perform PCA to reduce the correlation bet the stations\n  step_pca(all_numeric_predictors(), threshold = 0.95)\n\nNext we need to build the rest of the workflow. We use a linear regression specification\n\nlm_spec <-\n  linear_reg() %>%\n  set_engine(\"lm\")\n\nand we add these together in the workflow. To activate the case weights, we use the add_case_weights() function to specify the name of the case weights being used.\n\nlm_wflow <-\n  workflow() %>% \n  add_case_weights(weight) %>%\n  add_recipe(base_recipe) %>%\n  add_model(lm_spec)\n\nlm_wflow\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> 5 Recipe Steps\n#> \n#> • step_date()\n#> • step_holiday()\n#> • step_zv()\n#> • step_normalize()\n#> • step_pca()\n#> \n#> ── Case Weights ──────────────────────────────────────────────────────\n#> weight\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm\n\nWith all that done we can fit the workflow with the usual syntax:\n\nlm_fit <- fit(lm_wflow, data = Chicago_train)\nlm_fit\n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> 5 Recipe Steps\n#> \n#> • step_date()\n#> • step_holiday()\n#> • step_zv()\n#> • step_normalize()\n#> • step_pca()\n#> \n#> ── Case Weights ──────────────────────────────────────────────────────\n#> weight\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> \n#> Call:\n#> stats::lm(formula = ..y ~ ., data = data, weights = weights)\n#> \n#> Coefficients:\n#>   (Intercept)    date_dowMon    date_dowTue    date_dowWed    date_dowThu  \n#>      1.762599      13.307654      14.689027      14.620178      14.382313  \n#>   date_dowFri    date_dowSat  date_monthFeb  date_monthMar  date_monthApr  \n#>     13.695433       1.228233       0.364342       1.348229       1.409897  \n#> date_monthMay  date_monthJun  date_monthJul  date_monthAug  date_monthSep  \n#>      1.188189       2.598296       2.219721       2.406998       1.932061  \n#> date_monthOct  date_monthNov  date_monthDec            PC1            PC2  \n#>      2.655552       0.909007      -0.004751       0.073014      -1.591021  \n#>           PC3            PC4            PC5  \n#>      0.608386      -0.205305      -0.696010"
  },
  {
    "objectID": "content/learn/work/case-weights/index.html#session-information",
    "href": "content/learn/work/case-weights/index.html#session-information",
    "title": "Creating case weights based on time",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-03\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/author/davis/index.html",
    "href": "content/author/davis/index.html",
    "title": "tidymodels",
    "section": "",
    "text": "Davis Vaughan is a Software Engineer at RStudio. He is the author or maintainer of several R packages for finance and data analytics, including tidyquant, timetk, tibbletime, sweep, rray and hardhat. He is well-known for this work around creating modeling packages in R."
  },
  {
    "objectID": "content/author/max/index.html",
    "href": "content/author/max/index.html",
    "title": "tidymodels",
    "section": "",
    "text": "Dr. Max Kuhn is a Software Engineer at RStudio. He is the author or maintainer of several R packages for predictive modeling including caret, AppliedPredictiveModeling, Cubist, C50 and SparseLDA. He routinely teaches classes in predictive modeling at Predictive Analytics World and UseR! and his publications include work on neuroscience biomarkers, drug discovery, molecular diagnostics and response surface methodology."
  },
  {
    "objectID": "content/start/models/index.html",
    "href": "content/start/models/index.html",
    "title": "Build a model",
    "section": "",
    "text": "How do you create a statistical model using tidymodels? In this article, we will walk you through the steps. We start with data for modeling, learn how to specify and train models with different engines using the parsnip package, and understand why these functions are designed this way.\nTo use code in this article, you will need to install the following packages: broom.mixed, dotwhisker, readr, rstanarm, and tidymodels.\n\nlibrary(tidymodels)  # for the parsnip package, along with the rest of tidymodels\n\n# Helper packages\nlibrary(readr)       # for importing data\nlibrary(broom.mixed) # for converting bayesian models to tidy tibbles\nlibrary(dotwhisker)  # for visualizing regression results"
  },
  {
    "objectID": "content/start/models/index.html#data",
    "href": "content/start/models/index.html#data",
    "title": "Build a model",
    "section": "The Sea Urchins Data",
    "text": "The Sea Urchins Data\nLet’s use the data from Constable (1993) to explore how three different feeding regimes affect the size of sea urchins over time. The initial size of the sea urchins at the beginning of the experiment probably affects how big they grow as they are fed.\nTo start, let’s read our urchins data into R, which we’ll do by providing readr::read_csv() with a url where our CSV data is located (“https://tidymodels.org/start/models/urchins.csv”):\n\nurchins <-\n  # Data were assembled for a tutorial \n  # at https://www.flutterbys.com.au/stats/tut/tut7.5a.html\n  read_csv(\"https://tidymodels.org/start/models/urchins.csv\") %>% \n  # Change the names to be a little more verbose\n  setNames(c(\"food_regime\", \"initial_volume\", \"width\")) %>% \n  # Factors are very helpful for modeling, so we convert one column\n  mutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\n#> Rows: 72 Columns: 3\n#> ── Column specification ──────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): TREAT\n#> dbl (2): IV, SUTW\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nLet’s take a quick look at the data:\n\nurchins\n#> # A tibble: 72 × 3\n#>    food_regime initial_volume width\n#>    <fct>                <dbl> <dbl>\n#>  1 Initial                3.5 0.01 \n#>  2 Initial                5   0.02 \n#>  3 Initial                8   0.061\n#>  4 Initial               10   0.051\n#>  5 Initial               13   0.041\n#>  6 Initial               13   0.061\n#>  7 Initial               15   0.041\n#>  8 Initial               15   0.071\n#>  9 Initial               16   0.092\n#> 10 Initial               17   0.051\n#> # … with 62 more rows\n\nThe urchins data is a tibble. If you are new to tibbles, the best place to start is the tibbles chapter in R for Data Science. For each of the 72 urchins, we know their:\n\nexperimental feeding regime group (food_regime: either Initial, Low, or High),\nsize in milliliters at the start of the experiment (initial_volume), and\nsuture width at the end of the experiment (width).\n\nAs a first step in modeling, it’s always a good idea to plot the data:\n\nggplot(urchins,\n       aes(x = initial_volume, \n           y = width, \n           group = food_regime, \n           col = food_regime)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\n#> `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe can see that urchins that were larger in volume at the start of the experiment tended to have wider sutures at the end, but the slopes of the lines look different so this effect may depend on the feeding regime condition."
  },
  {
    "objectID": "content/start/models/index.html#build-model",
    "href": "content/start/models/index.html#build-model",
    "title": "Build a model",
    "section": "Build and fit a model",
    "text": "Build and fit a model\nA standard two-way analysis of variance (ANOVA) model makes sense for this dataset because we have both a continuous predictor and a categorical predictor. Since the slopes appear to be different for at least two of the feeding regimes, let’s build a model that allows for two-way interactions. Specifying an R formula with our variables in this way:\n\nwidth ~ initial_volume * food_regime\n\nallows our regression model depending on initial volume to have separate slopes and intercepts for each food regime.\nFor this kind of model, ordinary least squares is a good initial approach. With tidymodels, we start by specifying the functional form of the model that we want using the parsnip package. Since there is a numeric outcome and the model should be linear with slopes and intercepts, the model type is “linear regression”. We can declare this with:\n\nlinear_reg()\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm\n\nThat is pretty underwhelming since, on its own, it doesn’t really do much. However, now that the type of model has been specified, we can think about a method for fitting or training the model, the model engine. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method. The default for linear_reg() is \"lm\" for ordinary least squares, as you can see above. We could set a non-default option instead:\n\nlinear_reg() %>% \n  set_engine(\"keras\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: keras\n\nThe documentation page for linear_reg() lists all the possible engines. We’ll save our model object using the default engine as lm_mod.\n\nlm_mod <- linear_reg()\n\nFrom here, the model can be estimated or trained using the fit() function:\n\nlm_fit <- \n  lm_mod %>% \n  fit(width ~ initial_volume * food_regime, data = urchins)\nlm_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> stats::lm(formula = width ~ initial_volume * food_regime, data = data)\n#> \n#> Coefficients:\n#>                    (Intercept)                  initial_volume  \n#>                      0.0331216                       0.0015546  \n#>                 food_regimeLow                 food_regimeHigh  \n#>                      0.0197824                       0.0214111  \n#>  initial_volume:food_regimeLow  initial_volume:food_regimeHigh  \n#>                     -0.0012594                       0.0005254\n\nPerhaps our analysis requires a description of the model parameter estimates and their statistical properties. Although the summary() function for lm objects can provide that, it gives the results back in an unwieldy format. Many models have a tidy() method that provides the summary results in a more predictable and useful format (e.g. a data frame with standard column names):\n\ntidy(lm_fit)\n#> # A tibble: 6 × 5\n#>   term                            estimate std.error statistic  p.value\n#>   <chr>                              <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)                     0.0331    0.00962      3.44  0.00100 \n#> 2 initial_volume                  0.00155   0.000398     3.91  0.000222\n#> 3 food_regimeLow                  0.0198    0.0130       1.52  0.133   \n#> 4 food_regimeHigh                 0.0214    0.0145       1.47  0.145   \n#> 5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  0.0162  \n#> 6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 0.457\n\nThis kind of output can be used to generate a dot-and-whisker plot of our regression results using the dotwhisker package:\n\ntidy(lm_fit) %>% \n  dwplot(dot_args = list(size = 2, color = \"black\"),\n         whisker_args = list(color = \"black\"),\n         vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2))"
  },
  {
    "objectID": "content/start/models/index.html#predict-model",
    "href": "content/start/models/index.html#predict-model",
    "title": "Build a model",
    "section": "Use a model to predict",
    "text": "Use a model to predict\nThis fitted object lm_fit has the lm model output built-in, which you can access with lm_fit$fit, but there are some benefits to using the fitted parsnip model object when it comes to predicting.\nSuppose that, for a publication, it would be particularly interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml. To create such a graph, we start with some new example data that we will make predictions for, to show in our graph:\n\nnew_points <- expand.grid(initial_volume = 20, \n                          food_regime = c(\"Initial\", \"Low\", \"High\"))\nnew_points\n#>   initial_volume food_regime\n#> 1             20     Initial\n#> 2             20         Low\n#> 3             20        High\n\nTo get our predicted results, we can use the predict() function to find the mean values at 20ml.\nIt is also important to communicate the variability, so we also need to find the predicted confidence intervals. If we had used lm() to fit the model directly, a few minutes of reading the documentation page for predict.lm() would explain how to do this. However, if we decide to use a different model to estimate urchin size (spoiler: we will!), it is likely that a completely different syntax would be required.\nInstead, with tidymodels, the types of predicted values are standardized so that we can use the same syntax to get these values.\nFirst, let’s generate the mean body width values:\n\nmean_pred <- predict(lm_fit, new_data = new_points)\nmean_pred\n#> # A tibble: 3 × 1\n#>    .pred\n#>    <dbl>\n#> 1 0.0642\n#> 2 0.0588\n#> 3 0.0961\n\nWhen making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format:\n\nconf_int_pred <- predict(lm_fit, \n                         new_data = new_points, \n                         type = \"conf_int\")\nconf_int_pred\n#> # A tibble: 3 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1      0.0555      0.0729\n#> 2      0.0499      0.0678\n#> 3      0.0870      0.105\n\n# Now combine: \nplot_data <- \n  new_points %>% \n  bind_cols(mean_pred) %>% \n  bind_cols(conf_int_pred)\n\n# and plot:\nggplot(plot_data, aes(x = food_regime)) + \n  geom_point(aes(y = .pred)) + \n  geom_errorbar(aes(ymin = .pred_lower, \n                    ymax = .pred_upper),\n                width = .2) + \n  labs(y = \"urchin size\")"
  },
  {
    "objectID": "content/start/models/index.html#new-engine",
    "href": "content/start/models/index.html#new-engine",
    "title": "Build a model",
    "section": "Model with a different engine",
    "text": "Model with a different engine\nEvery one on your team is happy with that plot except that one person who just read their first book on Bayesian analysis. They are interested in knowing if the results would be different if the model were estimated using a Bayesian approach. In such an analysis, a prior distribution needs to be declared for each model parameter that represents the possible values of the parameters (before being exposed to the observed data). After some discussion, the group agrees that the priors should be bell-shaped but, since no one has any idea what the range of values should be, to take a conservative approach and make the priors wide using a Cauchy distribution (which is the same as a t-distribution with a single degree of freedom).\nThe documentation on the rstanarm package shows us that the stan_glm() function can be used to estimate this model, and that the function arguments that need to be specified are called prior and prior_intercept. It turns out that linear_reg() has a stan engine. Since these prior distribution arguments are specific to the Stan software, they are passed as arguments to parsnip::set_engine(). After that, the same exact fit() call is used:\n\n# set the prior distribution\nprior_dist <- rstanarm::student_t(df = 1)\n\nset.seed(123)\n\n# make the parsnip model\nbayes_mod <-   \n  linear_reg() %>% \n  set_engine(\"stan\", \n             prior_intercept = prior_dist, \n             prior = prior_dist) \n\n# train the model\nbayes_fit <- \n  bayes_mod %>% \n  fit(width ~ initial_volume * food_regime, data = urchins)\n\nprint(bayes_fit, digits = 5)\n#> parsnip model object\n#> \n#> stan_glm\n#>  family:       gaussian [identity]\n#>  formula:      width ~ initial_volume * food_regime\n#>  observations: 72\n#>  predictors:   6\n#> ------\n#>                                Median   MAD_SD  \n#> (Intercept)                     0.03336  0.01003\n#> initial_volume                  0.00156  0.00040\n#> food_regimeLow                  0.01963  0.01308\n#> food_regimeHigh                 0.02120  0.01421\n#> initial_volume:food_regimeLow  -0.00126  0.00051\n#> initial_volume:food_regimeHigh  0.00054  0.00070\n#> \n#> Auxiliary parameter(s):\n#>       Median  MAD_SD \n#> sigma 0.02129 0.00188\n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg\n\nThis kind of Bayesian analysis (like many models) involves randomly generated numbers in its fitting procedure. We can use set.seed() to ensure that the same (pseudo-)random numbers are generated each time we run this code. The number 123 isn’t special or related to our data; it is just a “seed” used to choose random numbers.\nTo update the parameter table, the tidy() method is once again used:\n\ntidy(bayes_fit, conf.int = TRUE)\n#> # A tibble: 6 × 5\n#>   term                            estimate std.error  conf.low conf.high\n#>   <chr>                              <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)                     0.0334    0.0100    0.0179    0.0493  \n#> 2 initial_volume                  0.00156   0.000404  0.000876  0.00219 \n#> 3 food_regimeLow                  0.0196    0.0131   -0.00271   0.0414  \n#> 4 food_regimeHigh                 0.0212    0.0142   -0.00289   0.0455  \n#> 5 initial_volume:food_regimeLow  -0.00126   0.000515 -0.00213  -0.000364\n#> 6 initial_volume:food_regimeHigh  0.000541  0.000696 -0.000669  0.00174\n\nA goal of the tidymodels packages is that the interfaces to common tasks are standardized (as seen in the tidy() results above). The same is true for getting predictions; we can use the same code even though the underlying packages use very different syntax:\n\nbayes_plot_data <- \n  new_points %>% \n  bind_cols(predict(bayes_fit, new_data = new_points)) %>% \n  bind_cols(predict(bayes_fit, new_data = new_points, type = \"conf_int\"))\n\nggplot(bayes_plot_data, aes(x = food_regime)) + \n  geom_point(aes(y = .pred)) + \n  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + \n  labs(y = \"urchin size\") + \n  ggtitle(\"Bayesian model with t(1) prior distribution\")\n\n\n\n\n\n\n\n\nThis isn’t very different from the non-Bayesian results (except in interpretation).\n\n\n\n\n\n\nNote\n\n\n\nThe parsnip package can work with many model types, engines, and arguments. Check out tidymodels.org/find/parsnip to see what is available."
  },
  {
    "objectID": "content/start/models/index.html#why",
    "href": "content/start/models/index.html#why",
    "title": "Build a model",
    "section": "Why does it work that way?",
    "text": "Why does it work that way?\nThe extra step of defining the model using a function like linear_reg() might seem superfluous since a call to lm() is much more succinct. However, the problem with standard modeling functions is that they don’t separate what you want to do from the execution. For example, the process of executing a formula has to happen repeatedly across model calls even when the formula does not change; we can’t recycle those computations.\nAlso, using the tidymodels framework, we can do some interesting things by incrementally creating a model (instead of using single function call). Model tuning with tidymodels uses the specification of the model to declare what parts of the model should be tuned. That would be very difficult to do if linear_reg() immediately fit the model.\nIf you are familiar with the tidyverse, you may have noticed that our modeling code uses the magrittr pipe (%>%). With dplyr and other tidyverse packages, the pipe works well because all of the functions take the data as the first argument. For example:\n\nurchins %>% \n  group_by(food_regime) %>% \n  summarize(med_vol = median(initial_volume))\n#> # A tibble: 3 × 2\n#>   food_regime med_vol\n#>   <fct>         <dbl>\n#> 1 Initial        20.5\n#> 2 Low            19.2\n#> 3 High           15\n\nwhereas the modeling code uses the pipe to pass around the model object:\n\nbayes_mod %>% \n  fit(width ~ initial_volume * food_regime, data = urchins)\n\nThis may seem jarring if you have used dplyr a lot, but it is extremely similar to how ggplot2 operates:\n\nggplot(urchins,\n       aes(initial_volume, width)) +      # returns a ggplot object \n  geom_jitter() +                         # same\n  geom_smooth(method = lm, se = FALSE) +  # same                    \n  labs(x = \"Volume\", y = \"Width\")         # etc"
  },
  {
    "objectID": "content/start/models/index.html#session-info",
    "href": "content/start/models/index.html#session-info",
    "title": "Build a model",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package     * version    date (UTC) lib source\n#>  broom       * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  broom.mixed * 0.2.9.4    2022-04-17 [1] CRAN (R 4.2.0)\n#>  dials       * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dotwhisker  * 0.7.4      2021-09-02 [1] CRAN (R 4.2.0)\n#>  dplyr       * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2     * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer       * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip     * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr       * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  readr       * 2.1.3      2022-10-01 [1] CRAN (R 4.2.0)\n#>  recipes     * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang         1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample     * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  rstanarm    * 2.21.3     2022-04-09 [1] CRAN (R 4.2.0)\n#>  tibble      * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels  * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune        * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows   * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick   * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/start/index.html",
    "href": "content/start/index.html",
    "title": "Get Started",
    "section": "",
    "text": "Here, learn what you need to get started with tidymodels in five articles, starting with how to create a model and ending with a beginning-to-end modeling case study. After you are comfortable with these basics, you can learn how to go farther with tidymodels."
  },
  {
    "objectID": "content/start/index.html#if-you-are-new-to-r-or-the-tidyverse",
    "href": "content/start/index.html#if-you-are-new-to-r-or-the-tidyverse",
    "title": "Get Started",
    "section": "If you are new to R or the tidyverse",
    "text": "If you are new to R or the tidyverse\nTo get the most out of tidymodels, we recommend that you start by learning some basics about R and the tidyverse first, then return here when you feel ready. Here are some resources to start learning:\n\nFinding Your Way To R, from the RStudio Education team.\nLearn the tidyverse, from the tidyverse team.\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse."
  },
  {
    "objectID": "content/start/index.html#getting-started",
    "href": "content/start/index.html#getting-started",
    "title": "Get Started",
    "section": "Getting Started:",
    "text": "Getting Started:\n\nBuild a model\nPreprocess your data with recipes\nEvaluate your model with resampling\nTune model parameters\nA predictive modeling case study"
  },
  {
    "objectID": "content/start/resampling/index.html",
    "href": "content/start/resampling/index.html",
    "title": "Evaluate your model with resampling",
    "section": "",
    "text": "So far, we have built a model and preprocessed data with a recipe. We also introduced workflows as a way to bundle a parsnip model and recipe together. Once we have a model trained, we need a way to measure how well that model predicts new data. This tutorial explains how to characterize model performance based on resampling statistics.\nTo use code in this article, you will need to install the following packages: modeldata, ranger, and tidymodels.\n\nlibrary(tidymodels) # for the rsample package, along with the rest of tidymodels\n\n# Helper packages\nlibrary(modeldata)  # for the cells data"
  },
  {
    "objectID": "content/start/resampling/index.html#data",
    "href": "content/start/resampling/index.html#data",
    "title": "Evaluate your model with resampling",
    "section": "The cell image data",
    "text": "The cell image data\nLet’s use data from Hill, LaPan, Li, and Haney (2007), available in the modeldata package, to predict cell image segmentation quality with resampling. To start, we load this data into R:\n\ndata(cells, package = \"modeldata\")\ncells\n#> # A tibble: 2,019 × 58\n#>    case  class angle_c…¹ area_…² avg_i…³ avg_i…⁴ avg_i…⁵ avg_i…⁶ conve…⁷ conve…⁸\n#>    <fct> <fct>     <dbl>   <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#>  1 Test  PS       143.       185    15.7    4.95    9.55    2.21    1.12   0.920\n#>  2 Train PS       134.       819    31.9  207.     69.9   164.      1.26   0.797\n#>  3 Train WS       107.       431    28.0  116.     63.9   107.      1.05   0.935\n#>  4 Train PS        69.2      298    19.5  102.     28.2    31.0     1.20   0.866\n#>  5 Test  PS         2.89     285    24.3  112.     20.5    40.6     1.11   0.957\n#>  6 Test  WS        40.7      172   326.   654.    129.    347.      1.01   0.993\n#>  7 Test  WS       174.       177   260.   596.    124.    273.      1.01   0.984\n#>  8 Test  PS       180.       251    18.3    5.73   17.2     1.55    1.20   0.831\n#>  9 Test  WS        18.9      495    16.1   89.5    13.7    51.4     1.19   0.822\n#> 10 Test  WS       153.       384    17.7   89.9    20.4    63.1     1.16   0.865\n#> # … with 2,009 more rows, 48 more variables: diff_inten_density_ch_1 <dbl>,\n#> #   diff_inten_density_ch_3 <dbl>, diff_inten_density_ch_4 <dbl>,\n#> #   entropy_inten_ch_1 <dbl>, entropy_inten_ch_3 <dbl>,\n#> #   entropy_inten_ch_4 <dbl>, eq_circ_diam_ch_1 <dbl>,\n#> #   eq_ellipse_lwr_ch_1 <dbl>, eq_ellipse_oblate_vol_ch_1 <dbl>,\n#> #   eq_ellipse_prolate_vol_ch_1 <dbl>, eq_sphere_area_ch_1 <dbl>,\n#> #   eq_sphere_vol_ch_1 <dbl>, fiber_align_2_ch_3 <dbl>, …\n\nWe have data for 2019 cells, with 58 variables. The main outcome variable of interest for us here is called class, which you can see is a factor. But before we jump into predicting the class variable, we need to understand it better. Below is a brief primer on cell image segmentation.\n\nPredicting image segmentation quality\nSome biologists conduct experiments on cells. In drug discovery, a particular type of cell can be treated with either a drug or control and then observed to see what the effect is (if any). A common approach for this kind of measurement is cell imaging. Different parts of the cells can be colored so that the locations of a cell can be determined.\nFor example, in top panel of this image of five cells, the green color is meant to define the boundary of the cell (coloring something called the cytoskeleton) while the blue color defines the nucleus of the cell.\n\n\n\n\n\n\n\n\n\nUsing these colors, the cells in an image can be segmented so that we know which pixels belong to which cell. If this is done well, the cell can be measured in different ways that are important to the biology. Sometimes the shape of the cell matters and different mathematical tools are used to summarize characteristics like the size or “oblongness” of the cell.\nThe bottom panel shows some segmentation results. Cells 1 and 5 are fairly well segmented. However, cells 2 to 4 are bunched up together because the segmentation was not very good. The consequence of bad segmentation is data contamination; when the biologist analyzes the shape or size of these cells, the data are inaccurate and could lead to the wrong conclusion.\nA cell-based experiment might involve millions of cells so it is unfeasible to visually assess them all. Instead, a subsample can be created and these cells can be manually labeled by experts as either poorly segmented (PS) or well-segmented (WS). If we can predict these labels accurately, the larger data set can be improved by filtering out the cells most likely to be poorly segmented.\n\n\nBack to the cells data\nThe cells data has class labels for 2019 cells — each cell is labeled as either poorly segmented (PS) or well-segmented (WS). Each also has a total of 56 predictors based on automated image analysis measurements. For example, avg_inten_ch_1 is the mean intensity of the data contained in the nucleus, area_ch_1 is the total size of the cell, and so on (some predictors are fairly arcane in nature).\n\ncells\n#> # A tibble: 2,019 × 58\n#>    case  class angle_c…¹ area_…² avg_i…³ avg_i…⁴ avg_i…⁵ avg_i…⁶ conve…⁷ conve…⁸\n#>    <fct> <fct>     <dbl>   <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#>  1 Test  PS       143.       185    15.7    4.95    9.55    2.21    1.12   0.920\n#>  2 Train PS       134.       819    31.9  207.     69.9   164.      1.26   0.797\n#>  3 Train WS       107.       431    28.0  116.     63.9   107.      1.05   0.935\n#>  4 Train PS        69.2      298    19.5  102.     28.2    31.0     1.20   0.866\n#>  5 Test  PS         2.89     285    24.3  112.     20.5    40.6     1.11   0.957\n#>  6 Test  WS        40.7      172   326.   654.    129.    347.      1.01   0.993\n#>  7 Test  WS       174.       177   260.   596.    124.    273.      1.01   0.984\n#>  8 Test  PS       180.       251    18.3    5.73   17.2     1.55    1.20   0.831\n#>  9 Test  WS        18.9      495    16.1   89.5    13.7    51.4     1.19   0.822\n#> 10 Test  WS       153.       384    17.7   89.9    20.4    63.1     1.16   0.865\n#> # … with 2,009 more rows, 48 more variables: diff_inten_density_ch_1 <dbl>,\n#> #   diff_inten_density_ch_3 <dbl>, diff_inten_density_ch_4 <dbl>,\n#> #   entropy_inten_ch_1 <dbl>, entropy_inten_ch_3 <dbl>,\n#> #   entropy_inten_ch_4 <dbl>, eq_circ_diam_ch_1 <dbl>,\n#> #   eq_ellipse_lwr_ch_1 <dbl>, eq_ellipse_oblate_vol_ch_1 <dbl>,\n#> #   eq_ellipse_prolate_vol_ch_1 <dbl>, eq_sphere_area_ch_1 <dbl>,\n#> #   eq_sphere_vol_ch_1 <dbl>, fiber_align_2_ch_3 <dbl>, …\n\nThe rates of the classes are somewhat imbalanced; there are more poorly segmented cells than well-segmented cells:\n\ncells %>% \n  count(class) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   class     n  prop\n#>   <fct> <int> <dbl>\n#> 1 PS     1300 0.644\n#> 2 WS      719 0.356"
  },
  {
    "objectID": "content/start/resampling/index.html#data-split",
    "href": "content/start/resampling/index.html#data-split",
    "title": "Evaluate your model with resampling",
    "section": "Data splitting",
    "text": "Data splitting\nIn our previous Preprocess your data with recipes article, we started by splitting our data. It is common when beginning a modeling project to separate the data set into two partitions:\n\nThe training set is used to estimate parameters, compare models and feature engineering techniques, tune models, etc.\nThe test set is held in reserve until the end of the project, at which point there should only be one or two models under serious consideration. It is used as an unbiased source for measuring final model performance.\n\nThere are different ways to create these partitions of the data. The most common approach is to use a random sample. Suppose that one quarter of the data were reserved for the test set. Random sampling would randomly select 25% for the test set and use the remainder for the training set. We can use the rsample package for this purpose.\nSince random sampling uses random numbers, it is important to set the random number seed. This ensures that the random numbers can be reproduced at a later time (if needed).\nThe function rsample::initial_split() takes the original data and saves the information on how to make the partitions. In the original analysis, the authors made their own training/test set and that information is contained in the column case. To demonstrate how to make a split, we’ll remove this column before we make our own split:\n\nset.seed(123)\ncell_split <- initial_split(cells %>% select(-case), \n                            strata = class)\n\nHere we used the strata argument, which conducts a stratified split. This ensures that, despite the imbalance we noticed in our class variable, our training and test data sets will keep roughly the same proportions of poorly and well-segmented cells as in the original data. After the initial_split, the training() and testing() functions return the actual data sets.\n\ncell_train <- training(cell_split)\ncell_test  <- testing(cell_split)\n\nnrow(cell_train)\n#> [1] 1514\nnrow(cell_train)/nrow(cells)\n#> [1] 0.7498762\n\n# training set proportions by class\ncell_train %>% \n  count(class) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   class     n  prop\n#>   <fct> <int> <dbl>\n#> 1 PS      975 0.644\n#> 2 WS      539 0.356\n\n# test set proportions by class\ncell_test %>% \n  count(class) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   class     n  prop\n#>   <fct> <int> <dbl>\n#> 1 PS      325 0.644\n#> 2 WS      180 0.356\n\nThe majority of the modeling work is then conducted on the training set data."
  },
  {
    "objectID": "content/start/resampling/index.html#modeling",
    "href": "content/start/resampling/index.html#modeling",
    "title": "Evaluate your model with resampling",
    "section": "Modeling",
    "text": "Modeling\nRandom forest models are ensembles of decision trees. A large number of decision tree models are created for the ensemble based on slightly different versions of the training set. When creating the individual decision trees, the fitting process encourages them to be as diverse as possible. The collection of trees are combined into the random forest model and, when a new sample is predicted, the votes from each tree are used to calculate the final predicted value for the new sample. For categorical outcome variables like class in our cells data example, the majority vote across all the trees in the random forest determines the predicted class for the new sample.\nOne of the benefits of a random forest model is that it is very low maintenance; it requires very little preprocessing of the data and the default parameters tend to give reasonable results. For that reason, we won’t create a recipe for the cells data.\nAt the same time, the number of trees in the ensemble should be large (in the thousands) and this makes the model moderately expensive to compute.\nTo fit a random forest model on the training set, let’s use the parsnip package with the ranger engine. We first define the model that we want to create:\n\nrf_mod <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"classification\")\n\nStarting with this parsnip model object, the fit() function can be used with a model formula. Since random forest models use random numbers, we again set the seed prior to computing:\n\nset.seed(234)\nrf_fit <- \n  rf_mod %>% \n  fit(class ~ ., data = cell_train)\nrf_fit\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  1000 \n#> Sample size:                      1514 \n#> Number of independent variables:  56 \n#> Mtry:                             7 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1189338\n\nThis new rf_fit object is our fitted model, trained on our training data set."
  },
  {
    "objectID": "content/start/resampling/index.html#performance",
    "href": "content/start/resampling/index.html#performance",
    "title": "Evaluate your model with resampling",
    "section": "Estimating performance",
    "text": "Estimating performance\nDuring a modeling project, we might create a variety of different models. To choose between them, we need to consider how well these models do, as measured by some performance statistics. In our example in this article, some options we could use are:\n\nthe area under the Receiver Operating Characteristic (ROC) curve, and\noverall classification accuracy.\n\nThe ROC curve uses the class probability estimates to give us a sense of performance across the entire set of potential probability cutoffs. Overall accuracy uses the hard class predictions to measure performance. The hard class predictions tell us whether our model predicted PS or WS for each cell. But, behind those predictions, the model is actually estimating a probability. A simple 50% probability cutoff is used to categorize a cell as poorly segmented.\nThe yardstick package has functions for computing both of these measures called roc_auc() and accuracy().\nAt first glance, it might seem like a good idea to use the training set data to compute these statistics. (This is actually a very bad idea.) Let’s see what happens if we try this. To evaluate performance based on the training set, we call the predict() method to get both types of predictions (i.e. probabilities and hard class predictions).\n\nrf_training_pred <- \n  predict(rf_fit, cell_train) %>% \n  bind_cols(predict(rf_fit, cell_train, type = \"prob\")) %>% \n  # Add the true outcome data back in\n  bind_cols(cell_train %>% \n              select(class))\n\nUsing the yardstick functions, this model has spectacular results, so spectacular that you might be starting to get suspicious:\n\nrf_training_pred %>%                # training set predictions\n  roc_auc(truth = class, .pred_PS)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary          1.00\nrf_training_pred %>%                # training set predictions\n  accuracy(truth = class, .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.991\n\nNow that we have this model with exceptional performance, we proceed to the test set. Unfortunately, we discover that, although our results aren’t bad, they are certainly worse than what we initially thought based on predicting the training set:\n\nrf_testing_pred <- \n  predict(rf_fit, cell_test) %>% \n  bind_cols(predict(rf_fit, cell_test, type = \"prob\")) %>% \n  bind_cols(cell_test %>% select(class))\n\n\nrf_testing_pred %>%                   # test set predictions\n  roc_auc(truth = class, .pred_PS)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.891\nrf_testing_pred %>%                   # test set predictions\n  accuracy(truth = class, .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.816\n\n\nWhat happened here?\nThere are several reasons why training set statistics like the ones shown in this section can be unrealistically optimistic:\n\nModels like random forests, neural networks, and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results.\nThe training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows.\n\nTo understand that second point better, think about an analogy from teaching. Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test."
  },
  {
    "objectID": "content/start/resampling/index.html#resampling",
    "href": "content/start/resampling/index.html#resampling",
    "title": "Evaluate your model with resampling",
    "section": "Resampling to the rescue",
    "text": "Resampling to the rescue\nResampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data are used for creating the model and a different subset is used to measure performance. Resampling is always used with the training set. This schematic from Kuhn and Johnson (2019) illustrates data usage for resampling methods:\n\n\n\n\n\n\n\n\n\nIn the first level of this diagram, you see what happens when you use rsample::initial_split(), which splits the original data into training and test sets. Then, the training set is chosen for resampling, and the test set is held out.\nLet’s use 10-fold cross-validation (CV) in this example. This method randomly allocates the 1514 cells in the training set to 10 groups of roughly equal size, called “folds”. For the first iteration of resampling, the first fold of about 151 cells are held out for the purpose of measuring performance. This is similar to a test set but, to avoid confusion, we call these data the assessment set in the tidymodels framework.\nThe other 90% of the data (about 1362 cells) are used to fit the model. Again, this sounds similar to a training set, so in tidymodels we call this data the analysis set. This model, trained on the analysis set, is applied to the assessment set to generate predictions, and performance statistics are computed based on those predictions.\nIn this example, 10-fold CV moves iteratively through the folds and leaves a different 10% out each time for model assessment. At the end of this process, there are 10 sets of performance statistics that were created on 10 data sets that were not used in the modeling process. For the cell example, this means 10 accuracies and 10 areas under the ROC curve. While 10 models were created, these are not used further; we do not keep the models themselves trained on these folds because their only purpose is calculating performance metrics.\nThe final resampling estimates for the model are the averages of the performance statistics replicates. For example, suppose for our data the results were:\n\n\n\n\n \n  \n    resample \n    accuracy \n    roc_auc \n    assessment size \n  \n \n\n  \n    Fold01 \n    0.8289474 \n    0.8937128 \n    152 \n  \n  \n    Fold02 \n    0.7697368 \n    0.8768989 \n    152 \n  \n  \n    Fold03 \n    0.8552632 \n    0.9017666 \n    152 \n  \n  \n    Fold04 \n    0.8552632 \n    0.8928076 \n    152 \n  \n  \n    Fold05 \n    0.7947020 \n    0.8816342 \n    151 \n  \n  \n    Fold06 \n    0.8476821 \n    0.9244306 \n    151 \n  \n  \n    Fold07 \n    0.8145695 \n    0.8960339 \n    151 \n  \n  \n    Fold08 \n    0.8543046 \n    0.9267677 \n    151 \n  \n  \n    Fold09 \n    0.8543046 \n    0.9231392 \n    151 \n  \n  \n    Fold10 \n    0.8476821 \n    0.9266917 \n    151 \n  \n\n\n\n\n\nFrom these resampling statistics, the final estimate of performance for this random forest model would be 0.904 for the area under the ROC curve and 0.832 for accuracy.\nThese resampling statistics are an effective method for measuring model performance without predicting the training set directly as a whole."
  },
  {
    "objectID": "content/start/resampling/index.html#fit-resamples",
    "href": "content/start/resampling/index.html#fit-resamples",
    "title": "Evaluate your model with resampling",
    "section": "Fit a model with resampling",
    "text": "Fit a model with resampling\nTo generate these results, the first step is to create a resampling object using rsample. There are several resampling methods implemented in rsample; cross-validation folds can be created using vfold_cv():\n\nset.seed(345)\nfolds <- vfold_cv(cell_train, v = 10)\nfolds\n#> #  10-fold cross-validation \n#> # A tibble: 10 × 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [1362/152]> Fold01\n#>  2 <split [1362/152]> Fold02\n#>  3 <split [1362/152]> Fold03\n#>  4 <split [1362/152]> Fold04\n#>  5 <split [1363/151]> Fold05\n#>  6 <split [1363/151]> Fold06\n#>  7 <split [1363/151]> Fold07\n#>  8 <split [1363/151]> Fold08\n#>  9 <split [1363/151]> Fold09\n#> 10 <split [1363/151]> Fold10\n\nThe list column for splits contains the information on which rows belong in the analysis and assessment sets. There are functions that can be used to extract the individual resampled data called analysis() and assessment().\nHowever, the tune package contains high-level functions that can do the required computations to resample a model for the purpose of measuring performance. You have several options for building an object for resampling:\n\nResample a model specification preprocessed with a formula or recipe, or\nResample a workflow() that bundles together a model specification and formula/recipe.\n\nFor this example, let’s use a workflow() that bundles together the random forest model and a formula, since we are not using a recipe. Whichever of these options you use, the syntax to fit_resamples() is very similar to fit():\n\nrf_wf <- \n  workflow() %>%\n  add_model(rf_mod) %>%\n  add_formula(class ~ .)\n\nset.seed(456)\nrf_fit_rs <- \n  rf_wf %>% \n  fit_resamples(folds)\n\n\nrf_fit_rs\n#> # Resampling results\n#> # 10-fold cross-validation \n#> # A tibble: 10 × 4\n#>    splits             id     .metrics         .notes          \n#>    <list>             <chr>  <list>           <list>          \n#>  1 <split [1362/152]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  2 <split [1362/152]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  3 <split [1362/152]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  4 <split [1362/152]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  5 <split [1363/151]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  6 <split [1363/151]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  7 <split [1363/151]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  8 <split [1363/151]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  9 <split [1363/151]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 10 <split [1363/151]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]>\n\nThe results are similar to the folds results with some extra columns. The column .metrics contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\ncollect_metrics(rf_fit_rs)\n#> # A tibble: 2 × 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.832    10 0.00952 Preprocessor1_Model1\n#> 2 roc_auc  binary     0.904    10 0.00610 Preprocessor1_Model1\n\nThink about these values we now have for accuracy and AUC. These performance metrics are now more realistic (i.e. lower) than our ill-advised first attempt at computing performance metrics in the section above. If we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance. We have looked at this once already before we started using resampling, but let’s remind ourselves of the results:\n\nrf_testing_pred %>%                   # test set predictions\n  roc_auc(truth = class, .pred_PS)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.891\nrf_testing_pred %>%                   # test set predictions\n  accuracy(truth = class, .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.816\n\nThe performance metrics from the test set are much closer to the performance metrics computed using resampling than our first (“bad idea”) attempt. Resampling allows us to simulate how well our model will perform on new data, and the test set acts as the final, unbiased check for our model’s performance."
  },
  {
    "objectID": "content/start/resampling/index.html#session-information",
    "href": "content/start/resampling/index.html#session-information",
    "title": "Evaluate your model with resampling",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  modeldata  * 1.0.1      2022-09-06 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  ranger     * 0.14.1     2022-06-18 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/start/recipes/index.html",
    "href": "content/start/recipes/index.html",
    "title": "Preprocess your data with recipes",
    "section": "",
    "text": "In our Build a Model article, we learned how to specify and train models with different engines using the parsnip package. In this article, we’ll explore another tidymodels package, recipes, which is designed to help you preprocess your data before training your model. Recipes are built as a series of preprocessing steps, such as:\n\nconverting qualitative predictors to indicator variables (also known as dummy variables),\ntransforming data to be on a different scale (e.g., taking the logarithm of a variable),\ntransforming whole groups of predictors together,\nextracting key features from raw variables (e.g., getting the day of the week out of a date variable),\n\nand so on. If you are familiar with R’s formula interface, a lot of this might sound familiar and like what a formula already does. Recipes can be used to do many of the same things, but they have a much wider range of possibilities. This article shows how to use recipes for modeling.\nTo use code in this article, you will need to install the following packages: nycflights13, skimr, and tidymodels.\n\nlibrary(tidymodels)      # for the recipes package, along with the rest of tidymodels\n\n# Helper packages\nlibrary(nycflights13)    # for flight data\nlibrary(skimr)           # for variable summaries"
  },
  {
    "objectID": "content/start/recipes/index.html#data",
    "href": "content/start/recipes/index.html#data",
    "title": "Preprocess your data with recipes",
    "section": "The New York City flight data",
    "text": "The New York City flight data\n\n\n\nLet’s use the nycflights13 data to predict whether a plane arrives more than 30 minutes late. This data set contains information on 325,819 flights departing near New York City in 2013. Let’s start by loading the data and making a few changes to the variables:\n\nset.seed(123)\n\nflight_data <- \n  flights %>% \n  mutate(\n    # Convert the arrival delay to a factor\n    arr_delay = ifelse(arr_delay >= 30, \"late\", \"on_time\"),\n    arr_delay = factor(arr_delay),\n    # We will use the date (not date-time) in the recipe below\n    date = lubridate::as_date(time_hour)\n  ) %>% \n  # Include the weather data\n  inner_join(weather, by = c(\"origin\", \"time_hour\")) %>% \n  # Only retain the specific columns we will use\n  select(dep_time, flight, origin, dest, air_time, distance, \n         carrier, date, arr_delay, time_hour) %>% \n  # Exclude missing data\n  na.omit() %>% \n  # For creating models, it is better to have qualitative columns\n  # encoded as factors (instead of character strings)\n  mutate_if(is.character, as.factor)\n\nWe can see that about 16% of the flights in this data set arrived more than 30 minutes late.\n\nflight_data %>% \n  count(arr_delay) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   arr_delay      n  prop\n#>   <fct>      <int> <dbl>\n#> 1 late       52540 0.161\n#> 2 on_time   273279 0.839\n\nBefore we start building up our recipe, let’s take a quick look at a few specific variables that will be important for both preprocessing and modeling.\nFirst, notice that the variable we created called arr_delay is a factor variable; it is important that our outcome variable for training a logistic regression model is a factor.\n\nglimpse(flight_data)\n#> Rows: 325,819\n#> Columns: 10\n#> $ dep_time  <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, 558, …\n#> $ flight    <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 49, 71…\n#> $ origin    <fct> EWR, LGA, JFK, JFK, LGA, EWR, EWR, LGA, JFK, LGA, JFK, JFK, …\n#> $ dest      <fct> IAH, IAH, MIA, BQN, ATL, ORD, FLL, IAD, MCO, ORD, PBI, TPA, …\n#> $ air_time  <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 158, 3…\n#> $ distance  <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, 1028,…\n#> $ carrier   <fct> UA, UA, AA, B6, DL, UA, B6, EV, B6, AA, B6, B6, UA, UA, AA, …\n#> $ date      <date> 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01,…\n#> $ arr_delay <fct> on_time, on_time, late, on_time, on_time, on_time, on_time, …\n#> $ time_hour <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 05:00:…\n\nSecond, there are two variables that we don’t want to use as predictors in our model, but that we would like to retain as identification variables that can be used to troubleshoot poorly predicted data points. These are flight, a numeric value, and time_hour, a date-time value.\nThird, there are 104 flight destinations contained in dest and 16 distinct carriers.\n\nflight_data %>% \n  skimr::skim(dest, carrier) \n\n\n\n\nData summary\n\n\n\n\nName\n\n\nPiped data\n\n\n\n\nNumber of rows\n\n\n325819\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n2\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\ndest\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n104\n\n\nATL: 16771, ORD: 16507, LAX: 15942, BOS: 14948\n\n\n\n\ncarrier\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n16\n\n\nUA: 57489, B6: 53715, EV: 50868, DL: 47465\n\n\n\n\n\n\n\nBecause we’ll be using a simple logistic regression model, the variables dest and carrier will be converted to dummy variables. However, some of these values do not occur very frequently and this could complicate our analysis. We’ll discuss specific steps later in this article that we can add to our recipe to address this issue before modeling."
  },
  {
    "objectID": "content/start/recipes/index.html#data-split",
    "href": "content/start/recipes/index.html#data-split",
    "title": "Preprocess your data with recipes",
    "section": "Data splitting",
    "text": "Data splitting\nTo get started, let’s split this single dataset into two: a training set and a testing set. We’ll keep most of the rows in the original dataset (subset chosen randomly) in the training set. The training data will be used to fit the model, and the testing set will be used to measure model performance.\nTo do this, we can use the rsample package to create an object that contains the information on how to split the data, and then two more rsample functions to create data frames for the training and testing sets:\n\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible when random numbers are used \nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(flight_data, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)"
  },
  {
    "objectID": "content/start/recipes/index.html#recipe",
    "href": "content/start/recipes/index.html#recipe",
    "title": "Preprocess your data with recipes",
    "section": "Create recipe and roles",
    "text": "Create recipe and roles\nTo get started, let’s create a recipe for a simple logistic regression model. Before training the model, we can use a recipe to create a few new predictors and conduct some preprocessing required by the model.\nLet’s initiate a new recipe:\n\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) \n\nThe recipe() function as we used it here has two arguments:\n\nA formula. Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, arr_delay). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.\nThe data. A recipe is associated with the data set used to create the model. This will typically be the training set, so data = train_data here. Naming a data set doesn’t actually change the data itself; it is only used to catalog the names of the variables and their types, like factors, integers, dates, etc.\n\nNow we can add roles to this recipe. We can use the update_role() function to let recipes know that flight and time_hour are variables with a custom role that we called \"ID\" (a role can have any character value). Whereas our formula included all variables in the training set other than arr_delay as predictors, this tells the recipe to keep these two variables but not use them as either outcomes or predictors.\n\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") \n\nThis step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model. This can be convenient when, after the model is fit, we want to investigate some poorly predicted value. These ID columns will be available and can be used to try to understand what went wrong.\nTo get the current set of variables and roles, use the summary() function:\n\nsummary(flights_rec)\n#> # A tibble: 10 × 4\n#>    variable  type      role      source  \n#>    <chr>     <list>    <chr>     <chr>   \n#>  1 dep_time  <chr [2]> predictor original\n#>  2 flight    <chr [2]> ID        original\n#>  3 origin    <chr [3]> predictor original\n#>  4 dest      <chr [3]> predictor original\n#>  5 air_time  <chr [2]> predictor original\n#>  6 distance  <chr [2]> predictor original\n#>  7 carrier   <chr [3]> predictor original\n#>  8 date      <chr [1]> predictor original\n#>  9 time_hour <chr [1]> ID        original\n#> 10 arr_delay <chr [3]> outcome   original"
  },
  {
    "objectID": "content/start/recipes/index.html#features",
    "href": "content/start/recipes/index.html#features",
    "title": "Preprocess your data with recipes",
    "section": "Create features",
    "text": "Create features\nNow we can start adding steps onto our recipe using the pipe operator. Perhaps it is reasonable for the date of the flight to have an effect on the likelihood of a late arrival. A little bit of feature engineering might go a long way to improving our model. How should the date be encoded into the model? The date column has an R date object so including that column “as is” will mean that the model will convert it to a numeric format equal to the number of days after a reference date:\n\nflight_data %>% \n  distinct(date) %>% \n  mutate(numeric_date = as.numeric(date)) \n#> # A tibble: 364 × 2\n#>    date       numeric_date\n#>    <date>            <dbl>\n#>  1 2013-01-01        15706\n#>  2 2013-01-02        15707\n#>  3 2013-01-03        15708\n#>  4 2013-01-04        15709\n#>  5 2013-01-05        15710\n#>  6 2013-01-06        15711\n#>  7 2013-01-07        15712\n#>  8 2013-01-08        15713\n#>  9 2013-01-09        15714\n#> 10 2013-01-10        15715\n#> # … with 354 more rows\n\nIt’s possible that the numeric date variable is a good option for modeling; perhaps the model would benefit from a linear trend between the log-odds of a late arrival and the numeric date variable. However, it might be better to add model terms derived from the date that have a better potential to be important to the model. For example, we could derive the following meaningful features from the single date variable:\n\nthe day of the week,\nthe month, and\nwhether or not the date corresponds to a holiday.\n\nLet’s do all three of these by adding steps to our recipe:\n\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") %>% \n  step_date(date, features = c(\"dow\", \"month\")) %>%               \n  step_holiday(date, \n               holidays = timeDate::listHolidays(\"US\"), \n               keep_original_cols = FALSE)\n\nWhat do each of these steps do?\n\nWith step_date(), we created two new factor columns with the appropriate day of the week and the month.\nWith step_holiday(), we created a binary variable indicating whether the current date is a holiday or not. The argument value of timeDate::listHolidays(\"US\") uses the timeDate package to list the 17 standard US holidays.\nWith keep_original_cols = FALSE, we remove the original date variable since we no longer want it in the model. Many recipe steps that create new variables have this argument.\n\nNext, we’ll turn our attention to the variable types of our predictors. Because we plan to train a logistic regression model, we know that predictors will ultimately need to be numeric, as opposed to nominal data like strings and factor variables. In other words, there may be a difference in how we store our data (in factors inside a data frame), and how the underlying equations require them (a purely numeric matrix).\nFor factors like dest and origin, standard practice is to convert them into dummy or indicator variables to make them numeric. These are binary values for each level of the factor. For example, our origin variable has values of \"EWR\", \"JFK\", and \"LGA\". The standard dummy variable encoding, shown below, will create two numeric columns of the data that are 1 when the originating airport is \"JFK\" or \"LGA\" and zero otherwise, respectively.\n\n\n\n\n \n  \n    origin \n    origin_JFK \n    origin_LGA \n  \n \n\n  \n    JFK \n    1 \n    0 \n  \n  \n    EWR \n    0 \n    0 \n  \n  \n    LGA \n    0 \n    1 \n  \n\n\n\n\n\nBut, unlike the standard model formula methods in R, a recipe does not automatically create these dummy variables for you; you’ll need to tell your recipe to add this step. This is for two reasons. First, many models do not require numeric predictors, so dummy variables may not always be preferred. Second, recipes can also be used for purposes outside of modeling, where non-dummy versions of the variables may work better. For example, you may want to make a table or a plot with a variable as a single factor. For those reasons, you need to explicitly tell recipes to create dummy variables using step_dummy():\n\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") %>% \n  step_date(date, features = c(\"dow\", \"month\")) %>%               \n  step_holiday(date, \n               holidays = timeDate::listHolidays(\"US\"), \n               keep_original_cols = FALSE) %>% \n  step_dummy(all_nominal_predictors())\n\nHere, we did something different than before: instead of applying a step to an individual variable, we used selectors to apply this recipe step to several variables at once, all_nominal_predictors(). The selector functions can be combined to select intersections of variables.\nAt this stage in the recipe, this step selects the origin, dest, and carrier variables. It also includes two new variables, date_dow and date_month, that were created by the earlier step_date().\nMore generally, the recipe selectors mean that you don’t always have to apply steps to individual variables one at a time. Since a recipe knows the variable type and role of each column, they can also be selected (or dropped) using this information.\nWe need one final step to add to our recipe. Since carrier and dest have some infrequently occurring factor values, it is possible that dummy variables might be created for values that don’t exist in the training set. For example, there is one destination that is only in the test set:\n\ntest_data %>% \n  distinct(dest) %>% \n  anti_join(train_data)\n#> Joining, by = \"dest\"\n#> # A tibble: 1 × 1\n#>   dest \n#>   <fct>\n#> 1 LEX\n\nWhen the recipe is applied to the training set, a column is made for LEX because the factor levels come from flight_data (not the training set), but this column will contain all zeros. This is a “zero-variance predictor” that has no information within the column. While some R functions will not produce an error for such predictors, it usually causes warnings and other issues. step_zv() will remove columns from the data when the training set data have a single value, so it is added to the recipe after step_dummy():\n\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") %>% \n  step_date(date, features = c(\"dow\", \"month\")) %>%               \n  step_holiday(date, \n               holidays = timeDate::listHolidays(\"US\"), \n               keep_original_cols = FALSE) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n\nNow we’ve created a specification of what should be done with the data. How do we use the recipe we made?"
  },
  {
    "objectID": "content/start/recipes/index.html#fit-workflow",
    "href": "content/start/recipes/index.html#fit-workflow",
    "title": "Preprocess your data with recipes",
    "section": "Fit a model with a recipe",
    "text": "Fit a model with a recipe\nLet’s use logistic regression to model the flight data. As we saw in Build a Model, we start by building a model specification using the parsnip package:\n\nlr_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\nWe will want to use our recipe across several steps as we train and test our model. We will:\n\nProcess the recipe using the training set: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.\nApply the recipe to the training set: We create the final predictor set on the training set.\nApply the recipe to the test set: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.\n\nTo simplify this process, we can use a model workflow, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. We’ll use the workflows package from tidymodels to bundle our parsnip model (lr_mod) with our recipe (flights_rec).\n\nflights_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flights_rec)\n\nflights_wflow\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> 4 Recipe Steps\n#> \n#> • step_date()\n#> • step_holiday()\n#> • step_dummy()\n#> • step_zv()\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n\nNow, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors:\n\nflights_fit <- \n  flights_wflow %>% \n  fit(data = train_data)\n\nThis object has the finalized recipe and fitted model objects inside. You may want to extract the model or recipe objects from the workflow. To do this, you can use the helper functions extract_fit_parsnip() and extract_recipe(). For example, here we pull the fitted model object then use the broom::tidy() function to get a tidy tibble of model coefficients:\n\nflights_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n#> # A tibble: 157 × 5\n#>    term                         estimate std.error statistic  p.value\n#>    <chr>                           <dbl>     <dbl>     <dbl>    <dbl>\n#>  1 (Intercept)                   7.28    2.73           2.67 7.64e- 3\n#>  2 dep_time                     -0.00166 0.0000141   -118.   0       \n#>  3 air_time                     -0.0440  0.000563     -78.2  0       \n#>  4 distance                      0.00507 0.00150        3.38 7.32e- 4\n#>  5 date_USChristmasDay           1.33    0.177          7.49 6.93e-14\n#>  6 date_USColumbusDay            0.724   0.170          4.25 2.13e- 5\n#>  7 date_USCPulaskisBirthday      0.807   0.139          5.80 6.57e- 9\n#>  8 date_USDecorationMemorialDay  0.585   0.117          4.98 6.32e- 7\n#>  9 date_USElectionDay            0.948   0.190          4.98 6.25e- 7\n#> 10 date_USGoodFriday             1.25    0.167          7.45 9.40e-14\n#> # … with 147 more rows"
  },
  {
    "objectID": "content/start/recipes/index.html#predict-workflow",
    "href": "content/start/recipes/index.html#predict-workflow",
    "title": "Preprocess your data with recipes",
    "section": "Use a trained workflow to predict",
    "text": "Use a trained workflow to predict\nOur goal was to predict whether a plane arrives more than 30 minutes late. We have just:\n\nBuilt the model (lr_mod),\nCreated a preprocessing recipe (flights_rec),\nBundled the model and recipe (flights_wflow), and\nTrained our workflow using a single call to fit().\n\nThe next step is to use the trained workflow (flights_fit) to predict with the unseen test data, which we will do with a single call to predict(). The predict() method applies the recipe to the new data, then passes them to the fitted model.\n\npredict(flights_fit, test_data)\n#> # A tibble: 81,455 × 1\n#>    .pred_class\n#>    <fct>      \n#>  1 on_time    \n#>  2 on_time    \n#>  3 on_time    \n#>  4 on_time    \n#>  5 on_time    \n#>  6 on_time    \n#>  7 on_time    \n#>  8 on_time    \n#>  9 on_time    \n#> 10 on_time    \n#> # … with 81,445 more rows\n\nBecause our outcome variable here is a factor, the output from predict() returns the predicted class: late versus on_time. But, let’s say we want the predicted class probabilities for each flight instead. To return those, we can specify type = \"prob\" when we use predict() or use augment() with the model plus test data to save them together:\n\nflights_aug <- \n  augment(flights_fit, test_data)\n\n# The data look like: \nflights_aug %>%\n  select(arr_delay, time_hour, flight, .pred_class, .pred_on_time)\n#> # A tibble: 81,455 × 5\n#>    arr_delay time_hour           flight .pred_class .pred_on_time\n#>    <fct>     <dttm>               <int> <fct>               <dbl>\n#>  1 on_time   2013-01-01 05:00:00   1545 on_time             0.945\n#>  2 on_time   2013-01-01 05:00:00   1714 on_time             0.949\n#>  3 on_time   2013-01-01 06:00:00    507 on_time             0.964\n#>  4 on_time   2013-01-01 06:00:00   5708 on_time             0.961\n#>  5 on_time   2013-01-01 06:00:00     71 on_time             0.962\n#>  6 on_time   2013-01-01 06:00:00    194 on_time             0.975\n#>  7 on_time   2013-01-01 06:00:00   1124 on_time             0.963\n#>  8 on_time   2013-01-01 05:00:00   1806 on_time             0.981\n#>  9 on_time   2013-01-01 06:00:00   1187 on_time             0.935\n#> 10 on_time   2013-01-01 06:00:00   4650 on_time             0.931\n#> # … with 81,445 more rows\n\nNow that we have a tibble with our predicted class probabilities, how will we evaluate the performance of our workflow? We can see from these first few rows that our model predicted these 5 on time flights correctly because the values of .pred_on_time are p > .50. But we also know that we have 81,455 rows total to predict. We would like to calculate a metric that tells how well our model predicted late arrivals, compared to the true status of our outcome variable, arr_delay.\nLet’s use the area under the ROC curve as our metric, computed using roc_curve() and roc_auc() from the yardstick package.\nTo generate a ROC curve, we need the predicted class probabilities for late and on_time, which we just calculated in the code chunk above. We can create the ROC curve with these values, using roc_curve() and then piping to the autoplot() method:\n\nflights_aug %>% \n  roc_curve(truth = arr_delay, .pred_late) %>% \n  autoplot()\n\n\n\n\n\n\n\n\nSimilarly, roc_auc() estimates the area under the curve:\n\nflights_aug %>% \n  roc_auc(truth = arr_delay, .pred_late)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.764\n\nNot too bad! We leave it to the reader to test out this workflow without this recipe. You can use workflows::add_formula(arr_delay ~ .) instead of add_recipe() (remember to remove the identification variables first!), and see whether our recipe improved our model’s ability to predict late arrivals."
  },
  {
    "objectID": "content/start/recipes/index.html#session-info",
    "href": "content/start/recipes/index.html#session-info",
    "title": "Preprocess your data with recipes",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  broom        * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials        * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr        * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2      * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer        * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  nycflights13 * 1.0.2      2021-04-12 [1] CRAN (R 4.2.0)\n#>  parsnip      * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr        * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes      * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang          1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample      * 1.1.1.9000 2022-12-13 [1] local\n#>  skimr        * 2.1.5      2022-12-23 [1] CRAN (R 4.2.0)\n#>  tibble       * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels   * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune         * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows    * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick    * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/start/tuning/index.html",
    "href": "content/start/tuning/index.html",
    "title": "Tune model parameters",
    "section": "",
    "text": "Some model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called hyperparameters. Some examples of hyperparameters include the number of predictors that are sampled at splits in a tree-based model (we call this mtry in tidymodels) or the learning rate in a boosted tree model (we call this learn_rate). Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called tuning.\nTo use code in this article, you will need to install the following packages: rpart, rpart.plot, tidymodels, and vip.\n\nlibrary(tidymodels)  # for the tune package, along with the rest of tidymodels\n\n# Helper packages\nlibrary(rpart.plot)  # for visualizing a decision tree\nlibrary(vip)         # for variable importance plots"
  },
  {
    "objectID": "content/start/tuning/index.html#data",
    "href": "content/start/tuning/index.html#data",
    "title": "Tune model parameters",
    "section": "The cell image data, revisited",
    "text": "The cell image data, revisited\nIn our previous Evaluate your model with resampling article, we introduced a data set of images of cells that were labeled by experts as well-segmented (WS) or poorly segmented (PS). We trained a random forest model to predict which images are segmented well vs. poorly, so that a biologist could filter out poorly segmented cell images in their analysis. We used resampling to estimate the performance of our model on this data.\n\ndata(cells, package = \"modeldata\")\ncells\n#> # A tibble: 2,019 × 58\n#>    case  class angle_c…¹ area_…² avg_i…³ avg_i…⁴ avg_i…⁵ avg_i…⁶ conve…⁷ conve…⁸\n#>    <fct> <fct>     <dbl>   <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#>  1 Test  PS       143.       185    15.7    4.95    9.55    2.21    1.12   0.920\n#>  2 Train PS       134.       819    31.9  207.     69.9   164.      1.26   0.797\n#>  3 Train WS       107.       431    28.0  116.     63.9   107.      1.05   0.935\n#>  4 Train PS        69.2      298    19.5  102.     28.2    31.0     1.20   0.866\n#>  5 Test  PS         2.89     285    24.3  112.     20.5    40.6     1.11   0.957\n#>  6 Test  WS        40.7      172   326.   654.    129.    347.      1.01   0.993\n#>  7 Test  WS       174.       177   260.   596.    124.    273.      1.01   0.984\n#>  8 Test  PS       180.       251    18.3    5.73   17.2     1.55    1.20   0.831\n#>  9 Test  WS        18.9      495    16.1   89.5    13.7    51.4     1.19   0.822\n#> 10 Test  WS       153.       384    17.7   89.9    20.4    63.1     1.16   0.865\n#> # … with 2,009 more rows, 48 more variables: diff_inten_density_ch_1 <dbl>,\n#> #   diff_inten_density_ch_3 <dbl>, diff_inten_density_ch_4 <dbl>,\n#> #   entropy_inten_ch_1 <dbl>, entropy_inten_ch_3 <dbl>,\n#> #   entropy_inten_ch_4 <dbl>, eq_circ_diam_ch_1 <dbl>,\n#> #   eq_ellipse_lwr_ch_1 <dbl>, eq_ellipse_oblate_vol_ch_1 <dbl>,\n#> #   eq_ellipse_prolate_vol_ch_1 <dbl>, eq_sphere_area_ch_1 <dbl>,\n#> #   eq_sphere_vol_ch_1 <dbl>, fiber_align_2_ch_3 <dbl>, …"
  },
  {
    "objectID": "content/start/tuning/index.html#why-tune",
    "href": "content/start/tuning/index.html#why-tune",
    "title": "Tune model parameters",
    "section": "Predicting image segmentation, but better",
    "text": "Predicting image segmentation, but better\nRandom forest models are a tree-based ensemble method, and typically perform well with default hyperparameters. However, the accuracy of some other tree-based models, such as boosted tree models or decision tree models, can be sensitive to the values of hyperparameters. In this article, we will train a decision tree model. There are several hyperparameters for decision tree models that can be tuned for better performance. Let’s explore:\n\nthe complexity parameter (which we call cost_complexity in tidymodels) for the tree, and\nthe maximum tree_depth.\n\nTuning these hyperparameters can improve model performance because decision tree models are prone to overfitting. This happens because single tree models tend to fit the training data too well — so well, in fact, that they over-learn patterns present in the training data that end up being detrimental when predicting new data.\nWe will tune the model hyperparameters to avoid overfitting. Tuning the value of cost_complexity helps by pruning back our tree. It adds a cost, or penalty, to error rates of more complex trees; a cost closer to zero decreases the number tree nodes pruned and is more likely to result in an overfit tree. However, a high cost increases the number of tree nodes pruned and can result in the opposite problem—an underfit tree. Tuning tree_depth, on the other hand, helps by stopping our tree from growing after it reaches a certain depth. We want to tune these hyperparameters to find what those two values should be for our model to do the best job predicting image segmentation.\nBefore we start the tuning process, we split our data into training and testing sets, just like when we trained the model with one default set of hyperparameters. As before, we can use strata = class if we want our training and testing sets to be created using stratified sampling so that both have the same proportion of both kinds of segmentation.\n\nset.seed(123)\ncell_split <- initial_split(cells %>% select(-case), \n                            strata = class)\ncell_train <- training(cell_split)\ncell_test  <- testing(cell_split)\n\nWe use the training data for tuning the model."
  },
  {
    "objectID": "content/start/tuning/index.html#tuning",
    "href": "content/start/tuning/index.html#tuning",
    "title": "Tune model parameters",
    "section": "Tuning hyperparameters",
    "text": "Tuning hyperparameters\nLet’s start with the parsnip package, using a decision_tree() model with the rpart engine. To tune the decision tree hyperparameters cost_complexity and tree_depth, we create a model specification that identifies which hyperparameters we plan to tune.\n\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n\ntune_spec\n#> Decision Tree Model Specification (classification)\n#> \n#> Main Arguments:\n#>   cost_complexity = tune()\n#>   tree_depth = tune()\n#> \n#> Computational engine: rpart\n\nThink of tune() here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will tune().\nWe can’t train this specification on a single data set (such as the entire training set) and learn what the hyperparameter values should be, but we can train many models using resampled data and see which models turn out best. We can create a regular grid of values to try using some convenience functions for each hyperparameter:\n\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\nThe function grid_regular() is from the dials package. It chooses sensible values to try for each hyperparameter; here, we asked for 5 of each. Since we have two to tune, grid_regular() returns 5 \\(\\times\\) 5 = 25 different possible tuning combinations to try in a tidy tibble format.\n\ntree_grid\n#> # A tibble: 25 × 2\n#>    cost_complexity tree_depth\n#>              <dbl>      <int>\n#>  1    0.0000000001          1\n#>  2    0.0000000178          1\n#>  3    0.00000316            1\n#>  4    0.000562              1\n#>  5    0.1                   1\n#>  6    0.0000000001          4\n#>  7    0.0000000178          4\n#>  8    0.00000316            4\n#>  9    0.000562              4\n#> 10    0.1                   4\n#> # … with 15 more rows\n\nHere, you can see all 5 values of cost_complexity ranging up to 0.1. These values get repeated for each of the 5 values of tree_depth:\n\ntree_grid %>% \n  count(tree_depth)\n#> # A tibble: 5 × 2\n#>   tree_depth     n\n#>        <int> <int>\n#> 1          1     5\n#> 2          4     5\n#> 3          8     5\n#> 4         11     5\n#> 5         15     5\n\nArmed with our grid filled with 25 candidate decision tree models, let’s create cross-validation folds for tuning:\n\nset.seed(234)\ncell_folds <- vfold_cv(cell_train)\n\nTuning in tidymodels requires a resampled object created with the rsample package."
  },
  {
    "objectID": "content/start/tuning/index.html#tune-grid",
    "href": "content/start/tuning/index.html#tune-grid",
    "title": "Tune model parameters",
    "section": "Model tuning with a grid",
    "text": "Model tuning with a grid\nWe are ready to tune! Let’s use tune_grid() to fit models at all the different values we chose for each tuned hyperparameter. There are several options for building the object for tuning:\n\nTune a model specification along with a recipe or model, or\nTune a workflow() that bundles together a model specification and a recipe or model preprocessor.\n\nHere we use a workflow() with a straightforward formula; if this model required more involved data preprocessing, we could use add_recipe() instead of add_formula().\n\nset.seed(345)\n\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_formula(class ~ .)\n\ntree_res <- \n  tree_wf %>% \n  tune_grid(\n    resamples = cell_folds,\n    grid = tree_grid\n    )\n\ntree_res\n#> # Tuning results\n#> # 10-fold cross-validation \n#> # A tibble: 10 × 4\n#>    splits             id     .metrics          .notes          \n#>    <list>             <chr>  <list>            <list>          \n#>  1 <split [1362/152]> Fold01 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  2 <split [1362/152]> Fold02 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  3 <split [1362/152]> Fold03 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  4 <split [1362/152]> Fold04 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  5 <split [1363/151]> Fold05 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  6 <split [1363/151]> Fold06 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  7 <split [1363/151]> Fold07 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  8 <split [1363/151]> Fold08 <tibble [50 × 6]> <tibble [0 × 3]>\n#>  9 <split [1363/151]> Fold09 <tibble [50 × 6]> <tibble [0 × 3]>\n#> 10 <split [1363/151]> Fold10 <tibble [50 × 6]> <tibble [0 × 3]>\n\nOnce we have our tuning results, we can both explore them through visualization and then select the best result. The function collect_metrics() gives us a tidy tibble with all the results. We had 25 candidate models and two metrics, accuracy and roc_auc, and we get a row for each .metric and model.\n\ntree_res %>% \n  collect_metrics()\n#> # A tibble: 50 × 8\n#>    cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n#>              <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>     \n#>  1    0.0000000001          1 accuracy binary     0.732    10  0.0148 Preproces…\n#>  2    0.0000000001          1 roc_auc  binary     0.777    10  0.0107 Preproces…\n#>  3    0.0000000178          1 accuracy binary     0.732    10  0.0148 Preproces…\n#>  4    0.0000000178          1 roc_auc  binary     0.777    10  0.0107 Preproces…\n#>  5    0.00000316            1 accuracy binary     0.732    10  0.0148 Preproces…\n#>  6    0.00000316            1 roc_auc  binary     0.777    10  0.0107 Preproces…\n#>  7    0.000562              1 accuracy binary     0.732    10  0.0148 Preproces…\n#>  8    0.000562              1 roc_auc  binary     0.777    10  0.0107 Preproces…\n#>  9    0.1                   1 accuracy binary     0.732    10  0.0148 Preproces…\n#> 10    0.1                   1 roc_auc  binary     0.777    10  0.0107 Preproces…\n#> # … with 40 more rows\n\nWe might get more out of plotting these results:\n\ntree_res %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nWe can see that our “stubbiest” tree, with a depth of 1, is the worst model according to both metrics and across all candidate values of cost_complexity. Our deepest tree, with a depth of 15, did better. However, the best tree seems to be between these values with a tree depth of 4. The show_best() function shows us the top 5 candidate models by default:\n\ntree_res %>%\n  show_best(\"accuracy\")\n#> # A tibble: 5 × 8\n#>   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n#>             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>      \n#> 1    0.0000000001          4 accuracy binary     0.807    10  0.0119 Preprocess…\n#> 2    0.0000000178          4 accuracy binary     0.807    10  0.0119 Preprocess…\n#> 3    0.00000316            4 accuracy binary     0.807    10  0.0119 Preprocess…\n#> 4    0.000562              4 accuracy binary     0.807    10  0.0119 Preprocess…\n#> 5    0.1                   4 accuracy binary     0.786    10  0.0124 Preprocess…\n\nWe can also use the select_best() function to pull out the single set of hyperparameter values for our best decision tree model:\n\nbest_tree <- tree_res %>%\n  select_best(\"accuracy\")\n\nbest_tree\n#> # A tibble: 1 × 3\n#>   cost_complexity tree_depth .config              \n#>             <dbl>      <int> <chr>                \n#> 1    0.0000000001          4 Preprocessor1_Model06\n\nThese are the values for tree_depth and cost_complexity that maximize accuracy in this data set of cell images."
  },
  {
    "objectID": "content/start/tuning/index.html#final-model",
    "href": "content/start/tuning/index.html#final-model",
    "title": "Tune model parameters",
    "section": "Finalizing our model",
    "text": "Finalizing our model\nWe can update (or “finalize”) our workflow object tree_wf with the values from select_best().\n\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\n\nfinal_wf\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> class ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Decision Tree Model Specification (classification)\n#> \n#> Main Arguments:\n#>   cost_complexity = 1e-10\n#>   tree_depth = 4\n#> \n#> Computational engine: rpart\n\nOur tuning is done!\n\nThe last fit\nFinally, let’s fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function last_fit() with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.\n\nfinal_fit <- \n  final_wf %>%\n  last_fit(cell_split) \n\nfinal_fit %>%\n  collect_metrics()\n#> # A tibble: 2 × 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.802 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.840 Preprocessor1_Model1\n\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_curve(class, .pred_PS) %>% \n  autoplot()\n\n\n\n\n\n\n\n\nThe performance metrics from the test set indicate that we did not overfit during our tuning procedure.\nThe final_fit object contains a finalized, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using one of the extract_ helper functions.\n\nfinal_tree <- extract_workflow(final_fit)\nfinal_tree\n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> class ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> n= 1514 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 1514 539 PS (0.64398943 0.35601057)  \n#>    2) total_inten_ch_2< 41732.5 642  33 PS (0.94859813 0.05140187)  \n#>      4) shape_p_2_a_ch_1>=1.251801 631  27 PS (0.95721078 0.04278922) *\n#>      5) shape_p_2_a_ch_1< 1.251801 11   5 WS (0.45454545 0.54545455) *\n#>    3) total_inten_ch_2>=41732.5 872 366 WS (0.41972477 0.58027523)  \n#>      6) fiber_width_ch_1< 11.37318 406 160 PS (0.60591133 0.39408867)  \n#>       12) avg_inten_ch_1< 145.4883 293  85 PS (0.70989761 0.29010239) *\n#>       13) avg_inten_ch_1>=145.4883 113  38 WS (0.33628319 0.66371681)  \n#>         26) total_inten_ch_3>=57919.5 33  10 PS (0.69696970 0.30303030) *\n#>         27) total_inten_ch_3< 57919.5 80  15 WS (0.18750000 0.81250000) *\n#>      7) fiber_width_ch_1>=11.37318 466 120 WS (0.25751073 0.74248927)  \n#>       14) eq_ellipse_oblate_vol_ch_1>=1673.942 30   8 PS (0.73333333 0.26666667)  \n#>         28) var_inten_ch_3>=41.10858 20   2 PS (0.90000000 0.10000000) *\n#>         29) var_inten_ch_3< 41.10858 10   4 WS (0.40000000 0.60000000) *\n#>       15) eq_ellipse_oblate_vol_ch_1< 1673.942 436  98 WS (0.22477064 0.77522936) *\n\nWe can create a visualization of the decision tree using another helper function to extract the underlying engine-specific fit.\n\nfinal_tree %>%\n  extract_fit_engine() %>%\n  rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\nPerhaps we would also like to understand what variables are important in this final model. We can use the vip package to estimate variable importance based on the model’s structure.\n\nlibrary(vip)\n\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n\n\n\n\n\n\n\n\nThese are the automated image analysis measurements that are the most important in driving segmentation quality predictions.\nWe leave it to the reader to explore whether you can tune a different decision tree hyperparameter. You can explore the reference docs, or use the args() function to see which parsnip object arguments are available:\n\nargs(decision_tree)\n#> function (mode = \"unknown\", engine = \"rpart\", cost_complexity = NULL, \n#>     tree_depth = NULL, min_n = NULL) \n#> NULL\n\nYou could tune the other hyperparameter we didn’t use here, min_n, which sets the minimum n to split at any node. This is another early stopping method for decision trees that can help prevent overfitting. Use this searchable table to find the original argument for min_n in the rpart package (hint). See whether you can tune a different combination of hyperparameters and/or values to improve a tree’s ability to predict cell segmentation quality."
  },
  {
    "objectID": "content/start/tuning/index.html#session-information",
    "href": "content/start/tuning/index.html#session-information",
    "title": "Tune model parameters",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Monterey 12.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-05\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.4      2022-12-02 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 1.0.0      2022-12-20 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3      2022-11-09 [1] CRAN (R 4.2.0)\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rpart      * 4.1.19     2022-10-21 [1] CRAN (R 4.2.0)\n#>  rpart.plot * 3.1.1      2022-05-21 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1      2022-12-07 [1] CRAN (R 4.2.0)\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-09 [1] Github (tidymodels/tune@e23abdf)\n#>  vip        * 0.3.2      2020-12-17 [1] CRAN (R 4.2.0)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-12-27 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/start/case-study/index.html",
    "href": "content/start/case-study/index.html",
    "title": "A predictive modeling case study",
    "section": "",
    "text": "Each of the four previous Get Started articles has focused on a single task related to modeling. Along the way, we also introduced core packages in the tidymodels ecosystem and some of the key functions you’ll need to start working with models. In this final case study, we will use all of the previous articles as a foundation to build a predictive model from beginning to end with data on hotel stays.\n\n\n\n\n\n\n\n\n\nTo use code in this article, you will need to install the following packages: glmnet, ranger, readr, tidymodels, and vip.\n\nlibrary(tidymodels)  \n\n# Helper packages\nlibrary(readr)       # for importing data\nlibrary(vip)         # for variable importance plots"
  },
  {
    "objectID": "content/start/case-study/index.html#data",
    "href": "content/start/case-study/index.html#data",
    "title": "A predictive modeling case study",
    "section": "The Hotel Bookings Data",
    "text": "The Hotel Bookings Data\nLet’s use hotel bookings data from Antonio, Almeida, and Nunes (2019) to predict which hotel stays included children and/or babies, based on the other characteristics of the stays such as which hotel the guests stay at, how much they pay, etc. This was also a #TidyTuesday dataset with a data dictionary you may want to look over to learn more about the variables. We’ll use a slightly edited version of the dataset for this case study.\nTo start, let’s read our hotel data into R, which we’ll do by providing readr::read_csv() with a url where our CSV data is located (“https://tidymodels.org/start/case-study/hotels.csv”):\n\nlibrary(tidymodels)\nlibrary(readr)\n\nhotels <- \n  read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>%\n  mutate(across(where(is.character), as.factor))\n\ndim(hotels)\n#> [1] 50000    23\n\nIn the original paper, the authors caution that the distribution of many variables (such as number of adults/children, room type, meals bought, country of origin of the guests, and so forth) is different for hotel stays that were canceled versus not canceled. This makes sense because much of that information is gathered (or gathered again more accurately) when guests check in for their stay, so canceled bookings are likely to have more missing data than non-canceled bookings, and/or to have different characteristics when data is not missing. Given this, it is unlikely that we can reliably detect meaningful differences between guests who cancel their bookings and those who do not with this dataset. To build our models here, we have already filtered the data to include only the bookings that did not cancel, so we’ll be analyzing hotel stays only.\n\nglimpse(hotels)\n#> Rows: 50,000\n#> Columns: 23\n#> $ hotel                          <fct> City_Hotel, City_Hotel, Resort_Hotel, R…\n#> $ lead_time                      <dbl> 217, 2, 95, 143, 136, 67, 47, 56, 80, 6…\n#> $ stays_in_weekend_nights        <dbl> 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, …\n#> $ stays_in_week_nights           <dbl> 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, …\n#> $ adults                         <dbl> 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, …\n#> $ children                       <fct> none, none, none, none, none, none, chi…\n#> $ meal                           <fct> BB, BB, BB, HB, HB, SC, BB, BB, BB, BB,…\n#> $ country                        <fct> DEU, PRT, GBR, ROU, PRT, GBR, ESP, ESP,…\n#> $ market_segment                 <fct> Offline_TA/TO, Direct, Online_TA, Onlin…\n#> $ distribution_channel           <fct> TA/TO, Direct, TA/TO, TA/TO, Direct, TA…\n#> $ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ reserved_room_type             <fct> A, D, A, A, F, A, C, B, D, A, A, D, A, …\n#> $ assigned_room_type             <fct> A, K, A, A, F, A, C, A, D, A, D, D, A, …\n#> $ booking_changes                <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ deposit_type                   <fct> No_Deposit, No_Deposit, No_Deposit, No_…\n#> $ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ customer_type                  <fct> Transient-Party, Transient, Transient, …\n#> $ average_daily_rate             <dbl> 80.75, 170.00, 8.00, 81.00, 157.60, 49.…\n#> $ required_car_parking_spaces    <fct> none, none, none, none, none, none, non…\n#> $ total_of_special_requests      <dbl> 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, …\n#> $ arrival_date                   <date> 2016-09-01, 2017-08-25, 2016-11-19, 20…\n\nWe will build a model to predict which actual hotel stays included children and/or babies, and which did not. Our outcome variable children is a factor variable with two levels:\n\nhotels %>% \n  count(children) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   children     n   prop\n#>   <fct>    <int>  <dbl>\n#> 1 children  4038 0.0808\n#> 2 none     45962 0.919\n\nWe can see that children were only in 8.1% of the reservations. This type of class imbalance can often wreak havoc on an analysis. While there are several methods for combating this issue using recipes (search for steps to upsample or downsample) or other more specialized packages like themis, the analyses shown below analyze the data as-is."
  },
  {
    "objectID": "content/start/case-study/index.html#data-split",
    "href": "content/start/case-study/index.html#data-split",
    "title": "A predictive modeling case study",
    "section": "Data Splitting & Resampling",
    "text": "Data Splitting & Resampling\nFor a data splitting strategy, let’s reserve 25% of the stays to the test set. As in our Evaluate your model with resampling article, we know our outcome variable children is pretty imbalanced so we’ll use a stratified random sample:\n\nset.seed(123)\nsplits      <- initial_split(hotels, strata = children)\n\nhotel_other <- training(splits)\nhotel_test  <- testing(splits)\n\n# training set proportions by children\nhotel_other %>% \n  count(children) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   children     n   prop\n#>   <fct>    <int>  <dbl>\n#> 1 children  3027 0.0807\n#> 2 none     34473 0.919\n\n# test set proportions by children\nhotel_test  %>% \n  count(children) %>% \n  mutate(prop = n/sum(n))\n#> # A tibble: 2 × 3\n#>   children     n   prop\n#>   <fct>    <int>  <dbl>\n#> 1 children  1011 0.0809\n#> 2 none     11489 0.919\n\nIn our articles so far, we’ve relied on 10-fold cross-validation as the primary resampling method using rsample::vfold_cv(). This has created 10 different resamples of the training set (which we further split into analysis and assessment sets), producing 10 different performance metrics that we then aggregated.\nFor this case study, rather than using multiple iterations of resampling, let’s create a single resample called a validation set. In tidymodels, a validation set is treated as a single iteration of resampling. This will be a split from the 37,500 stays that were not used for testing, which we called hotel_other. This split creates two new datasets:\n\nthe set held out for the purpose of measuring performance, called the validation set, and\nthe remaining data used to fit the model, called the training set.\n\n\n\n\n\n\n\n\n\n\nWe’ll use the validation_split() function to allocate 20% of the hotel_other stays to the validation set and 30,000 stays to the training set. This means that our model performance metrics will be computed on a single set of 7,500 hotel stays. This is fairly large, so the amount of data should provide enough precision to be a reliable indicator for how well each model predicts the outcome with a single iteration of resampling.\n\nset.seed(234)\nval_set <- validation_split(hotel_other, \n                            strata = children, \n                            prop = 0.80)\nval_set\n#> # Validation Set Split (0.8/0.2)  using stratification \n#> # A tibble: 1 × 2\n#>   splits               id        \n#>   <list>               <chr>     \n#> 1 <split [30000/7500]> validation\n\nThis function, like initial_split(), has the same strata argument, which uses stratified sampling to create the resample. This means that we’ll have roughly the same proportions of hotel stays with and without children in our new validation and training sets, as compared to the original hotel_other proportions."
  },
  {
    "objectID": "content/start/case-study/index.html#first-model",
    "href": "content/start/case-study/index.html#first-model",
    "title": "A predictive modeling case study",
    "section": "A first model: penalized logistic regression",
    "text": "A first model: penalized logistic regression\nSince our outcome variable children is categorical, logistic regression would be a good first model to start. Let’s use a model that can perform feature selection during training. The glmnet R package fits a generalized linear model via penalized maximum likelihood. This method of estimating the logistic regression slope parameters uses a penalty on the process so that less relevant predictors are driven towards a value of zero. One of the glmnet penalization methods, called the lasso method, can actually set the predictor slopes to zero if a large enough penalty is used.\n\nBuild the model\nTo specify a penalized logistic regression model that uses a feature selection penalty, let’s use the parsnip package with the glmnet engine:\n\nlr_mod <- \n  logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\nWe’ll set the penalty argument to tune() as a placeholder for now. This is a model hyperparameter that we will tune to find the best value for making predictions with our data. Setting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.\n\n\nCreate the recipe\nLet’s create a recipe to define the preprocessing steps we need to prepare our hotel stays data for this model. It might make sense to create a set of date-based predictors that reflect important components related to the arrival date. We have already introduced a number of useful recipe steps for creating features from dates:\n\nstep_date() creates predictors for the year, month, and day of the week.\nstep_holiday() generates a set of indicator variables for specific holidays. Although we don’t know where these two hotels are located, we do know that the countries for origin for most stays are based in Europe.\nstep_rm() removes variables; here we’ll use it to remove the original date variable since we no longer want it in the model.\n\nAdditionally, all categorical predictors (e.g., distribution_channel, hotel, …) should be converted to dummy variables, and all numeric predictors need to be centered and scaled.\n\nstep_dummy() converts characters or factors (i.e., nominal variables) into one or more numeric binary model terms for the levels of the original data.\nstep_zv() removes indicator variables that only contain a single unique value (e.g. all zeros). This is important because, for penalized models, the predictors should be centered and scaled.\nstep_normalize() centers and scales numeric variables.\n\nPutting all these steps together into a recipe for a penalized logistic regression model, we have:\n\nholidays <- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\", \n              \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n\nlr_recipe <- \n  recipe(children ~ ., data = hotel_other) %>% \n  step_date(arrival_date) %>% \n  step_holiday(arrival_date, holidays = holidays) %>% \n  step_rm(arrival_date) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n\n\n\nCreate the workflow\nAs we introduced in Preprocess your data with recipes, let’s bundle the model and recipe into a single workflow() object to make management of the R objects easier:\n\nlr_workflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(lr_recipe)\n\n\n\nCreate the grid for tuning\nBefore we fit this model, we need to set up a grid of penalty values to tune. In our Tune model parameters article, we used dials::grid_regular() to create an expanded grid based on a combination of two hyperparameters. Since we have only one hyperparameter to tune here, we can set the grid up manually using a one-column tibble with 30 candidate values:\n\nlr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n\nlr_reg_grid %>% top_n(-5) # lowest penalty values\n#> Selecting by penalty\n#> # A tibble: 5 × 1\n#>    penalty\n#>      <dbl>\n#> 1 0.0001  \n#> 2 0.000127\n#> 3 0.000161\n#> 4 0.000204\n#> 5 0.000259\nlr_reg_grid %>% top_n(5)  # highest penalty values\n#> Selecting by penalty\n#> # A tibble: 5 × 1\n#>   penalty\n#>     <dbl>\n#> 1  0.0386\n#> 2  0.0489\n#> 3  0.0621\n#> 4  0.0788\n#> 5  0.1\n\n\n\nTrain and tune the model\nLet’s use tune::tune_grid() to train these 30 penalized logistic regression models. We’ll also save the validation set predictions (via the call to control_grid()) so that diagnostic information can be available after the model fit. The area under the ROC curve will be used to quantify how well the model performs across a continuum of event thresholds (recall that the event rate—the proportion of stays including children— is very low for these data).\n\nlr_res <- \n  lr_workflow %>% \n  tune_grid(val_set,\n            grid = lr_reg_grid,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))\n\nIt might be easier to visualize the validation set metrics by plotting the area under the ROC curve against the range of penalty values:\n\nlr_plot <- \n  lr_res %>% \n  collect_metrics() %>% \n  ggplot(aes(x = penalty, y = mean)) + \n  geom_point() + \n  geom_line() + \n  ylab(\"Area under the ROC Curve\") +\n  scale_x_log10(labels = scales::label_number())\n\nlr_plot \n\n\n\n\n\n\n\n\nThis plots shows us that model performance is generally better at the smaller penalty values. This suggests that the majority of the predictors are important to the model. We also see a steep drop in the area under the ROC curve towards the highest penalty values. This happens because a large enough penalty will remove all predictors from the model, and not surprisingly predictive accuracy plummets with no predictors in the model (recall that an ROC AUC value of 0.50 means that the model does no better than chance at predicting the correct class).\nOur model performance seems to plateau at the smaller penalty values, so going by the roc_auc metric alone could lead us to multiple options for the “best” value for this hyperparameter:\n\ntop_models <-\n  lr_res %>% \n  show_best(\"roc_auc\", n = 15) %>% \n  arrange(penalty) \ntop_models\n#> # A tibble: 15 × 7\n#>     penalty .metric .estimator  mean     n std_err .config              \n#>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1 0.000127 roc_auc binary     0.872     1      NA Preprocessor1_Model02\n#>  2 0.000161 roc_auc binary     0.872     1      NA Preprocessor1_Model03\n#>  3 0.000204 roc_auc binary     0.873     1      NA Preprocessor1_Model04\n#>  4 0.000259 roc_auc binary     0.873     1      NA Preprocessor1_Model05\n#>  5 0.000329 roc_auc binary     0.874     1      NA Preprocessor1_Model06\n#>  6 0.000418 roc_auc binary     0.874     1      NA Preprocessor1_Model07\n#>  7 0.000530 roc_auc binary     0.875     1      NA Preprocessor1_Model08\n#>  8 0.000672 roc_auc binary     0.875     1      NA Preprocessor1_Model09\n#>  9 0.000853 roc_auc binary     0.876     1      NA Preprocessor1_Model10\n#> 10 0.00108  roc_auc binary     0.876     1      NA Preprocessor1_Model11\n#> 11 0.00137  roc_auc binary     0.876     1      NA Preprocessor1_Model12\n#> 12 0.00174  roc_auc binary     0.876     1      NA Preprocessor1_Model13\n#> 13 0.00221  roc_auc binary     0.876     1      NA Preprocessor1_Model14\n#> 14 0.00281  roc_auc binary     0.875     1      NA Preprocessor1_Model15\n#> 15 0.00356  roc_auc binary     0.873     1      NA Preprocessor1_Model16\n\n\n\n\nEvery candidate model in this tibble likely includes more predictor variables than the model in the row below it. If we used select_best(), it would return candidate model 11 with a penalty value of 0.00137, shown with the dotted line below.\n\n\n\n\n\n\n\n\n\nHowever, we may want to choose a penalty value further along the x-axis, closer to where we start to see the decline in model performance. For example, candidate model 12 with a penalty value of 0.00174 has effectively the same performance as the numerically best model, but might eliminate more predictors. This penalty value is marked by the solid line above. In general, fewer irrelevant predictors is better. If performance is about the same, we’d prefer to choose a higher penalty value.\nLet’s select this value and visualize the validation set ROC curve:\n\nlr_best <- \n  lr_res %>% \n  collect_metrics() %>% \n  arrange(penalty) %>% \n  slice(12)\nlr_best\n#> # A tibble: 1 × 7\n#>   penalty .metric .estimator  mean     n std_err .config              \n#>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 0.00137 roc_auc binary     0.876     1      NA Preprocessor1_Model12\n\n\nlr_auc <- \n  lr_res %>% \n  collect_predictions(parameters = lr_best) %>% \n  roc_curve(children, .pred_children) %>% \n  mutate(model = \"Logistic Regression\")\n\nautoplot(lr_auc)\n\n\n\n\n\n\n\n\nThe level of performance generated by this logistic regression model is good, but not groundbreaking. Perhaps the linear nature of the prediction equation is too limiting for this data set. As a next step, we might consider a highly non-linear model generated using a tree-based ensemble method."
  },
  {
    "objectID": "content/start/case-study/index.html#second-model",
    "href": "content/start/case-study/index.html#second-model",
    "title": "A predictive modeling case study",
    "section": "A second model: tree-based ensemble",
    "text": "A second model: tree-based ensemble\nAn effective and low-maintenance modeling technique is a random forest. This model was also used in our Evaluate your model with resampling article. Compared to logistic regression, a random forest model is more flexible. A random forest is an ensemble model typically made up of thousands of decision trees, where each individual tree sees a slightly different version of the training data and learns a sequence of splitting rules to predict new data. Each tree is non-linear, and aggregating across trees makes random forests also non-linear but more robust and stable compared to individual trees. Tree-based models like random forests require very little preprocessing and can effectively handle many types of predictors (sparse, skewed, continuous, categorical, etc.).\n\nBuild the model and improve training time\nAlthough the default hyperparameters for random forests tend to give reasonable results, we’ll plan to tune two hyperparameters that we think could improve performance. Unfortunately, random forest models can be computationally expensive to train and to tune. The computations required for model tuning can usually be easily parallelized to improve training time. The tune package can do parallel processing for you, and allows users to use multiple cores or separate machines to fit models.\nBut, here we are using a single validation set, so parallelization isn’t an option using the tune package. For this specific case study, a good alternative is provided by the engine itself. The ranger package offers a built-in way to compute individual random forest models in parallel. To do this, we need to know the the number of cores we have to work with. We can use the parallel package to query the number of cores on your own computer to understand how much parallelization you can do:\n\ncores <- parallel::detectCores()\ncores\n#> [1] 20\n\nWe have 20 cores to work with. We can pass this information to the ranger engine when we set up our parsnip rand_forest() model. To enable parallel processing, we can pass engine-specific arguments like num.threads to ranger when we set the engine:\n\nrf_mod <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"classification\")\n\nThis works well in this modeling context, but it bears repeating: if you use any other resampling method, let tune do the parallel processing for you — we typically do not recommend relying on the modeling engine (like we did here) to do this.\nIn this model, we used tune() as a placeholder for the mtry and min_n argument values, because these are our two hyperparameters that we will tune.\n\n\nCreate the recipe and workflow\nUnlike penalized logistic regression models, random forest models do not require dummy or normalized predictor variables. Nevertheless, we want to do some feature engineering again with our arrival_date variable. As before, the date predictor is engineered so that the random forest model does not need to work hard to tease these potential patterns from the data.\n\nrf_recipe <- \n  recipe(children ~ ., data = hotel_other) %>% \n  step_date(arrival_date) %>% \n  step_holiday(arrival_date) %>% \n  step_rm(arrival_date) \n\nAdding this recipe to our parsnip model gives us a new workflow for predicting whether a hotel stay included children and/or babies as guests with a random forest:\n\nrf_workflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(rf_recipe)\n\n\n\nTrain and tune the model\nWhen we set up our parsnip model, we chose two hyperparameters for tuning:\n\nrf_mod\n#> Random Forest Model Specification (classification)\n#> \n#> Main Arguments:\n#>   mtry = tune()\n#>   trees = 1000\n#>   min_n = tune()\n#> \n#> Engine-Specific Arguments:\n#>   num.threads = cores\n#> \n#> Computational engine: ranger\n\n# show what will be tuned\nextract_parameter_set_dials(rf_mod)\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier  type    object\n#>        mtry  mtry nparam[?]\n#>       min_n min_n nparam[+]\n#> \n#> Model parameters needing finalization:\n#>    # Randomly Selected Predictors ('mtry')\n#> \n#> See `?dials::finalize` or `?dials::update.parameters` for more information.\n\nThe mtry hyperparameter sets the number of predictor variables that each node in the decision tree “sees” and can learn about, so it can range from 1 to the total number of features present; when mtry = all possible features, the model is the same as bagging decision trees. The min_n hyperparameter sets the minimum n to split at any node.\nWe will use a space-filling design to tune, with 25 candidate models:\n\nset.seed(345)\nrf_res <- \n  rf_workflow %>% \n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))\n#> i Creating pre-processing data to finalize unknown parameter: mtry\n\nThe message printed above “Creating pre-processing data to finalize unknown parameter: mtry” is related to the size of the data set. Since mtry depends on the number of predictors in the data set, tune_grid() determines the upper bound for mtry once it receives the data.\nHere are our top 5 random forest models, out of the 25 candidates:\n\nrf_res %>% \n  show_best(metric = \"roc_auc\")\n#> # A tibble: 5 × 8\n#>    mtry min_n .metric .estimator  mean     n std_err .config              \n#>   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1     8     7 roc_auc binary     0.926     1      NA Preprocessor1_Model13\n#> 2    12     7 roc_auc binary     0.926     1      NA Preprocessor1_Model01\n#> 3    13     4 roc_auc binary     0.925     1      NA Preprocessor1_Model05\n#> 4     9    12 roc_auc binary     0.924     1      NA Preprocessor1_Model19\n#> 5     6    18 roc_auc binary     0.924     1      NA Preprocessor1_Model24\n\nRight away, we see that these values for area under the ROC look more promising than our top model using penalized logistic regression, which yielded an ROC AUC of 0.876.\nPlotting the results of the tuning process highlights that both mtry (number of predictors at each node) and min_n (minimum number of data points required to keep splitting) should be fairly small to optimize performance. However, the range of the y-axis indicates that the model is very robust to the choice of these parameter values — all but one of the ROC AUC values are greater than 0.90.\n\nautoplot(rf_res)\n\n\n\n\n\n\n\n\nLet’s select the best model according to the ROC AUC metric. Our final tuning parameter values are:\n\nrf_best <- \n  rf_res %>% \n  select_best(metric = \"roc_auc\")\nrf_best\n#> # A tibble: 1 × 3\n#>    mtry min_n .config              \n#>   <int> <int> <chr>                \n#> 1     8     7 Preprocessor1_Model13\n\nTo calculate the data needed to plot the ROC curve, we use collect_predictions(). This is only possible after tuning with control_grid(save_pred = TRUE). In the output, you can see the two columns that hold our class probabilities for predicting hotel stays including and not including children.\n\nrf_res %>% \n  collect_predictions()\n#> # A tibble: 187,500 × 8\n#>    id         .pred_children .pred_none  .row  mtry min_n children .config      \n#>    <chr>               <dbl>      <dbl> <int> <int> <int> <fct>    <chr>        \n#>  1 validation        0.152        0.848    13    12     7 none     Preprocessor…\n#>  2 validation        0.0302       0.970    20    12     7 none     Preprocessor…\n#>  3 validation        0.513        0.487    22    12     7 children Preprocessor…\n#>  4 validation        0.0103       0.990    23    12     7 none     Preprocessor…\n#>  5 validation        0.0111       0.989    31    12     7 none     Preprocessor…\n#>  6 validation        0            1        38    12     7 none     Preprocessor…\n#>  7 validation        0            1        39    12     7 none     Preprocessor…\n#>  8 validation        0.00325      0.997    50    12     7 none     Preprocessor…\n#>  9 validation        0.0241       0.976    54    12     7 none     Preprocessor…\n#> 10 validation        0.0441       0.956    57    12     7 children Preprocessor…\n#> # … with 187,490 more rows\n\nTo filter the predictions for only our best random forest model, we can use the parameters argument and pass it our tibble with the best hyperparameter values from tuning, which we called rf_best:\n\nrf_auc <- \n  rf_res %>% \n  collect_predictions(parameters = rf_best) %>% \n  roc_curve(children, .pred_children) %>% \n  mutate(model = \"Random Forest\")\n\nNow, we can compare the validation set ROC curves for our top penalized logistic regression model and random forest model:\n\nbind_rows(rf_auc, lr_auc) %>% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nThe random forest is uniformly better across event probability thresholds."
  },
  {
    "objectID": "content/start/case-study/index.html#last-fit",
    "href": "content/start/case-study/index.html#last-fit",
    "title": "A predictive modeling case study",
    "section": "The last fit",
    "text": "The last fit\nOur goal was to predict which hotel stays included children and/or babies. The random forest model clearly performed better than the penalized logistic regression model, and would be our best bet for predicting hotel stays with and without children. After selecting our best model and hyperparameter values, our last step is to fit the final model on all the rows of data not originally held out for testing (both the training and the validation sets combined), and then evaluate the model performance one last time with the held-out test set.\nWe’ll start by building our parsnip model object again from scratch. We take our best hyperparameter values from our random forest model. When we set the engine, we add a new argument: importance = \"impurity\". This will provide variable importance scores for this last model, which gives some insight into which predictors drive model performance.\n\n# the last model\nlast_rf_mod <- \n  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") %>% \n  set_mode(\"classification\")\n\n# the last workflow\nlast_rf_workflow <- \n  rf_workflow %>% \n  update_model(last_rf_mod)\n\n# the last fit\nset.seed(345)\nlast_rf_fit <- \n  last_rf_workflow %>% \n  last_fit(splits)\n\nlast_rf_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 × 6\n#>   splits                id               .metrics .notes   .predict…¹ .workflow \n#>   <list>                <chr>            <list>   <list>   <list>     <list>    \n#> 1 <split [37500/12500]> train/test split <tibble> <tibble> <tibble>   <workflow>\n#> # … with abbreviated variable name ¹​.predictions\n\nThis fitted workflow contains everything, including our final metrics based on the test set. So, how did this model do on the test set? Was the validation set a good estimate of future performance?\n\nlast_rf_fit %>% \n  collect_metrics()\n#> # A tibble: 2 × 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.946 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.923 Preprocessor1_Model1\n\nThis ROC AUC value is pretty close to what we saw when we tuned the random forest model with the validation set, which is good news. That means that our estimate of how well our model would perform with new data was not too far off from how well our model actually performed with the unseen test data.\nWe can access those variable importance scores via the .workflow column. We can extract out the fit from the workflow object, and then use the vip package to visualize the variable importance scores for the top 20 features:\n\nlast_rf_fit %>% \n  extract_fit_parsnip() %>% \n  vip(num_features = 20)\n\n\n\n\n\n\n\n\nThe most important predictors in whether a hotel stay had children or not were the daily cost for the room, the type of room reserved, the time between the creation of the reservation and the arrival date, and the type of room that was ultimately assigned.\nLet’s generate our last ROC curve to visualize. Since the event we are predicting is the first level in the children factor (“children”), we provide roc_curve() with the relevant class probability .pred_children:\n\nlast_rf_fit %>% \n  collect_predictions() %>% \n  roc_curve(children, .pred_children) %>% \n  autoplot()\n\n\n\n\n\n\n\n\nBased on these results, the validation set and test set performance statistics are very close, so we would have pretty high confidence that our random forest model with the selected hyperparameters would perform well when predicting new data."
  },
  {
    "objectID": "content/start/case-study/index.html#next",
    "href": "content/start/case-study/index.html#next",
    "title": "A predictive modeling case study",
    "section": "Where to next?",
    "text": "Where to next?\nIf you’ve made it to the end of this series of Get Started articles, we hope you feel ready to learn more! You now know the core tidymodels packages and how they fit together. After you are comfortable with the basics we introduced in this series, you can learn how to go farther with tidymodels in your modeling and machine learning projects.\nHere are some more ideas for where to go next:\n\nStudy up on statistics and modeling with our comprehensive books.\nDig deeper into the package documentation sites to find functions that meet your modeling needs. Use the searchable tables to explore what is possible.\nKeep up with the latest about tidymodels packages at the tidyverse blog.\nFind ways to ask for help and contribute to tidymodels to help others.\n\n###\n\nHappy modeling!"
  },
  {
    "objectID": "content/start/case-study/index.html#session-information",
    "href": "content/start/case-study/index.html#session-information",
    "title": "A predictive modeling case study",
    "section": "Session information",
    "text": "Session information\n\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-04\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  glmnet     * 4.1-4      2022-04-15 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  ranger     * 0.14.1     2022-06-18 [1] CRAN (R 4.2.0)\n#>  readr      * 2.1.3      2022-10-01 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  vip        * 0.3.2      2020-12-17 [1] CRAN (R 4.2.0)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/books/smltar/index.html",
    "href": "content/books/smltar/index.html",
    "title": "Supervised Machine Learning for Text Analysis in R",
    "section": "",
    "text": "This book explains how to preprocess text data for modeling, train models, and evaluate model performance using tools from the tidyverse and tidymodels ecosystem. Models like these can be used to make predictions for new observations, to understand what natural language features or characteristics contribute to differences in the output, and more. If you are already familiar with the basics of predictive modeling, use the comprehensive, detailed examples in this book to extend your skills to the domain of natural language processing.\nThis book provides practical guidance and directly applicable knowledge for data scientists and analysts who want to integrate unstructured text data into their modeling pipelines. Learn how to use text data for both regression and classification tasks, and how to apply more straightforward algorithms like regularized regression or support vector machines as well as deep learning approaches. Natural language must be dramatically transformed to be ready for computation, so we explore typical text preprocessing and feature engineering steps like tokenization and word embeddings from the ground up. These steps influence model results in ways we can measure, both in terms of model metrics and other tangible consequences such as how fair or appropriate model results are.\nAn HTML version of this text can be found at https://smltar.com/.\nThe data sets and R code are available in the GitHub repository https://github.com/EmilHvitfeldt/smltar.\nThe physical copies are sold by Amazon and CRC Press."
  },
  {
    "objectID": "content/books/fes/index.html",
    "href": "content/books/fes/index.html",
    "title": "Feature Engineering & Selection",
    "section": "",
    "text": "A primary goal of predictive modeling is to find a reliable and effective predictive relationship between an available set of features and an outcome. This book provides an extensive set of techniques for uncovering effective representations of the features for modeling the outcome and for finding an optimal subset of features to improve a model’s predictive performance.\nAn HTML version of this text can be found at https://bookdown.org/max/FES.\nThe data sets and R code are available in the GitHub repository https://github.com/topepo/FES.\nThe physical copies are sold by Amazon and Taylor & Francis."
  },
  {
    "objectID": "content/books/moderndive/index.html",
    "href": "content/books/moderndive/index.html",
    "title": "Statistical Inference via Data Science",
    "section": "",
    "text": "This book is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would. Over the course of this book, you will develop your “data science toolbox,” equipping yourself with tools such as data visualization, data formatting, data wrangling, data modeling using regression, and statistical inference via hypothesis testing and confidence intervals.\nAn HTML version of this text can be found at https://moderndive.com/.\nThe data sets and R code are available in the GitHub repository https://github.com/moderndive/ModernDive_book and also the accompanying moderndive R package on CRAN.\nThe physical copies are sold by Amazon and CRC Press."
  },
  {
    "objectID": "content/books/tmwr/index.html",
    "href": "content/books/tmwr/index.html",
    "title": "Tidy Modeling with R",
    "section": "",
    "text": "Modeling of data is integral to science, business, politics, and many other aspects of our lives. The goals of this book are to:\n\nintroduce and demonstrate how to use the tidymodels packages, and\noutline good practices for the phases of the modeling process.\n\nAn HTML version of this text can be found at https://tmwr.org.\nThe sources to create the book are available in the GitHub repository https://github.com/tidymodels/TMwR."
  },
  {
    "objectID": "content/books/tidytext/index.html",
    "href": "content/books/tidytext/index.html",
    "title": "Text Mining with R",
    "section": "",
    "text": "This practical book provides an introduction to text mining using tidy data principles in R, focusing on exploratory data analysis for text. Using tidy data principles can make text mining task easier and more effective; in this book, learn how to manipulate, summarize, and visualize characteristics of text using these methods and R packages from the tidy tool ecosystem.\nAn HTML version of this text can be found at https://www.tidytextmining.com/.\nThe data sets and R code are available in the GitHub repository https://github.com/dgrtwo/tidy-text-mining.\nThe physical copies are sold by Amazon and O’Reilly."
  },
  {
    "objectID": "content/contribute/index.html",
    "href": "content/contribute/index.html",
    "title": "How to contribute to tidymodels",
    "section": "",
    "text": "The ecosystem of tidymodels packages would not be possible without the contributions of the R community. No matter your current skills, it’s possible to contribute back to tidymodels. Contributions are guided by our design goals."
  },
  {
    "objectID": "content/contribute/index.html#design-goals",
    "href": "content/contribute/index.html#design-goals",
    "title": "How to contribute to tidymodels",
    "section": "Design goals",
    "text": "Design goals\nThe goals of tidymodels packages are to:\n\nEncourage empirical validation and good statistical practice.\nSmooth out heterogeneous interfaces.\nEstablish highly reusable infrastructure.\nEnable a wider variety of methodologies.\nHelp package developers quickly build high quality model packages of their own.\n\nThese goals are guided by our principles for creating modeling packages.\nWhat are different ways you can contribute?"
  },
  {
    "objectID": "content/contribute/index.html#answer-questions",
    "href": "content/contribute/index.html#answer-questions",
    "title": "How to contribute to tidymodels",
    "section": "Answer questions",
    "text": "Answer questions\nYou can help others use and learn tidymodels by answering questions on the RStudio community site, Stack Overflow, and Twitter. Many people asking for help with tidymodels don’t know what a reprex is or how to craft one. Acknowledging an individual’s problem, showing them how to build a reprex, and pointing them to helpful resources are all enormously beneficial, even if you don’t immediately solve their problem.\nRemember that while you might have seen a problem a hundred times before, it’s new to the person asking it. Be patient, polite, and empathic."
  },
  {
    "objectID": "content/contribute/index.html#file-issues",
    "href": "content/contribute/index.html#file-issues",
    "title": "How to contribute to tidymodels",
    "section": "File issues",
    "text": "File issues\nIf you’ve found a bug, first create a minimal reprex. Spend some time working to make it as minimal as possible; the more time you spend doing this, the easier it is to fix the bug. When your reprex is ready, file it on the GitHub repo of the appropriate package.\nThe tidymodels team often focuses on one package at a time to reduce context switching and be more efficient. We may not address each issue right away, but we will use the reprex you create to understand your problem when it is time to focus on that package."
  },
  {
    "objectID": "content/contribute/index.html#contribute-documentation",
    "href": "content/contribute/index.html#contribute-documentation",
    "title": "How to contribute to tidymodels",
    "section": "Contribute documentation",
    "text": "Contribute documentation\nDocumentation is a high priority for tidymodels, and pull requests to correct or improve documentation are welcome. The most important thing to know is that tidymodels packages use roxygen2; this means that documentation is found in the R code close to the source of each function. There are some special tags, but most tidymodels packages now use markdown in the documentation. This makes it particularly easy to get started!"
  },
  {
    "objectID": "content/contribute/index.html#contribute-code",
    "href": "content/contribute/index.html#contribute-code",
    "title": "How to contribute to tidymodels",
    "section": "Contribute code",
    "text": "Contribute code\nIf you are a more experienced R programmer, you may have the inclination, interest, and ability to contribute directly to package development. Before you submit a pull request on a tidymodels package, always file an issue and confirm the tidymodels team agrees with your idea and is happy with your basic proposal.\nIn tidymodels packages, we use the tidyverse style guide which will make sure that your new code and documentation matches the existing style. This makes the review process much smoother.\nThe tidymodels packages are explicitly built to support the creation of other modeling packages, and we would love to hear about what you build yourself! Check out our learning resources for developing custom modeling tools."
  },
  {
    "objectID": "content/help/index.html",
    "href": "content/help/index.html",
    "title": "Get Help",
    "section": "",
    "text": "If you’re asking for R help, reporting a bug, or requesting a new feature, you’re more likely to succeed if you include a good reproducible example, which is precisely what the reprex package is built for. You can learn more about reprex, along with other tips on how to help others help you in the tidyverse.org help section."
  },
  {
    "objectID": "content/help/index.html#where-to-ask",
    "href": "content/help/index.html#where-to-ask",
    "title": "Get Help",
    "section": "Where to ask",
    "text": "Where to ask\n\nNow that you’ve made a reprex, you need to share it in an appropriate forum. Here are some options:\n\ncommunity.rstudio.com: This is a warm and welcoming place to ask any questions you might have about tidymodels or more generally about modeling, machine learning, and deep learning. (You can also ask questions about the tidyverse and RStudio there, too!)\nStack Overflow. You’re probably already familiar with Stack Overflow from googling; it’s a frequent source of answers to coding related questions. Asking a question on Stack Overflow can be intimidating, but if you’ve taken the time to create a reprex, you’re much more likely to get a useful answer. Make sure to tag your question with r and tidymodels so that the right people are more likely to see it.\nTwitter and Mastodon. These sites are great places to share a link to your reprex that’s hosted elsewhere! The #rstats twitter and #rstats fosstodon communities are extremely friendly and active, and have great crowds to be a part of. Make sure you tag your tweet with #rstats and #tidymodels.\nIf you think you’ve found a bug, please follow the instructions for filing an issue on contributing to tidymodels."
  },
  {
    "objectID": "content/help/index.html#resources",
    "href": "content/help/index.html#resources",
    "title": "Get Help",
    "section": "Resources",
    "text": "Resources\n\nSee what you need to know to get started with tidymodels, and learn more about using tidymodels for specific tasks.\nEach tidymodels package has its own documentation site, full of helpful information. Find links to all package documentation sites and explore them!\nSearch all tidymodels functions, and check out our books on these topics.\nStay up to date with the latest news about tidymodels through our posts on the tidyverse blog."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tidymodels",
    "section": "",
    "text": "The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\nInstall tidymodels with:\ninstall.packages(\"tidymodels\")\nWhether you are just starting out today or have years of experience with modeling, tidymodels offers a consistent, flexible framework for your work."
  },
  {
    "objectID": "content/find/all/index.html",
    "href": "content/find/all/index.html",
    "title": "Search all of tidymodels",
    "section": "",
    "text": "Here are all the functions available across all of the tidymodels packages. Click on the link in the topic column to find the relevant reference documentation."
  },
  {
    "objectID": "content/find/parsnip/index.html",
    "href": "content/find/parsnip/index.html",
    "title": "Search parsnip models",
    "section": "",
    "text": "To learn about the parsnip package, see Get Started: Build a Model. Use the tables below to find model types and engines and to explore model arguments."
  },
  {
    "objectID": "content/find/parsnip/index.html#models",
    "href": "content/find/parsnip/index.html#models",
    "title": "Search parsnip models",
    "section": "Explore models",
    "text": "Explore models\n\n\n\n\n\n\nModels can be added by the user too. The article How to build a parsnip model walks you through the steps."
  },
  {
    "objectID": "content/find/parsnip/index.html#model-args",
    "href": "content/find/parsnip/index.html#model-args",
    "title": "Search parsnip models",
    "section": "Explore model arguments",
    "text": "Explore model arguments\nThe parsnip package provides consistent interface for working with similar models across different engines. This means that parsnip adopts standardized parameter names as arguments, and those names may be different from those used by the individual engines. The searchable table below provides a mapping between the parsnip and the engine arguments:"
  },
  {
    "objectID": "content/find/index.html",
    "href": "content/find/index.html",
    "title": "Explore tidymodels",
    "section": "",
    "text": "Below you’ll find searchable tables to help you explore the tidymodels packages and functions. The tables also include links to the relevant reference page to help you navigate the package documentation. Use the following categories to guide you:\n\nSearch all of tidymodels\nSearch parsnip models\nSearch recipe steps\nSearch broom methods"
  },
  {
    "objectID": "content/find/recipes/index.html",
    "href": "content/find/recipes/index.html",
    "title": "Search recipe steps",
    "section": "",
    "text": "To learn about the recipes package, see Get Started: Preprocess your data with recipes. The table below allows you to search for recipe steps across tidymodels packages."
  },
  {
    "objectID": "content/learn/develop/broom/index.html#glossaries",
    "href": "content/learn/develop/broom/index.html#glossaries",
    "title": "Create your own broom tidier methods",
    "section": "Glossaries",
    "text": "Glossaries\n\nArguments\nTidier methods have a standardized set of acceptable argument and output column names. The currently acceptable argument names by tidier method are:\n\n\n\n\n\n\n\n\n\nColumn Names\nThe currently acceptable column names by tidier method are:\n\n\n\n\n\n\n\nThe alexpghayes/modeltests package provides unit testing infrastructure to check your new tidier methods. Please file an issue there to request new arguments/columns to be added to the glossaries!"
  },
  {
    "objectID": "content/find/broom/index.html",
    "href": "content/find/broom/index.html",
    "title": "Search broom methods",
    "section": "",
    "text": "Here are all the broom functions available across CRAN packages. Click on the link in the topic column to find more information."
  }
]