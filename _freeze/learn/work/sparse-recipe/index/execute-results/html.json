{
  "hash": "6d7ff76b7be6b2ecc6ba0b549be7f6ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model tuning using a recipe\"\ncategories:\n - tuning\n - classification\n - sparse data\ntype: learn-subsection\nweight: 1\ndescription: | \n Fitting a model using tidymodels where sparse data is created using a recipe.\ntoc: true\ntoc-depth: 2\ninclude-after-body: ../../../resources.html\n---\n\n\n\n\n\n\n\n\n## Introduction\n\nTo use code in this article,  you will need to install the following packages: nycflights13 and tidymodels.\n\n\nThis article demonstrates how we can use a recipe to generate data sparsity in tidymodels.\n\nWe use the term **sparse data** to denote a data set that contains a lot of 0s. Such data is commonly seen as a result of dealing with categorical variables, text tokenization, or graph data sets. The word sparse describes how the information is packed. Namely, it represents the presence of a lot of zeroes. For some tasks, we can easily get above 99% percent of 0s in the predictors. \n\nThe reason we use sparse data as a construct is that it is a lot more memory efficient to store the positions and values of the non-zero entries than to encode all the values. One could think of this as a compression, but one that is done such that data tasks are still fast. The following vector requires 25 values to store it normally (dense representation). This representation will be referred to as a **dense vector**.\n\n```r\nc(100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n```\nThe sparse representation of this vector only requires 5 values. 1 value for the length (25), 2 values for the locations of the non-zero values (1, 22), and 2 values for the non-zero values (100, 1). This idea can also be extended to matrices as is done in the Matrix package.\n\nNot all modeling tasks can handle sparsity, we have a [list of compatible](../../../find/sparse/index.qmd) steps you can use to guide the recipe creation.\n\n## The data\n\nWe will be using the [nycflights13](https://nycflights13.tidyverse.org/) data set for this demonstration. We are using this data specifically because it contains a number of categorical with a lot of levels, that when converted to binary indicator columns (a.k.a. \"dummy variables\") will create a lot of sparse columns.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(nycflights13)\n\nglimpse(flights)\n#> Rows: 336,776\n#> Columns: 19\n#> $ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n#> $ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n#> $ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n#> $ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n#> $ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n#> $ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n#> $ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n#> $ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n#> $ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n#> $ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n#> $ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n#> $ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n#> $ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n#> $ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n#> $ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n#> $ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n#> $ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n```\n:::\n\n\n\n\nOur modeling objective is to fit a model that predicts the arrival delay, using a regression model. We could just as well have done a classification model on \"Will plane land on time?,\" but using the regression model we can hopefully be able to quantify how early or late the plane will be.\n\nWe are furthermore assuming that this prediction will take place at departure time. This means we have to exclude some variables as they contain information that is not yet available.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nflights <- flights |>\n  select(-arr_time, -air_time)\n```\n:::\n\n\n\n\nThis data set contains a number of redundant variables. We don't need to know the departure time `dep_time`, scheduled departure time `sched_dep_time`, and the departure delay `dep_delay` as they are a linear combination of each other `dep_delay = dep_time - sched_dep_time`. So we can remove one of them and choose to get rid of `sched_dep_time`.\n\nLikewise, the `time_hour` variable is a datetime that contains data also located in `year`, `month`, `day`, `hour`, and `minute`. We will thus also remove that one.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nflights <- flights |>\n  select(-time_hour, -sched_dep_time)\n```\n:::\n\n\n\n\nYou may or may not have noticed that `dep_time` and `sched_arr_time` have a weird encoding. What is happening is that `517` is actually `5:17` e.i. 17 minutes past 5 AM. So we need to update that, which we will use a little helper function for.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_minutes <- function(x) {\n minutes <- x %% 100\n hours <- x %/% 100\n\n hours * 60 + minutes\n}\n\nflights <- flights |>\n  mutate(across(c(dep_time, sched_arr_time), get_minutes))\n```\n:::\n\n\n\n\nWe will fit a model using the first month of the year, and then try to assess how well it will generalize over the remaining years. We will also exclude any observations where `arr_delay` is `NA`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nflights <- drop_na(flights, arr_delay)\nflights_train <- filter(flights, month == 1)\nflights_test <- filter(flights, month != 1)\n```\n:::\n\n\n\n\n## Creating a recipe\n\nThe data is quite simple in terms of types. We have numeric variables and categorical variables. We will do some simple imputation of the numeric variables and create dummy variables on the categorical predictors.\n\nWe'll use a recipe to preprocess the data. If you have never seen a recipe, see Chapter 8 of [_Tidy Models with R_](https://www.tmwr.org/recipes). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrec_spec <- recipe(arr_delay ~ ., data = flights_train) |>\n  step_impute_mean(all_numeric_predictors()) |>\n  step_zv(all_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_novel(all_nominal_predictors()) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\nrec_spec\n#> \n#> ── Recipe ────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 14\n#> \n#> ── Operations\n#> • Mean imputation for: all_numeric_predictors()\n#> • Zero variance filter on: all_predictors()\n#> • Centering and scaling for: all_numeric_predictors()\n#> • Novel factor level assignment for: all_nominal_predictors()\n#> • Unknown factor level assignment for: all_nominal_predictors()\n#> • Dummy variables from: all_nominal_predictors()\n```\n:::\n\n\n\n\nYou will notice we aren't doing anything special here to denote the recipe that will acknowledge or produce sparsity. Next, we will go into some details to explain how you, the user, should approach recipes when you suspect that sparsity will be produced.\n\n## How is sparsity handled in recipes\n\nThere have been made two types of changes to recipes steps regarding sparsity. \n\nFirst, some steps can augment the data set with many columns that are naturally sparse. Creating binary indicators from a factor predictor, via `step_dummy()`, is a good example. Because of this, a number of steps have gained a `sparse` argument, which toggles the creation of sparse vectors. \n\nThe second change is that a number of steps are now able to take sparse vectors as input and _preserve sparsity_. You can see a full [list of these steps](../../../find/sparse/index.qmd) at the link.\n\nMost of the changes with regard to sparsity are done to minimize the changes the user needs to make to their code. This means that in many cases you don't need to change anything, the steps will know when to produce sparse data or not.\n\nWhen a recipe is used in a workflow and it is being `fit()`, an internal check is being done to figure out whether or not to produce sparse features. This check looks at the sparsity of the data itself, what model is being used, and the recipe. Since [only some models](../../../find/sparse/index.qmd) support sparsity this is the first check. \n\nA rough estimate of the sparsity of the data that will come out of the recipe is calculated. This is done using the input data set, and the steps present in the recipe. But since this check has to happen before the recipe is prepped, it will be quite simple. What this means in practice is that it is fairly good at estimating the sparsity that is produced by sparsity-generating steps, but it isn't able to detect if those variables are passed to a different step that doesn't preserve it.\n\nThe following recipe would be accurately estimated as the dummies produced by `step_dummy()` aren't passed to any other steps.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(outcome ~ ., data = data_train) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n\n\nBut the next recipe would have the same sparsity estimate despite not being able to produce any sparsity since `step_normalize()` can't preserve the sparsity as it subtracts a constant value.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(outcome ~ ., data = data_train) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n\nIf you were able to modify the above recipe to use `step_scale()` instead of `step_normalize()` then the estimate is still valid as `step_scale()` is a sparsity-preserving step.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(outcome ~ ., data = data_train) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_scale(all_numeric_predictors())\n```\n:::\n\n\n\n\nIs it for this reason the steps that produce sparsity have the `sparse` argument. It defaults to `\"auto\"`, which means that the estimating process in workflows decides whether or not sparsity should be created. This argument can take two other values `\"yes\"` and `\"no\"`. If you know for certain that the recipe should or shouldn't produce sparsity you can overwrite with this argument.\n\nThis means the recipe below wouldn't try to initially produce sparse vectors since they will immediately be turned into dense vectors by the next step.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(outcome ~ ., data = data_train) |>\n  step_dummy(all_nominal_predictors(), sparse = \"no\") |>\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n\nA lot of time went into trying to make `sparse = \"auto\"` work as well as possible, but since nothing is perfect you have the ability to overwrite.\n\n## Modeling\n\nWe will finish the workflow using a model/engine combination that supports sparse data.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod_spec <- boost_tree() |>\n  set_mode(\"regression\") |>\n  set_engine(\"xgboost\")\n\nmod_spec\n#> Boosted Tree Model Specification (regression)\n#> \n#> Computational engine: xgboost\n```\n:::\n\n\n\n\nThen combine it in a workflow.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwf_spec <- workflow(rec_spec, mod_spec)\n```\n:::\n\n\n\n\nAnd fit it like we usually do.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwf_fit <- fit(wf_spec, flights_train)\n```\n:::\n\n\n\n\nNow that the model has been fit we can calculate the RMSE to see how well the model has performed.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain_preds <- augment(wf_fit, flights_train)\n\nrmse(train_preds, arr_delay, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        13.9\n```\n:::\n\n\n\n\nWe can also take a visual look at the performance by plotting the predicted values against the real values.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain_preds |>\n  ggplot(aes(arr_delay, .pred)) +\n  geom_point(alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![](figs/unnamed-chunk-17-1.svg){fig-align='center' fig-alt='Scatter chart. Arrival delay along the x-axis and predictions along the \ny-axis. The majority of the points are along the diagonal, with a shift\ndown.' width=672}\n:::\n:::\n\n\n\n\nThe model appears to work fairly well on the training data set. We notice the shift down, which would suggest that the model has a bias towards underestimating the delay.\n\nNow we will see how well the model performs in the remaining months.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_preds <- augment(wf_fit, flights_test)\n\nrmse(test_preds, arr_delay, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        18.9\n```\n:::\n\n\n\n\nAnd they see that the performance is quite a bit worse. Let's see how the performance goes on a month-by-month basis.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_preds |>\n  group_by(month) |>\n  rmse(arr_delay, .pred) |>\n  ggplot(aes(month, .estimate)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](figs/unnamed-chunk-19-1.svg){fig-align='center' fig-alt='Scatter chart. Month along the x-axis, estimate of RSME along the y-axis.\nStarting on the second month with a value around 17, it goes up for each\nmonth to 23 in July, afterward it does back down to 16 in September, \nwith November having the same value and December having a value of 20.' width=672}\n:::\n:::\n\n\n\n\nWe see the same result that the model doesn't generalize to the other months. This should not be that surprising as the model was only fit in January. Furthermore, it appears that there is a seasonal trend happening, further showing us that fitting this model in January alone was not the best idea.\n\n## Session information {#session-info}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.4.2 (2024-10-31)\n#>  os       macOS Sequoia 15.3.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/Los_Angeles\n#>  date     2025-03-14\n#>  pandoc   3.6.1 @ /usr/local/bin/ (via rmarkdown)\n#>  quarto   1.6.42 @ /Applications/quarto/bin/quarto\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  broom        * 1.0.7      2024-09-26 [1] CRAN (R 4.4.1)\n#>  dials        * 1.4.0      2025-02-13 [1] CRAN (R 4.4.2)\n#>  dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.4.0)\n#>  ggplot2      * 3.5.1      2024-04-23 [1] CRAN (R 4.4.0)\n#>  infer        * 1.0.7      2024-03-25 [1] CRAN (R 4.4.0)\n#>  nycflights13 * 1.0.2      2021-04-12 [1] CRAN (R 4.4.0)\n#>  parsnip      * 1.3.1      2025-03-12 [1] CRAN (R 4.4.1)\n#>  purrr        * 1.0.4      2025-02-05 [1] CRAN (R 4.4.1)\n#>  recipes      * 1.1.1.9000 2025-03-10 [1] local\n#>  rlang          1.1.5      2025-01-17 [1] CRAN (R 4.4.2)\n#>  rsample      * 1.2.1      2024-03-25 [1] CRAN (R 4.4.0)\n#>  tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.4.0)\n#>  tidymodels   * 1.3.0      2025-02-21 [1] CRAN (R 4.4.1)\n#>  tune         * 1.3.0      2025-02-21 [1] CRAN (R 4.4.1)\n#>  workflows    * 1.2.0      2025-02-19 [1] CRAN (R 4.4.1)\n#>  yardstick    * 1.3.2      2025-01-22 [1] CRAN (R 4.4.1)\n#> \n#>  [1] /Users/emilhvitfeldt/Library/R/arm64/4.4/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n#>  * ── Packages attached to the search path.\n#> \n#> ────────────────────────────────────────────────────────────────────\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}