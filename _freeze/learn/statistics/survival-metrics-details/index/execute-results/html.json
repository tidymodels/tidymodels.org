{
  "hash": "279851b6d43f7aff69f90b0ec697d0bc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Accounting for Censoring in Performance Metrics for Event Time Data\"\ncategories:\n  - statistical analysis\n  - survival analysis\ntype: learn-subsection\nweight: 9\ndescription: | \n  Learn how tidymodels uses causal inference tools to measure performance of \n  survival models.\ntoc: true\ntoc-depth: 2\ninclude-after-body: ../../../resources.html\n---\n\n\n\n\n\n\n\n\n## Introduction\n\nTo use code in this article,  you will need to install the following packages: censored, prodlim, and tidymodels.\n\nOne trend in modern survival analysis is to compute time-dependent measures of performance. These are primarily driven by an increased focus on predictions for the probability of survival at a given time (as opposed to the predictions of event times or linear predictors). Since these are conditional on the time of evaluation, we call them dynamic performance metrics.\n\nMany dynamic metrics are similar to those used for binary classification models. Examples include the Brier score and ROC curves (see the [Dynamic Performance Metrics for Event Time Data](../survival-metrics/) article for details). The basic idea is that, for a given time $t$ for model evaluation, we try to encode the observed event time data into a binary \"has there been an event at time $t$?\" version. We also convert the predicted survival probabilities into predicted events/non-events based on a threshold (default is 0.50). The survival versions of these metrics need those binary versions of observed truth and predictions as well as a way to account for censoring.\n\nCensoring plays into the details of the conversion and is additionally captured in the form of weights. This article covers both those aspects in detail to complement the [main article](../survival-metrics/) on performance metrics for event time data.\n\nTo start, let's define the various types of times that will be mentioned:\n\n- Observed time: time recorded in the data\n- Event time: observed times for actual events\n- Evaluation time: the time, specified by the analyst, that the model is evaluated. \n\n## Example data\n\nAs an example, we'll simulate some data with the prodlim package, using the methods of [Bender _et al_ (2005)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Generating+survival+times+to+simulate+Cox+proportional+hazards+models.%22&btnG=). A training and a validation set are simulated. We'll also load the censored package so that we can fit a model to these time-to-event data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(censored)\n#> Loading required package: survival\nlibrary(prodlim)\n\nset.seed(5882)\nsim_dat <- SimSurv(2000) %>%\n  mutate(event_time = Surv(time, event)) %>%\n  select(event_time, X1, X2)\n\nset.seed(2)\nsplit   <- initial_split(sim_dat)\nsim_tr  <- training(split)\nsim_val <- testing(split)\n```\n:::\n\n\n\n\nWe'll need a model to illustrate the code and concepts. Let's fit a bagged survival tree model to the training set:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(17)\nbag_tree_fit <- \n  bag_tree() %>% \n  set_mode(\"censored regression\") %>% \n  set_engine(\"rpart\") %>% \n  fit(event_time ~ ., data = sim_tr)\nbag_tree_fit\n#> parsnip model object\n#> \n#> \n#> Bagging survival trees with 25 bootstrap replications \n#> \n#> Call: bagging.data.frame(formula = event_time ~ ., data = data)\n```\n:::\n\n\n\n\nUsing this model, we can make predictions of different types and `augment()` provides us with a version of the data augmented with the various predictions. Here we are interested in the predicted probability of survival at different evaluation time points. The largest event time in the training set is 21.083 so we will use a set of evaluation times between zero and 21. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntime_points <- seq(0, 21, by = 0.25)\n\nval_pred <- augment(bag_tree_fit, sim_val, eval_time = time_points)\nval_pred\n#> # A tibble: 500 × 5\n#>    .pred             .pred_time event_time    X1      X2\n#>    <list>                 <dbl>     <Surv> <dbl>   <dbl>\n#>  1 <tibble [85 × 5]>       6.66  4.831957      1 -0.630 \n#>  2 <tibble [85 × 5]>       6.66  6.110031      1 -0.606 \n#>  3 <tibble [85 × 5]>       7.47  6.597774+     1 -1.03  \n#>  4 <tibble [85 × 5]>       3.29  2.717484      1  0.811 \n#>  5 <tibble [85 × 5]>       5.10  4.727042+     1 -0.376 \n#>  6 <tibble [85 × 5]>       4.99  8.699061      0  1.18  \n#>  7 <tibble [85 × 5]>       7.23 10.818670      1 -0.851 \n#>  8 <tibble [85 × 5]>       6.46  6.886378      0  0.493 \n#>  9 <tibble [85 × 5]>       4.75  2.451893+     1  0.0207\n#> 10 <tibble [85 × 5]>      13.4   8.231911+     0 -1.52  \n#> # ℹ 490 more rows\n```\n:::\n\n\n\n\nThe observed data are in the `event_time` column. The predicted survival probabilities are in the `.pred` column. This is a list column with a data frame for each observation, containing the predictions at the 85 evaluation time points in the (nested) column `.pred_survival`. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nval_pred$.pred[[1]]\n#> # A tibble: 85 × 5\n#>    .eval_time .pred_survival .weight_time .pred_censored .weight_censored\n#>         <dbl>          <dbl>        <dbl>          <dbl>            <dbl>\n#>  1       0             1            0              1                 1   \n#>  2       0.25          1            0.250          0.999             1.00\n#>  3       0.5           0.999        0.500          0.996             1.00\n#>  4       0.75          0.992        0.750          0.993             1.01\n#>  5       1             0.988        1.00           0.991             1.01\n#>  6       1.25          0.980        1.25           0.987             1.01\n#>  7       1.5           0.972        1.50           0.981             1.02\n#>  8       1.75          0.959        1.75           0.971             1.03\n#>  9       2             0.938        2.00           0.966             1.04\n#> 10       2.25          0.925        2.25           0.959             1.04\n#> # ℹ 75 more rows\n```\n:::\n\n\n\n\nFirst, let's dive into how to convert the observed event time in `event_time` to a binary version. Then we will discuss the remaining columns as part of generating the required weights for the dynamic performance metrics.\n\n\n## Converting censored data to binary data\n\nTo assess model performance at evaluation time $t$, we turn the observed event time data into a binary “was there an event at time $t$?” version. For this, we follow the process described by [Graf _et al_ (1999)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Assessment+and+Comparison+of+Prognostic+Classification+Schemes+for+Survival+Data.%22&btnG=) where observations are categorized into three groups, at evaluation time $t$.\n\n - **Category 1 - Events**: Evaluation time is greater than or equal to the event time (\"it has already happened\").\n - **Category 2 - Non-events**: Evaluation time is less than the observed time, censored or not (\"nothing has happened yet\"). \n - **Category 3 - Ambiguous outcomes**: Evaluation time is greater than or equal to the observed censored time (\"we don't know if anything might have happened by now\"). \n\nWe can use binary versions of the observations in the first two categories to compute binary performance metrics, but the observations in the third category are not used directly in these calculations. (They do influence the calculation of the weights, see next section.) So our usable sample size changes with the evaluation time.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/plot-graf-categories-1.svg){fig-align='center' width=864}\n:::\n:::\n\n\n\n\n## Censoring weights\n\nUnfortunately, this categorization scheme alone is not sufficient to compute metrics. Graf _et al_ took a page from the causal inference literature and added a propensity-type score based on the likelihood that each data point would be censored (regardless of the observed event status). This is not the probability than the original time-to-event data point is censored but rather the probability that at evaluation time, we have not observed an event (or a censoring) yet, i.e., that the data point falls into category 2.\n\nHow do we compute this probability? The standard approach is to compute a \"reverse Kaplan-Meier\" (RKM) curve. Ordinarily, the Kaplan-Meier (KM) curve models the probability of survival. For the reverse Kaplan-Meier curve, the meaning of the status indicator is flipped, i.e., the event of interest changes from \"death\" to \"censoring\". This should give us a reasonably reliable non-parametric model for estimating the probability of being censored at a given time. \n\nEvery time a censored regression model is created using tidymodels, the RKM is estimated on the same data used to fit the model and attached to the parsnip object. \n\nFor our simulated data, here is what the RKM curve looks like: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/RKM-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nThe red rug on the bottom shows the training point event times and the blue values at the top represent the times for the censored training set observations. As (evaluation) time increases, we pass more and more observed time points, and the probability of being censored, i.e., the probability of an observation to fall into category 2, decreases.\n\nThe weights used in the calculation of the dynamic performance metrics are the inverse of these probabilities, hence the name \"inverse probability of censoring weights\" (IPCW). These weights should theoretically balance the exposure/effect/influence that the definitive observations have on performance calculations. \n\n### The finer details\n\nFirst, when do we evaluate the probability of censoring? There are different approaches used in the literature, and we follow what Graf _et al_ suggest (as it seems most appropriate):\n\n- If the evaluation time is less than the observed time (like in category 2), the evaluation time is used to predict the probability of censoring.\n- If the evaluation is greater than or equal to the event time (like in category 1), the event time is used to predict the probability of censoring.\n- If the evaluation time is greater than or equal to the observed censoring time, the observation falls into category 3 and is not used, i.e., also no weight is needed.\n\nWe call this time at which to predict the probability of censoring the _weight time_. Here's an example using the first data point in the validation set: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndyn_val_pred <- \n  val_pred %>% \n  select(.pred, event_time) %>% \n  add_rowindex() %>% \n  unnest(.pred) \n\ndyn_val_pred %>% \n  filter(.row == 1 & .eval_time %in% c(1, 2, 4, 5, 10)) %>% \n  select(event_time, .eval_time, .weight_time, .pred_censored, .weight_censored)\n#> # A tibble: 5 × 5\n#>   event_time .eval_time .weight_time .pred_censored .weight_censored\n#>       <Surv>      <dbl>        <dbl>          <dbl>            <dbl>\n#> 1   4.831957          1         1.00          0.991             1.01\n#> 2   4.831957          2         2.00          0.966             1.04\n#> 3   4.831957          4         4.00          0.848             1.18\n#> 4   4.831957          5         4.83          0.786             1.27\n#> 5   4.831957         10         4.83          0.786             1.27\n```\n:::\n\n\n\n\n\n\nThis observation was an event, observed at time 4.832 The column `.weight_time` captures at which time the probability of censoring was calculated. Up until that event time, the probability of being censored is computed at the evaluation time. After that, it is based on the event time. \n\nWe add a slight modification to the weight time: If our evaluation time is today, we don't have today's data yet. In tidymodels, we calculate the probability of censoring just before the requested weight time. We are basically subtracting a small numerical value from the weight time used in the RKM model. You'll only really see a difference if there is a bulk of censored observations at the original evaluation time.\n\nFinally, we use a simple RKM curve (i.e., no covariates or strata). This implies that there is non-informative censoring. Other applications of IPCW try to mitigate the effects of informative censoring. In the future, we may allow the censoring model to include covariates (as well as models beyond the RKM). \n\n## Illustration: Confusion matrix\n\nTo illustrate how these two tools for accounting for censoring are used in calculating dynamic performance metrics, we'll take a look here at the 2x2 confusion matrices at a few evaluation time points. More details on performance metrics for censored data are in the aforementioned [Dynamic Performance Metrics for Event Time Data](../survival-metrics/) article.\n\nFirst, let's turn the observed event time data and the predictions into their binary versions.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntime_as_binary_event <- function(surv, eval_time) {\n  event_time <- .extract_surv_time(surv)\n  status <- .extract_surv_status(surv)\n  is_event_before_t <- event_time <= eval_time & status == 1\n  \n  # Three possible contributions to the statistic from Graf 1999\n  # Censoring time before eval_time, no contribution (Graf category 3)\n  binary_res <- rep(NA_character_, length(event_time))\n  \n  # A real event prior to eval_time (Graf category 1)\n  binary_res <- if_else(is_event_before_t, \"event\", binary_res)\n  \n  # Observed time greater than eval_time (Graf category 2)\n  binary_res <- if_else(event_time > eval_time, \"non-event\", binary_res)\n  factor(binary_res, levels = c(\"event\", \"non-event\"))\n}\n\nbinary_encoding <- \n  dyn_val_pred %>% \n  mutate(\n    obs_class = time_as_binary_event(event_time, .eval_time),\n    pred_class = if_else(.pred_survival >= 1 / 2, \"non-event\", \"event\"),\n    pred_class = factor(pred_class, levels = c(\"event\", \"non-event\")),\n  )\n```\n:::\n\n\n\n\nRemember how observations falling into category 3 are removed from the analysis? This means we'll likely have fewer data points to evaluate as the evaluation time increases. This implies that the variation in the metrics will be considerable as evaluation time goes on. For our simulated training set: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndyn_val_pred %>% \n  summarize(num_usable = sum(!is.na(.weight_censored)), .by = c(.eval_time)) %>% \n  ggplot() + \n  geom_step(aes(.eval_time, num_usable)) +\n  labs(x = \"time\", y = \"number of usable observations\") +\n  lims(y = c(0, nrow(sim_val))) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](figs/usable-data-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nKeeping this in mind, let's look at what happens with the data points we can use. Let's start with an evaluation time of 1.00. To compute the confusion matrix for a classification problem, we would simply use:\n\n```r\nbinary_encoding %>% \n  filter(.eval_time == 1.00) %>% \n  conf_mat(truth = obs_class, estimate = pred_class)\n```\n\nFor censored regression problems, we need to additionally use the censoring weights so we'll include them via the `case_weights` argument:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbinary_encoding %>%\n  filter(.eval_time == 1.00) %>%\n  conf_mat(truth = obs_class,\n           estimate = pred_class,\n           case_weights = .weight_censored)\n#>            Truth\n#> Prediction      event non-event\n#>   event       0.00000   0.00000\n#>   non-event  14.11046 482.54963\n```\n:::\n\n\n\n\n\n\nThe values in the cells are the sum of the censoring weights, There are 14 actual events (out of 492 usable observations) before this evaluation time, so there are empty cells. Also note that the cell values are close to the actual counts. This early, the predicted censoring probabilities are very close to one so their inverse values are also. \n\nThis early, performance looks very good but that is mostly because there are few events.\n\nLet's shift to an evaluation time of 5.0. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbinary_encoding %>%\n  filter(.eval_time == 5.00) %>%\n  conf_mat(truth = obs_class,\n           estimate = pred_class,\n           case_weights = .weight_censored)\n#>            Truth\n#> Prediction      event non-event\n#>   event     112.98854  54.36531\n#>   non-event  56.14133 252.41039\n```\n:::\n\n\n\n\n\n\nNow we have fewer total observations to consider (391 instead of 492 usable values) and more events (154 this time). Performance is fairly good; the sensitivity is 66.8% and the specificty is 82.3%.\n\nWhat happends when the evaluation time is 17?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbinary_encoding %>%\n  filter(.eval_time == 17.00) %>%\n  conf_mat(truth = obs_class,\n           estimate = pred_class,\n           case_weights = .weight_censored)\n#>            Truth\n#> Prediction     event non-event\n#>   event     429.9920  123.4458\n#>   non-event   0.0000    0.0000\n```\n:::\n\n\n\n\n\n\nThe data are overwhelmingly events. Also, the censoring weights are larger now since the probability of being censored is very low. The mean censoring weight is 1.96.\n\nThis concludes the illustration of how to account for censoring when using a confusion matrix for performance assessment. There's more on dynamic performance metrics in the [Dynamic Performance Metrics for Event Time Data](../survival-metrics/) article.\n\n## Summary\n\nWhen accounting for censoring in dynamic performance metrics, the main points to remember are:\n\n* Event time data can be converted to a binary format.\n* Some data points cannot be used in the calculations. \n* To properly estimate statistical quantities, we weight the computations by the inverse of the probability of being censored. \n* tidymodels currently assumes non-informative censoring. \n\n\n## Session information {#session-info}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  version  R version 4.4.2 (2024-10-31)\n#>  language (EN)\n#>  date     2025-03-24\n#>  pandoc   3.6.1\n#>  quarto   1.6.42\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package      version    date (UTC) lib source\n#>  broom        1.0.7      2024-09-26 [1] CRAN (R 4.4.1)\n#>  censored     0.3.3      2025-02-14 [1] CRAN (R 4.4.1)\n#>  dials        1.4.0      2025-02-13 [1] CRAN (R 4.4.2)\n#>  dplyr        1.1.4      2023-11-17 [1] CRAN (R 4.4.0)\n#>  ggplot2      3.5.1      2024-04-23 [1] CRAN (R 4.4.0)\n#>  infer        1.0.7      2024-03-25 [1] CRAN (R 4.4.0)\n#>  parsnip      1.3.1      2025-03-12 [1] CRAN (R 4.4.1)\n#>  prodlim      2024.06.25 2024-06-24 [1] CRAN (R 4.4.0)\n#>  purrr        1.0.4      2025-02-05 [1] CRAN (R 4.4.1)\n#>  recipes      1.2.0      2025-03-17 [1] CRAN (R 4.4.1)\n#>  rlang        1.1.5      2025-01-17 [1] CRAN (R 4.4.2)\n#>  rsample      1.2.1      2024-03-25 [1] CRAN (R 4.4.0)\n#>  tibble       3.2.1      2023-03-20 [1] CRAN (R 4.4.0)\n#>  tidymodels   1.3.0      2025-02-21 [1] CRAN (R 4.4.1)\n#>  tune         1.3.0      2025-02-21 [1] CRAN (R 4.4.1)\n#>  workflows    1.2.0      2025-02-19 [1] CRAN (R 4.4.1)\n#>  yardstick    1.3.2      2025-01-22 [1] CRAN (R 4.4.1)\n#> \n#> ────────────────────────────────────────────────────────────────────\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}