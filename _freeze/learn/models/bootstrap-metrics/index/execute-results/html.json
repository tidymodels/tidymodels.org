{
  "hash": "0e1a986d2b5c127f6433a22bacca3557",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Confidence Intervals for Performance Metrics\"\ncategories:\n  - model fitting\n  - confidence intervals\n  - bootstrapping\ntype: learn-subsection\nweight: 5\ndescription: | \n  Create models that use coefficients, extract them from fitted models, and visualize them.\ntoc: true\ntoc-depth: 2\ninclude-after-body: ../../../resources.html\n---\n\n\n\n\n\n## Introduction \n\nThe tidymodels framework focuses on evaluating models via _empirical validation_: out-of-sample data are used to compute model accuracy/fitness measures. Because of this, data splitting and resampling are essential components of model development. \n\nIf a model uses traditional resampling (such as 10-fold cross-validation), it is easy to get confidence intervals (or Bayesian intervals) of performances. For example, if you are looking at classification accuracy, you can say something like, \n\n> Our accuracy was estimated to be 91.3% with a 90% confidence interval of (80.1%, 99.9%).\n\nFor some performance metrics, it is very difficult to compute interval estimates of validation sets (or test sets). \n\nThis article discusses using the bootstrap to estimate confidence intervals for performance using tidymodels. To use code in this article,  you will need to install the following packages: earth and tidymodels. We'll use data from the modeldata package. \n\n## Example Data\n\n\n\n\n\n\n\nWe'll use the [delivery time data](https://modeldata.tidymodels.org/reference/deliveries.html) and follow the analysis used on [this website](https://aml4td.org/chapters/whole-game.html#sec-delivery-times). The outcome is the time for food to be delivered, and the predictors include the day/hour of the order, the distance, and what was included in the order (columns starting with `item_`):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\nstr(deliveries)\n#> tibble [10,012 × 31] (S3: tbl_df/tbl/data.frame)\n#>  $ time_to_delivery: num [1:10012] 16.1 22.9 30.3 33.4 27.2 ...\n#>  $ hour            : num [1:10012] 11.9 19.2 18.4 15.8 19.6 ...\n#>  $ day             : Factor w/ 7 levels \"Mon\",\"Tue\",\"Wed\",..: 4 2 5 4 5 6 7 4 5 7 ...\n#>  $ distance        : num [1:10012] 3.15 3.69 2.06 5.97 2.52 3.35 2.46 2.21 2.62 2.75 ...\n#>  $ item_01         : int [1:10012] 0 0 0 0 0 1 0 0 0 0 ...\n#>  $ item_02         : int [1:10012] 0 0 0 0 0 0 0 0 0 2 ...\n#>  $ item_03         : int [1:10012] 2 0 0 0 0 0 1 1 0 1 ...\n#>  $ item_04         : int [1:10012] 0 0 0 0 1 1 1 0 0 0 ...\n#>  $ item_05         : int [1:10012] 0 0 1 0 0 0 0 0 0 0 ...\n#>  $ item_06         : int [1:10012] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ item_07         : int [1:10012] 0 0 0 0 1 1 0 0 1 0 ...\n#>  $ item_08         : int [1:10012] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ item_09         : int [1:10012] 0 0 0 0 0 1 0 0 0 0 ...\n#>  $ item_10         : int [1:10012] 1 0 0 0 0 0 0 0 0 0 ...\n#>  $ item_11         : int [1:10012] 1 0 0 0 1 1 0 0 0 0 ...\n#>  $ item_12         : int [1:10012] 0 0 0 1 0 0 0 0 0 1 ...\n#>  $ item_13         : int [1:10012] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ item_14         : int [1:10012] 0 0 0 0 1 0 0 0 0 0 ...\n#>  $ item_15         : int [1:10012] 0 0 0 0 0 0 0 0 1 0 ...\n#>  $ item_16         : int [1:10012] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ item_17         : int [1:10012] 0 0 0 0 0 0 0 0 0 1 ...\n#>  $ item_18         : int [1:10012] 0 1 0 0 0 0 0 0 0 1 ...\n#>  $ item_19         : int [1:10012] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ item_20         : int [1:10012] 0 0 1 0 0 0 0 0 0 0 ...\n#>  $ item_21         : int [1:10012] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ item_22         : int [1:10012] 0 0 0 0 0 1 1 0 0 1 ...\n#>  $ item_23         : int [1:10012] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ item_24         : int [1:10012] 0 0 1 0 0 1 0 0 0 0 ...\n#>  $ item_25         : int [1:10012] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ item_26         : int [1:10012] 0 0 0 0 0 0 0 0 1 0 ...\n#>  $ item_27         : int [1:10012] 0 0 0 1 1 0 0 0 0 0 ...\n```\n:::\n\n\n\nGiven the amount of data, a validation set was used _in lieu_ of multiple resamples. This means that we can fit models on the training set, evaluate/compare them with the validation set, and reserve the test set for a final performance assessment (after model development). The data splitting code is:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(991)\ndelivery_split <- initial_validation_split(deliveries, prop = c(0.6, 0.2), \n                                           strata = time_to_delivery)\n\n# Make data frames for each of the three data sets\ndelivery_train <- training(delivery_split)\ndelivery_test  <- testing(delivery_split)\ndelivery_val   <- validation(delivery_split)\n\n# Create an object that makes a validation set \"look like resampling\"\n# to tidymodels: \ndelivery_rs    <- validation_set(delivery_split)\n```\n:::\n\n\n\n## Tuning a Model\n\nTo demonstrate, we'll use a multivariate adaptive regression spline (MARS) model produced by the earth package. The original analysis of these data shows some significant interactions between predictors, so we will specify a model that can estimate them using the argument `prod_degree = 2`. Let's create a model specification that tunes the number of terms to retain: \n \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_spec <- \n  mars(num_terms = tune(), prod_degree = 2, prune_method = \"none\") %>% \n  set_mode(\"regression\")\n```\n:::\n\n\n\nLet's use grid search to evaluate a grid of values between 2 and 50. We'll use `tune_grid()` with an option to save the out-of-sample (i.e., validation set) predictions for each candidate model in the grid. By default, for regression models, the function computes the root mean squared error (RMSE) and R<sup>2</sup>:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid <- tibble(num_terms = 2:50)\n\nctrl <- control_grid(save_pred = TRUE)\n\nmars_res <- \n  mars_spec %>% \n  tune_grid(\n    time_to_delivery ~ .,\n    resamples = delivery_rs,\n    grid = grid,\n    control = ctrl\n  )\n```\n:::\n\n\n\nHow did the model look?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(mars_res)\n```\n\n::: {.cell-output-display}\n![](figs/perf-plot-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nAfter about 20 retained terms, both statistics appear to plateau.  However, we have no sense of the noise around these values. Is a model with 20 terms just as good as a model using 50? \n\nFor that, we'll compute confidence intervals. However, there are no analytical formulas for most performance statistics, so we need a more general method for computing them.\n\n## Bootstrap Confidence Intervals\n\nIn statistics, the bootstrap is a resampling method that takes a random sample the same size as the original but samples [with replacement](https://en.wikipedia.org/wiki/Sampling_(statistics)#Replacement_of_selected_units). This means that, in the bootstrap sample, some rows of data are represented multiple times and others not at all. \n\nThere is [some theory](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22bootstrap+confidence+intervals%22&btnG=) that shows that if we recompute a statistic a large number of times using the bootstrap, we can understand its sampling distribution and, from that, compute confidence intervals. A good recent reference on this subject (but the original inventor of the bootstrap) is [_Computer Age Statistical Inference_](https://hastie.su.domains/CASI/) (which includes a book PDF).\n\nFor our application, we'll take the validation set predictions for each candidate model in the grid, bootstrap them, and then compute confidence intervals using the percentile method. \n\nThere's a tidymodels function called `int_pctl()` for this purpose. It has a method to work with objects produced by the tune package, such as our `mars_res` object. Let's compute 90% confidence intervals using 2,000 bootstrap samples:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(140)\nmars_boot <- int_pctl(mars_res, alpha = 0.10)\nmars_boot\n#> # A tibble: 98 × 7\n#>    .metric .estimator .lower .estimate .upper .config               num_terms\n#>    <chr>   <chr>       <dbl>     <dbl>  <dbl> <chr>                     <int>\n#>  1 rmse    bootstrap  6.57      6.79   7.02   Preprocessor1_Model01         2\n#>  2 rsq     bootstrap  0.0462    0.0599 0.0746 Preprocessor1_Model01         2\n#>  3 rmse    bootstrap  4.71      4.96   5.22   Preprocessor1_Model02         3\n#>  4 rsq     bootstrap  0.468     0.497  0.527  Preprocessor1_Model02         3\n#>  5 rmse    bootstrap  3.99      4.15   4.32   Preprocessor1_Model03         4\n#>  6 rsq     bootstrap  0.624     0.649  0.674  Preprocessor1_Model03         4\n#>  7 rmse    bootstrap  3.78      3.94   4.12   Preprocessor1_Model04         5\n#>  8 rsq     bootstrap  0.657     0.683  0.706  Preprocessor1_Model04         5\n#>  9 rmse    bootstrap  3.54      3.70   3.88   Preprocessor1_Model05         6\n#> 10 rsq     bootstrap  0.699     0.721  0.741  Preprocessor1_Model05         6\n#> # ℹ 88 more rows\n```\n:::\n\n\n\nThe results have columns for the mean of the sampling distribution (`.estimate`) and the upper and lower confidence bounds (`.upper` and `.lower`, respectively). Let's visualize these results: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_boot %>% \n  ggplot(aes(num_terms)) + \n    geom_line(aes(y = .estimate)) + \n    geom_point(aes(y = .estimate), cex = 1 / 2) + \n    geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 1 / 5, fill = \"blue\") +\n    facet_wrap(~ .metric, scales = \"free_y\", ncol = 1) +\n    labs(y = NULL, x = \"# Retained Terms\")\n```\n\n::: {.cell-output-display}\n![](figs/int-plot-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nThose are very tight! Maybe there is some credence to using many terms. Let's say that 40 terms seems like a reasonable value for that tuning parameter. It's large, but the out-of-sample statistics indicate that it does not overfit at this point. \n\n## Test Set Intervals\n\nSuppose the MARS model was the best we could do for these data. We would then fit the model (with 40 terms) on the training set then finally evaluate the test set. tidymodels has an API called `last_fit()` that uses our original data splitting object (`delivery_split`) and the model specification. To get the test set predictions, we can use `collect_metrics()`: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_final_spec <- \n    mars(num_terms = 40, prod_degree = 2, prune_method = \"none\") %>% \n  set_mode(\"regression\")\n\nmars_test_res <- \n  mars_final_spec %>% \n  last_fit(time_to_delivery ~ ., split = delivery_split)\n\ncollect_metrics(mars_test_res)\n#> # A tibble: 2 × 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 rmse    standard       2.20  Preprocessor1_Model1\n#> 2 rsq     standard       0.892 Preprocessor1_Model1\n```\n:::\n\n\n\nThese values are pretty consistent with what the validation set (and its confidence intervals) produced: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_boot %>% filter(num_terms == 40)\n#> # A tibble: 2 × 7\n#>   .metric .estimator .lower .estimate .upper .config               num_terms\n#>   <chr>   <chr>       <dbl>     <dbl>  <dbl> <chr>                     <int>\n#> 1 rmse    bootstrap   2.11      2.20   2.28  Preprocessor1_Model39        40\n#> 2 rsq     bootstrap   0.894     0.902  0.909 Preprocessor1_Model39        40\n```\n:::\n\n\n\n`int_pctl()` also works on objects produced by `last_fit()`: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(168)\nmars_test_boot <- int_pctl(mars_test_res, alpha = 0.10)\nmars_test_boot\n#> # A tibble: 2 × 6\n#>   .metric .estimator .lower .estimate .upper .config             \n#>   <chr>   <chr>       <dbl>     <dbl>  <dbl> <chr>               \n#> 1 rmse    bootstrap   2.12      2.20   2.28  Preprocessor1_Model1\n#> 2 rsq     bootstrap   0.883     0.892  0.900 Preprocessor1_Model1\n```\n:::\n\n\n\n## Notes\n\n - The point estimates produced by `collect_metrics()` and `int_pctl()` may disagree. They are two different statistical estimates of the same quantity. For reasonably sized data sets, they should be pretty close. \n \n - Parallel processing can be used for bootstrapping (using the [same tools](https://www.tmwr.org/grid-search#parallel-processing) that can be used with the tune package). \n \n \n - `int_pctl()` works with classification and censored regression models. For the latter, the default is to compute intervals for every metric and every evaluation time. \n\n - `int_pctl()` has options to filter which models, metrics, and/or evaluation times are used for analysis. If you investigated many grid points, you don't have to bootstrap them all. \n \n - Since bootstrap percentiles use... percentiles, you should generate a few thousand bootstrap samples to compute the intervals to get stable and accurate results. \n \n\n## Session information {#session-info}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.2 (2023-10-31)\n#>  os       macOS Ventura 13.6.4\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2024-04-12\n#>  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version date (UTC) lib source\n#>  broom      * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n#>  dials      * 1.2.1   2024-02-22 [1] CRAN (R 4.3.1)\n#>  dplyr      * 1.1.4   2023-11-17 [1] CRAN (R 4.3.1)\n#>  earth      * 5.3.2   2023-01-26 [1] CRAN (R 4.3.0)\n#>  ggplot2    * 3.5.0   2024-02-23 [1] CRAN (R 4.3.1)\n#>  infer      * 1.0.7   2024-03-25 [1] CRAN (R 4.3.1)\n#>  parsnip    * 1.2.1   2024-03-22 [1] CRAN (R 4.3.1)\n#>  purrr      * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n#>  recipes    * 1.0.10  2024-02-18 [1] CRAN (R 4.3.1)\n#>  rlang        1.1.3   2024-01-10 [1] CRAN (R 4.3.1)\n#>  rsample    * 1.2.1   2024-03-25 [1] CRAN (R 4.3.1)\n#>  tibble     * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidymodels * 1.2.0   2024-03-25 [1] CRAN (R 4.3.1)\n#>  tune       * 1.2.0   2024-03-20 [1] CRAN (R 4.3.1)\n#>  workflows  * 1.1.4   2024-02-19 [1] CRAN (R 4.3.1)\n#>  yardstick  * 1.3.1   2024-03-21 [1] CRAN (R 4.3.1)\n#> \n#>  [1] /Users/max/Library/R/arm64/4.3/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}