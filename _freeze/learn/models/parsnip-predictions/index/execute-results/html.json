{
  "hash": "3ab2d8887c852419f30d5a7b3fc00f6a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Fitting and predicting with parsnip\"\ncategories:\n  - model fitting\n  - parsnip\n  - regression\n  - classification\ntype: learn-subsection\nweight: 1\ndescription: | \n  Examples that show how to fit and predict with different combinations of model, mode, and engine.\ntoc: true\ntoc-depth: 3\ninclude-after-body: ../../../resources.html\nexecute: \n  eval: true\n---\n\n\n\n\n\n\n## Introduction\n\nTo use code in this article,  you will need to install the following packages: agua, baguette, bonsai, censored, discrim, multilevelmod, plsmod, poissonreg, rules, sparklyr, and tidymodels.\n\nThese examples show how to *fit* and *predict* with different combinations of model, mode, and engine. As a reminder, in parsnip, \n\n- the **model type** differentiates basic modeling approaches, such as random forests, logistic regression, linear support vector machines, etc.,\n\n- the **mode** denotes in what kind of modeling context it will be used (most commonly, classification or regression), and\n\n- the computational **engine** indicates how the model is fit, such as with a specific R package implementation or even methods outside of R like Keras or Stan.\n\nThe following examples use consistent data sets throughout. \n\ntodo \n\n- multielvel examples \n- get automl working\n- expand survival prediction tibbles\n- keras3 updates\n- use `<details>` for long model prints\n- avoid subsection titles capitalizing the engine name (e.g., \"CATBOOST\") and text within backticks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n:::\n\n\n### Apache Spark\n\nTo use [Apache Spark](https://spark.apache.org/) as an engine, we will first \nneed a connection to a cluster. For this article, we will setup and use a \nsingle-node Spark cluster running on a laptop:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nsc <- spark_connect(\"local\", version = \"4.0.1\")\n```\n:::\n\n\n\n# Classification Models\n\nTo demonstrate classification, let's make a small training and test sets for a binary outcome. We'll center and scale the data since some models require the same units.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(207)\nbin_split <- \n\tmodeldata::two_class_dat |> \n\trename(class = Class) |> \n\tinitial_split(prop = 0.994, strata = class)\nbin_split\n#> <Training/Testing/Total>\n#> <785/6/791>\n\nbin_rec <- \n  recipe(class ~ ., data = training(bin_split)) |> \n  step_normalize(all_numeric_predictors()) |> \n  prep()\n\nbin_train <- bake(bin_rec, new_data = NULL)\nbin_test <- bake(bin_rec, new_data = testing(bin_split))\n```\n:::\n\n\nFor models that _only_ work for three or more classes, we'll simulate:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1752)\nmtl_data <-\n sim_multinomial(\n    200,\n  ~  -0.5    +  0.6 * abs(A),\n  ~ ifelse(A > 0 & B > 0, 1.0 + 0.2 * A / B, - 2),\n  ~ A + B -  A * B)\n\nmtl_split <- initial_split(mtl_data, prop = 0.967, strata = class)\nmtl_split\n#> <Training/Testing/Total>\n#> <192/8/200>\n\n# Predictors are in the same units\nmtl_train <- training(mtl_split)\nmtl_test <- testing(mtl_split)\n```\n:::\n\n\nIf using the **Apache Spark** engine, we will need to identify the data source, \nand then use it to create the splits. For this article, we will copy the \n`two_class_dat` and the `mtl_data` data sets into the Spark session.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntbl_two_class <- copy_to(sc, modeldata::two_class_dat)\n\ntbl_bin <- sdf_random_split(tbl_two_class, training = 0.994, test = 1-0.994, seed = 100)\n\ntbl_sim_mtl <- copy_to(sc, mtl_data)\n\ntbl_mtl <- sdf_random_split(tbl_sim_mtl, training = 0.967, test = 1-0.967, seed = 100)\n```\n:::\n\n\n\n\n## Auto Ml (`auto_ml()`) \n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n\n# and initialize a server\nh20_server <- agua::h2o_start()\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_ml_spec <- auto_ml() |>\n  # We dont need to set the engine (since there is only one) but we'll set\n  # a time limit\n  set_engine(\"h2o\", max_runtime_secs = 60 * 3) |> \n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_ml_fit <- auto_ml_spec |> fit(class ~ ., data = bin_train)\nauto_ml_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(auto_ml_fit, type = \"class\", new_data = bin_test)\npredict(auto_ml_fit, type = \"prob\", new_data = bin_test)\n```\n:::\n\n\n## Bagged MARS (`bag_mars()`) \n\n## `earth` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mars_spec <- bag_mars() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and earth is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mars_fit <- bag_mars_spec |> fit(class ~ ., data = bin_train)\n#> \n#> Attaching package: 'plotrix'\n#> The following object is masked from 'package:scales':\n#> \n#>     rescale\n#> Warning: There was 1 warning in `dplyr::mutate()`.\n#> ℹ In argument: `model = iter(...)`.\n#> Caused by warning:\n#> ! glm.fit: fitted probabilities numerically 0 or 1 occurred\n#> Registered S3 method overwritten by 'butcher':\n#>   method                 from    \n#>   as.character.dev_topic generics\nbag_mars_fit\n#> parsnip model object\n#> \n#> Bagged MARS (classification with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term  value std.error  used\n#>   <chr> <dbl>     <dbl> <int>\n#> 1 B     100        0       11\n#> 2 A      40.8      1.22    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_mars_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(bag_mars_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.444       0.556 \n#> 2        0.860       0.140 \n#> 3        0.458       0.542 \n#> 4        0.950       0.0497\n#> 5        0.941       0.0593\n#> 6        0.868       0.132\n```\n:::\n\n\n## Bagged Neural Networks (`bag_mlp()`) \n\n## `nnet` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mlp_spec <- bag_mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and nnet is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mlp_fit <- bag_mlp_spec |> fit(class ~ ., data = bin_train)\nbag_mlp_fit\n#> parsnip model object\n#> \n#> Bagged nnet (classification with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term  value std.error  used\n#>   <chr> <dbl>     <dbl> <int>\n#> 1 B      55.1      1.98    11\n#> 2 A      44.9      1.98    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_mlp_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(bag_mlp_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.421        0.579\n#> 2        0.655        0.345\n#> 3        0.429        0.571\n#> 4        0.727        0.273\n#> 5        0.716        0.284\n#> 6        0.700        0.300\n```\n:::\n\n\n## Bagged Decision Trees (`bag_tree()`) \n\n## `C5.0` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_spec <- bag_tree() |> \n  set_mode(\"classification\") |> \n  set_engine(\"C5.0\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_fit <- bag_tree_spec |> fit(class ~ ., data = bin_train)\nbag_tree_fit\n#> parsnip model object\n#> \n#> Bagged C5.0 (classification with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term  value std.error  used\n#>   <chr> <dbl>     <dbl> <int>\n#> 1 B     100        0       11\n#> 2 A      58.9      6.71    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(bag_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.450       0.550 \n#> 2        0.825       0.175 \n#> 3        0.322       0.678 \n#> 4        0.911       0.0892\n#> 5        0.911       0.0892\n#> 6        0.710       0.290\n```\n:::\n\n\n## `rpart` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_spec <- bag_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and rpart is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_fit <- bag_tree_spec |> fit(class ~ ., data = bin_train)\nbag_tree_fit\n#> parsnip model object\n#> \n#> Bagged CART (classification with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term  value std.error  used\n#>   <chr> <dbl>     <dbl> <int>\n#> 1 B      275.      3.21    11\n#> 2 A      239.      4.04    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(bag_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1       0.0909        0.909\n#> 2       1             0    \n#> 3       0             1    \n#> 4       1             0    \n#> 5       0.727         0.273\n#> 6       1             0\n```\n:::\n\n\n## Bayesian Additive Regression Trees (`bart()`) \n\n## `dbarts` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbart_spec <- bart() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and dbarts is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbart_fit <- bart_spec |> fit(class ~ ., data = bin_train)\nbart_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> `NULL`()\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bart_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(bart_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.427        0.573\n#> 2        0.744        0.256\n#> 3        0.375        0.625\n#> 4        0.951        0.049\n#> 5        0.922        0.078\n#> 6        0.786        0.214\npredict(bart_fit, type = \"conf_int\", new_data = bin_test)\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_lower_Class2 .pred_upper_Class1 .pred_upper_Class2\n#>                <dbl>              <dbl>              <dbl>              <dbl>\n#> 1              0.812            0.00247              0.998              0.188\n#> 2              0.785            0.0248               0.975              0.215\n#> 3              0.605            0.0713               0.929              0.395\n#> 4              0.561            0.102                0.898              0.439\n#> 5              0.251            0.340                0.660              0.749\n#> 6              0.200            0.416                0.584              0.800\npredict(bart_fit, type = \"pred_int\", new_data = bin_test)\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_lower_Class2 .pred_upper_Class1 .pred_upper_Class2\n#>                <dbl>              <dbl>              <dbl>              <dbl>\n#> 1                  0                  0                  1                  1\n#> 2                  0                  0                  1                  1\n#> 3                  0                  0                  1                  1\n#> 4                  0                  0                  1                  1\n#> 5                  0                  0                  1                  1\n#> 6                  0                  0                  1                  1\n```\n:::\n\n\n## Boosted Decision Trees (`boost_tree()`) \n\n## `C5.0` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |> \n  set_mode(\"classification\") |> \n  set_engine(\"C5.0\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> C5.0.default(x = x, y = y, trials = 15, control = C50::C5.0Control(minCases\n#>  = 2, sample = 0))\n#> \n#> Classification Tree\n#> Number of samples: 785 \n#> Number of predictors: 2 \n#> \n#> Number of boosting iterations: 15 requested;  7 used due to early stopping\n#> Average tree size: 3.1 \n#> \n#> Non-standard options: attempt to group attributes\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(boost_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.307        0.693\n#> 2        0.756        0.244\n#> 3        0.281        0.719\n#> 4        1            0    \n#> 5        1            0    \n#> 6        0.626        0.374\n```\n:::\n\n\n## `catboost` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"catboost\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> CatBoost model (1000 trees)\n#> Loss function: Logloss\n#> Fit to 2 feature(s)\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(boost_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.252      0.748  \n#> 2        0.839      0.161  \n#> 3        0.348      0.652  \n#> 4        0.997      0.00279\n#> 5        0.807      0.193  \n#> 6        0.884      0.116\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"h2o_gbm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: gbm\n#> Model ID:  GBM_model_R_1763571327438_3826 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              50                       50               25380         6\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1         6    6.00000         21         55    35.70000\n#> \n#> \n#> H2OBinomialMetrics: gbm\n#> ** Reported on training data. **\n#> \n#> MSE:  0.007948832\n#> RMSE:  0.08915622\n#> LogLoss:  0.05942305\n#> Mean Per-Class Error:  0\n#> AUC:  1\n#> AUCPR:  1\n#> Gini:  1\n#> R^2:  0.9678452\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error    Rate\n#> Class1    434      0 0.000000  =0/434\n#> Class2      0    351 0.000000  =0/351\n#> Totals    434    351 0.000000  =0/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.598690   1.000000 200\n#> 2                       max f2  0.598690   1.000000 200\n#> 3                 max f0point5  0.598690   1.000000 200\n#> 4                 max accuracy  0.598690   1.000000 200\n#> 5                max precision  0.998192   1.000000   0\n#> 6                   max recall  0.598690   1.000000 200\n#> 7              max specificity  0.998192   1.000000   0\n#> 8             max absolute_mcc  0.598690   1.000000 200\n#> 9   max min_per_class_accuracy  0.598690   1.000000 200\n#> 10 max mean_per_class_accuracy  0.598690   1.000000 200\n#> 11                     max tns  0.998192 434.000000   0\n#> 12                     max fns  0.998192 349.000000   0\n#> 13                     max fps  0.000831 434.000000 399\n#> 14                     max tps  0.598690 351.000000 200\n#> 15                     max tnr  0.998192   1.000000   0\n#> 16                     max fnr  0.998192   0.994302   0\n#> 17                     max fpr  0.000831   1.000000 399\n#> 18                     max tpr  0.598690   1.000000 200\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(boost_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1       0.0496      0.950  \n#> 2       0.905       0.0953 \n#> 3       0.0738      0.926  \n#> 4       0.997       0.00273\n#> 5       0.979       0.0206 \n#> 6       0.878       0.122\n```\n:::\n\n\n## `h2o_gbm` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"h2o_gbm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: gbm\n#> Model ID:  GBM_model_R_1763571327438_3878 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              50                       50               25378         6\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1         6    6.00000         21         55    35.70000\n#> \n#> \n#> H2OBinomialMetrics: gbm\n#> ** Reported on training data. **\n#> \n#> MSE:  0.007948832\n#> RMSE:  0.08915622\n#> LogLoss:  0.05942305\n#> Mean Per-Class Error:  0\n#> AUC:  1\n#> AUCPR:  1\n#> Gini:  1\n#> R^2:  0.9678452\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error    Rate\n#> Class1    434      0 0.000000  =0/434\n#> Class2      0    351 0.000000  =0/351\n#> Totals    434    351 0.000000  =0/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.598690   1.000000 200\n#> 2                       max f2  0.598690   1.000000 200\n#> 3                 max f0point5  0.598690   1.000000 200\n#> 4                 max accuracy  0.598690   1.000000 200\n#> 5                max precision  0.998192   1.000000   0\n#> 6                   max recall  0.598690   1.000000 200\n#> 7              max specificity  0.998192   1.000000   0\n#> 8             max absolute_mcc  0.598690   1.000000 200\n#> 9   max min_per_class_accuracy  0.598690   1.000000 200\n#> 10 max mean_per_class_accuracy  0.598690   1.000000 200\n#> 11                     max tns  0.998192 434.000000   0\n#> 12                     max fns  0.998192 349.000000   0\n#> 13                     max fps  0.000831 434.000000 399\n#> 14                     max tps  0.598690 351.000000 200\n#> 15                     max tnr  0.998192   1.000000   0\n#> 16                     max fnr  0.998192   0.994302   0\n#> 17                     max fpr  0.000831   1.000000 399\n#> 18                     max tpr  0.598690   1.000000 200\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(boost_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1       0.0496      0.950  \n#> 2       0.905       0.0953 \n#> 3       0.0738      0.926  \n#> 4       0.997       0.00273\n#> 5       0.979       0.0206 \n#> 6       0.878       0.122\n```\n:::\n\n\n## `lightgbm` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"lightgbm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> LightGBM Model (100 trees)\n#> Objective: binary\n#> Fitted to dataset with 2 columns\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(boost_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.147       0.853 \n#> 2        0.930       0.0699\n#> 3        0.237       0.763 \n#> 4        0.990       0.0101\n#> 5        0.929       0.0714\n#> 6        0.956       0.0445\n```\n:::\n\n\n## `xgboost` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and xgboost is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(class ~ ., data = bin_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> ##### xgb.Booster\n#> raw: 40.4 Kb \n#> call:\n#>   xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n#>     colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n#>     subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n#>     verbose = 0, nthread = 1, objective = \"binary:logistic\")\n#> params (as set within xgb.train):\n#>   eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\n#> xgb.attributes:\n#>   niter\n#> callbacks:\n#>   cb.evaluation.log()\n#> # of features: 2 \n#> niter: 15\n#> nfeatures : 2 \n#> evaluation_log:\n#>   iter training_logloss\n#>  <num>            <num>\n#>      1        0.5546750\n#>      2        0.4719804\n#>    ---              ---\n#>     14        0.2587640\n#>     15        0.2528938\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(boost_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.244       0.756 \n#> 2        0.770       0.230 \n#> 3        0.307       0.693 \n#> 4        0.944       0.0565\n#> 5        0.821       0.179 \n#> 6        0.938       0.0621\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |> \n  set_mode(\"classification\") |> \n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(Class ~ ., data = tbl_bin$training)\nboost_tree_fit\n#> parsnip model object\n#> \n#> Formula: Class ~ .\n#> \n#> GBTClassificationModel: uid = gradient_boosted_trees__c61f3c19_30b0_416f_af47_e371c1aea2db, numTrees=20, numClasses=2, numFeatures=2\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"class\", new_data = tbl_bin$test)\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>   pred_class\n#>   <chr>     \n#> 1 Class2    \n#> 2 Class2    \n#> 3 Class1    \n#> 4 Class2    \n#> 5 Class2    \n#> 6 Class1    \n#> 7 Class2\npredict(boost_tree_fit, type = \"prob\", new_data = tbl_bin$test)\n#> # Source:   SQL [?? x 2]\n#> # Database: spark_connection\n#>   pred_Class1 pred_Class2\n#>         <dbl>       <dbl>\n#> 1      0.307       0.693 \n#> 2      0.292       0.708 \n#> 3      0.856       0.144 \n#> 4      0.192       0.808 \n#> 5      0.332       0.668 \n#> 6      0.952       0.0476\n#> 7      0.0865      0.914\n```\n:::\n\n\n\n## C5 Rules (`C5_rules()`) \n\n## `C5.0` Engine \n\nThis engine requires the rules extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rules)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and C5.0 is the default engine so there is no need to set that either.\nC5_rules_spec <- C5_rules()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nC5_rules_fit <- C5_rules_spec |> fit(class ~ ., data = bin_train)\nC5_rules_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> C5.0.default(x = x, y = y, trials = trials, rules = TRUE, control\n#>  = C50::C5.0Control(minCases = minCases, seed = sample.int(10^5,\n#>  1), earlyStopping = FALSE))\n#> \n#> Rule-Based Model\n#> Number of samples: 785 \n#> Number of predictors: 2 \n#> \n#> Number of Rules: 4 \n#> \n#> Non-standard options: attempt to group attributes\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(C5_rules_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class1     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(C5_rules_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1            1            0\n#> 2            1            0\n#> 3            0            1\n#> 4            1            0\n#> 5            1            0\n#> 6            1            0\n```\n:::\n\n\n## Decision Tree (`decision_tree()`) \n\n## `C5.0` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |> \n  set_mode(\"classification\") |> \n  set_engine(\"C5.0\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(class ~ ., data = bin_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> C5.0.default(x = x, y = y, trials = 1, control = C50::C5.0Control(minCases =\n#>  2, sample = 0))\n#> \n#> Classification Tree\n#> Number of samples: 785 \n#> Number of predictors: 2 \n#> \n#> Tree size: 4 \n#> \n#> Non-standard options: attempt to group attributes\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class1     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(decision_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.732        0.268\n#> 2        0.846        0.154\n#> 3        0.236        0.764\n#> 4        0.846        0.154\n#> 5        0.846        0.154\n#> 6        0.846        0.154\n```\n:::\n\n\n## `partykit` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"partykit\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(class ~ ., data = bin_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> \n#> Model formula:\n#> class ~ A + B\n#> \n#> Fitted party:\n#> [1] root\n#> |   [2] B <= -0.06906\n#> |   |   [3] B <= -0.50486: Class1 (n = 291, err = 8.2%)\n#> |   |   [4] B > -0.50486\n#> |   |   |   [5] A <= -0.07243: Class1 (n = 77, err = 45.5%)\n#> |   |   |   [6] A > -0.07243: Class1 (n = 31, err = 6.5%)\n#> |   [7] B > -0.06906\n#> |   |   [8] B <= 0.72938\n#> |   |   |   [9] A <= 0.60196: Class2 (n = 145, err = 24.8%)\n#> |   |   |   [10] A > 0.60196\n#> |   |   |   |   [11] B <= 0.44701: Class1 (n = 23, err = 4.3%)\n#> |   |   |   |   [12] B > 0.44701: Class1 (n = 26, err = 46.2%)\n#> |   |   [13] B > 0.72938: Class2 (n = 192, err = 12.5%)\n#> \n#> Number of inner nodes:    6\n#> Number of terminal nodes: 7\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class1     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(decision_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.538       0.462 \n#> 2        0.935       0.0645\n#> 3        0.248       0.752 \n#> 4        0.918       0.0825\n#> 5        0.918       0.0825\n#> 6        0.935       0.0645\n```\n:::\n\n\n## `rpart` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and rpart is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(class ~ ., data = bin_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> n= 785 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 785 351 Class1 (0.5528662 0.4471338)  \n#>    2) B< -0.06526451 399  61 Class1 (0.8471178 0.1528822) *\n#>    3) B>=-0.06526451 386  96 Class2 (0.2487047 0.7512953)  \n#>      6) B< 0.7339337 194  72 Class2 (0.3711340 0.6288660)  \n#>       12) A>=0.6073948 49  13 Class1 (0.7346939 0.2653061) *\n#>       13) A< 0.6073948 145  36 Class2 (0.2482759 0.7517241) *\n#>      7) B>=0.7339337 192  24 Class2 (0.1250000 0.8750000) *\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class1     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(decision_tree_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.735        0.265\n#> 2        0.847        0.153\n#> 3        0.248        0.752\n#> 4        0.847        0.153\n#> 5        0.847        0.153\n#> 6        0.847        0.153\n```\n:::\n\n\n## `sparklyr` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(Class ~ ., data = tbl_bin$training)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> n= 784 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 784 350 Class1 (0.5535714 0.4464286)  \n#>    2) B< 1.495535 401  62 Class1 (0.8453865 0.1546135) *\n#>    3) B>=1.495535 383  95 Class2 (0.2480418 0.7519582)  \n#>      6) B< 2.079458 192  71 Class2 (0.3697917 0.6302083)  \n#>       12) A>=2.572663 50  14 Class1 (0.7200000 0.2800000) *\n#>       13) A< 2.572663 142  35 Class2 (0.2464789 0.7535211) *\n#>      7) B>=2.079458 191  24 Class2 (0.1256545 0.8743455) *\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, type = \"class\", new_data = tbl_bin$test)\n#> # A tibble: 7 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 <NA>       \n#> 2 <NA>       \n#> 3 <NA>       \n#> 4 <NA>       \n#> 5 <NA>       \n#> 6 <NA>       \n#> 7 <NA>\npredict(decision_tree_fit, type = \"prob\", new_data = tbl_bin$test)\n#> # A tibble: 7 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.246        0.754\n#> 2        0.246        0.754\n#> 3        0.845        0.155\n#> 4        0.246        0.754\n#> 5        0.246        0.754\n#> 6        0.845        0.155\n#> 7        0.126        0.874\n```\n:::\n\n\n## Flexible Discriminant Analysis (`discrim_flexible()`) \n\n## `earth` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and earth is the default engine so there is no need to set that either.\ndiscrim_flexible_spec <- discrim_flexible()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_flexible_fit <- discrim_flexible_spec |> fit(class ~ ., data = bin_train)\ndiscrim_flexible_fit\n#> parsnip model object\n#> \n#> Call:\n#> mda::fda(formula = class ~ ., data = data, method = earth::earth)\n#> \n#> Dimension: 1 \n#> \n#> Percent Between-Group Variance Explained:\n#>  v1 \n#> 100 \n#> \n#> Training Misclassification Error: 0.1707 ( N = 785 )\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_flexible_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_flexible_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.339       0.661 \n#> 2        0.848       0.152 \n#> 3        0.342       0.658 \n#> 4        0.964       0.0360\n#> 5        0.964       0.0360\n#> 6        0.875       0.125\n```\n:::\n\n\n## Linear Discriminant Analysis (`discrim_linear()`) \n\n## `MASS` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and MASS is the default engine so there is no need to set that either.\ndiscrim_linear_spec <- discrim_linear()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)\ndiscrim_linear_fit\n#> parsnip model object\n#> \n#> Call:\n#> lda(class ~ ., data = data)\n#> \n#> Prior probabilities of groups:\n#>    Class1    Class2 \n#> 0.5528662 0.4471338 \n#> \n#> Group means:\n#>                 A          B\n#> Class1 -0.2982900 -0.5573140\n#> Class2  0.3688258  0.6891006\n#> \n#> Coefficients of linear discriminants:\n#>          LD1\n#> A -0.6068479\n#> B  1.7079953\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_linear_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_linear_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.369       0.631 \n#> 2        0.868       0.132 \n#> 3        0.541       0.459 \n#> 4        0.984       0.0158\n#> 5        0.928       0.0718\n#> 6        0.854       0.146\n```\n:::\n\n\n## `mda` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_spec <- discrim_linear() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"mda\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)\ndiscrim_linear_fit\n#> parsnip model object\n#> \n#> Call:\n#> mda::fda(formula = class ~ ., data = data, method = mda::gen.ridge, \n#>     keep.fitted = FALSE)\n#> \n#> Dimension: 1 \n#> \n#> Percent Between-Group Variance Explained:\n#>  v1 \n#> 100 \n#> \n#> Degrees of Freedom (per dimension): 1.99423 \n#> \n#> Training Misclassification Error: 0.17707 ( N = 785 )\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_linear_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_linear_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.368       0.632 \n#> 2        0.867       0.133 \n#> 3        0.542       0.458 \n#> 4        0.984       0.0158\n#> 5        0.928       0.0718\n#> 6        0.853       0.147\n```\n:::\n\n\n## `sda` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_spec <- discrim_linear() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"sda\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)\ndiscrim_linear_fit\n#> parsnip model object\n#> \n#> $regularization\n#>       lambda   lambda.var lambda.freqs \n#>  0.003136201  0.067551534  0.112819609 \n#> \n#> $freqs\n#>    Class1    Class2 \n#> 0.5469019 0.4530981 \n#> \n#> $alpha\n#>     Class1     Class2 \n#> -0.8934125 -1.2349286 \n#> \n#> $beta\n#>                 A         B\n#> Class1  0.4565325 -1.298858\n#> Class2 -0.5510473  1.567757\n#> attr(,\"class\")\n#> [1] \"shrinkage\"\n#> \n#> attr(,\"class\")\n#> [1] \"sda\"\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_linear_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_linear_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.366       0.634 \n#> 2        0.860       0.140 \n#> 3        0.536       0.464 \n#> 4        0.982       0.0176\n#> 5        0.923       0.0768\n#> 6        0.845       0.155\n```\n:::\n\n\n## `sparsediscrim` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_spec <- discrim_linear() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"sparsediscrim\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_linear_fit <- discrim_linear_spec |> fit(class ~ ., data = bin_train)\ndiscrim_linear_fit\n#> parsnip model object\n#> \n#> Diagonal LDA\n#> \n#> Sample Size: 785 \n#> Number of Features: 2 \n#> \n#> Classes and Prior Probabilities:\n#>   Class1 (55.29%), Class2 (44.71%)\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_linear_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_linear_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.182      0.818  \n#> 2        0.755      0.245  \n#> 3        0.552      0.448  \n#> 4        0.996      0.00372\n#> 5        0.973      0.0274 \n#> 6        0.629      0.371\n```\n:::\n\n\n## Quandratic Discriminant Analysis (`discrim_quad()`) \n\n## `MASS` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_quad_spec <- discrim_quad()\n  # This engine works with a single mode so no need to set that\n  # and MASS is the default engine so there is no need to set that either.\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_quad_fit <- discrim_quad_spec |> fit(class ~ ., data = bin_train)\ndiscrim_quad_fit\n#> parsnip model object\n#> \n#> Call:\n#> qda(class ~ ., data = data)\n#> \n#> Prior probabilities of groups:\n#>    Class1    Class2 \n#> 0.5528662 0.4471338 \n#> \n#> Group means:\n#>                 A          B\n#> Class1 -0.2982900 -0.5573140\n#> Class2  0.3688258  0.6891006\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_quad_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_quad_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.340       0.660 \n#> 2        0.884       0.116 \n#> 3        0.500       0.500 \n#> 4        0.965       0.0349\n#> 5        0.895       0.105 \n#> 6        0.895       0.105\n```\n:::\n\n\n## `sparsediscrim` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_quad_spec <- discrim_quad() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"sparsediscrim\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_quad_fit <- discrim_quad_spec |> fit(class ~ ., data = bin_train)\ndiscrim_quad_fit\n#> parsnip model object\n#> \n#> Diagonal QDA\n#> \n#> Sample Size: 785 \n#> Number of Features: 2 \n#> \n#> Classes and Prior Probabilities:\n#>   Class1 (55.29%), Class2 (44.71%)\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_quad_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_quad_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.180      0.820  \n#> 2        0.750      0.250  \n#> 3        0.556      0.444  \n#> 4        0.994      0.00634\n#> 5        0.967      0.0328 \n#> 6        0.630      0.370\n```\n:::\n\n\n## Regularized Discriminant Analysis (`discrim_regularized()`) \n\n## `klaR` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and klaR is the default engine so there is no need to set that either.\ndiscrim_regularized_spec <- discrim_regularized()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiscrim_regularized_fit <- discrim_regularized_spec |> fit(class ~ ., data = bin_train)\ndiscrim_regularized_fit\n#> parsnip model object\n#> \n#> Call: \n#> rda(formula = class ~ ., data = data)\n#> \n#> Regularization parameters: \n#>        gamma       lambda \n#> 5.344614e-15 1.032850e-02 \n#> \n#> Prior probabilities of groups: \n#>    Class1    Class2 \n#> 0.5528662 0.4471338 \n#> \n#> Misclassification rate: \n#>        apparent: 17.707 %\n#> cross-validated: 17.844 %\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(discrim_regularized_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(discrim_regularized_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.340       0.660 \n#> 2        0.884       0.116 \n#> 3        0.501       0.499 \n#> 4        0.965       0.0346\n#> 5        0.895       0.105 \n#> 6        0.895       0.105\n```\n:::\n\n\n## Generalized Additive Models (`gen_additive_mod()`) \n\n## `mgcv` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngen_additive_mod_spec <- gen_additive_mod() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and mgcv is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngen_additive_mod_fit <- \n  gen_additive_mod_spec |> \n  fit(class ~ s(A) + s(B), data = bin_train)\ngen_additive_mod_fit\n#> parsnip model object\n#> \n#> \n#> Family: binomial \n#> Link function: logit \n#> \n#> Formula:\n#> class ~ s(A) + s(B)\n#> \n#> Estimated degrees of freedom:\n#> 2.76 4.22  total = 7.98 \n#> \n#> UBRE score: -0.153537\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(gen_additive_mod_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(gen_additive_mod_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.400       0.600 \n#> 2        0.826       0.174 \n#> 3        0.454       0.546 \n#> 4        0.975       0.0250\n#> 5        0.929       0.0711\n#> 6        0.829       0.171\npredict(gen_additive_mod_fit, type = \"conf_int\", new_data = bin_test)\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_upper_Class1 .pred_lower_Class2 .pred_upper_Class2\n#>            <dbl[1d]>          <dbl[1d]>          <dbl[1d]>          <dbl[1d]>\n#> 1              0.304              0.504            0.496                0.696\n#> 2              0.739              0.889            0.111                0.261\n#> 3              0.364              0.546            0.454                0.636\n#> 4              0.846              0.996            0.00358              0.154\n#> 5              0.881              0.958            0.0416               0.119\n#> 6              0.735              0.894            0.106                0.265\n```\n:::\n\n\n## Logistic Regression (`logistic_reg()`) \n\n## `brulee` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"brulee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> Logistic regression\n#> \n#> 785 samples, 2 features, 2 classes \n#> class weights Class1=1, Class2=1 \n#> weight decay: 0.001 \n#> batch size: 707 \n#> validation loss after 2 epochs: 0.375\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.409       0.591 \n#> 2        0.865       0.135 \n#> 3        0.544       0.456 \n#> 4        0.976       0.0239\n#> 5        0.909       0.0914\n#> 6        0.857       0.143\n```\n:::\n\n\n## `gee` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"gee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n```\n:::\n\n\n## `glm` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg()\n  # This engine works with a single mode so no need to set that\n  # and glm is the default engine so there is no need to set that either.\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  stats::glm(formula = class ~ ., family = stats::binomial, data = data)\n#> \n#> Coefficients:\n#> (Intercept)            A            B  \n#>     -0.3563      -1.1250       2.8154  \n#> \n#> Degrees of Freedom: 784 Total (i.e. Null);  782 Residual\n#> Null Deviance:\t    1079 \n#> Residual Deviance: 666.9 \tAIC: 672.9\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.400       0.600 \n#> 2        0.862       0.138 \n#> 3        0.541       0.459 \n#> 4        0.977       0.0234\n#> 5        0.909       0.0905\n#> 6        0.853       0.147\npredict(logistic_reg_fit, type = \"conf_int\", new_data = bin_test)\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_upper_Class1 .pred_lower_Class2 .pred_upper_Class2\n#>                <dbl>              <dbl>              <dbl>              <dbl>\n#> 1              0.339              0.465             0.535              0.661 \n#> 2              0.816              0.897             0.103              0.184 \n#> 3              0.493              0.588             0.412              0.507 \n#> 4              0.960              0.986             0.0137             0.0395\n#> 5              0.875              0.935             0.0647             0.125 \n#> 6              0.800              0.894             0.106              0.200\n```\n:::\n\n\n## `glmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.400       0.600 \n#> 2        0.862       0.138 \n#> 3        0.541       0.459 \n#> 4        0.977       0.0234\n#> 5        0.909       0.0905\n#> 6        0.853       0.147\n```\n:::\n\n\n## `glmnet` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg(penalty = 0.01) |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmnet\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"binomial\") \n#> \n#>    Df  %Dev   Lambda\n#> 1   0  0.00 0.308300\n#> 2   1  4.75 0.280900\n#> 3   1  8.73 0.256000\n#> 4   1 12.10 0.233200\n#> 5   1 14.99 0.212500\n#> 6   1 17.46 0.193600\n#> 7   1 19.60 0.176400\n#> 8   1 21.45 0.160800\n#> 9   1 23.05 0.146500\n#> 10  1 24.44 0.133500\n#> 11  1 25.65 0.121600\n#> 12  1 26.70 0.110800\n#> 13  1 27.61 0.101000\n#> 14  1 28.40 0.091990\n#> 15  1 29.08 0.083820\n#> 16  1 29.68 0.076370\n#> 17  1 30.19 0.069590\n#> 18  1 30.63 0.063410\n#> 19  1 31.00 0.057770\n#> 20  1 31.33 0.052640\n#> 21  1 31.61 0.047960\n#> 22  1 31.85 0.043700\n#> 23  1 32.05 0.039820\n#> 24  2 32.62 0.036280\n#> 25  2 33.41 0.033060\n#> 26  2 34.10 0.030120\n#> 27  2 34.68 0.027450\n#> 28  2 35.19 0.025010\n#> 29  2 35.63 0.022790\n#> 30  2 36.01 0.020760\n#> 31  2 36.33 0.018920\n#> 32  2 36.62 0.017240\n#> 33  2 36.86 0.015710\n#> 34  2 37.06 0.014310\n#> 35  2 37.24 0.013040\n#> 36  2 37.39 0.011880\n#> 37  2 37.52 0.010830\n#> 38  2 37.63 0.009864\n#> 39  2 37.72 0.008988\n#> 40  2 37.80 0.008189\n#> 41  2 37.86 0.007462\n#> 42  2 37.92 0.006799\n#> 43  2 37.97 0.006195\n#> 44  2 38.01 0.005644\n#> 45  2 38.04 0.005143\n#> 46  2 38.07 0.004686\n#> 47  2 38.10 0.004270\n#> 48  2 38.12 0.003891\n#> 49  2 38.13 0.003545\n#> 50  2 38.15 0.003230\n#> 51  2 38.16 0.002943\n#> 52  2 38.17 0.002682\n#> 53  2 38.18 0.002443\n#> 54  2 38.18 0.002226\n#> 55  2 38.19 0.002029\n#> 56  2 38.19 0.001848\n#> 57  2 38.20 0.001684\n#> 58  2 38.20 0.001534\n#> 59  2 38.20 0.001398\n#> 60  2 38.21 0.001274\n#> 61  2 38.21 0.001161\n#> 62  2 38.21 0.001058\n#> 63  2 38.21 0.000964\n#> 64  2 38.21 0.000878\n#> 65  2 38.21 0.000800\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.383       0.617 \n#> 2        0.816       0.184 \n#> 3        0.537       0.463 \n#> 4        0.969       0.0313\n#> 5        0.894       0.106 \n#> 6        0.797       0.203\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: glm\n#> Model ID:  GLM_model_R_1763571327438_3930 \n#> GLM Model: summary\n#>     family  link                                regularization\n#> 1 binomial logit Elastic Net (alpha = 0.5, lambda = 6.162E-4 )\n#>   number_of_predictors_total number_of_active_predictors number_of_iterations\n#> 1                          2                           2                    4\n#>      training_frame\n#> 1 object_xtqmofwsbr\n#> \n#> Coefficients: glm coefficients\n#>       names coefficients standardized_coefficients\n#> 1 Intercept    -0.350788                 -0.350788\n#> 2         A    -1.084233                 -1.084233\n#> 3         B     2.759366                  2.759366\n#> \n#> H2OBinomialMetrics: glm\n#> ** Reported on training data. **\n#> \n#> MSE:  0.130451\n#> RMSE:  0.3611799\n#> LogLoss:  0.4248206\n#> Mean Per-Class Error:  0.1722728\n#> AUC:  0.8889644\n#> AUCPR:  0.8520865\n#> Gini:  0.7779288\n#> R^2:  0.4722968\n#> Residual Deviance:  666.9684\n#> AIC:  672.9684\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error      Rate\n#> Class1    350     84 0.193548   =84/434\n#> Class2     53    298 0.150997   =53/351\n#> Totals    403    382 0.174522  =137/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.411045   0.813097 213\n#> 2                       max f2  0.229916   0.868991 279\n#> 3                 max f0point5  0.565922   0.816135 166\n#> 4                 max accuracy  0.503565   0.826752 185\n#> 5                max precision  0.997356   1.000000   0\n#> 6                   max recall  0.009705   1.000000 395\n#> 7              max specificity  0.997356   1.000000   0\n#> 8             max absolute_mcc  0.411045   0.652014 213\n#> 9   max min_per_class_accuracy  0.454298   0.822581 201\n#> 10 max mean_per_class_accuracy  0.411045   0.827727 213\n#> 11                     max tns  0.997356 434.000000   0\n#> 12                     max fns  0.997356 349.000000   0\n#> 13                     max fps  0.001723 434.000000 399\n#> 14                     max tps  0.009705 351.000000 395\n#> 15                     max tnr  0.997356   1.000000   0\n#> 16                     max fnr  0.997356   0.994302   0\n#> 17                     max fpr  0.001723   1.000000 399\n#> 18                     max tpr  0.009705   1.000000 395\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.399       0.601 \n#> 2        0.857       0.143 \n#> 3        0.540       0.460 \n#> 4        0.976       0.0243\n#> 5        0.908       0.0925\n#> 6        0.848       0.152\n```\n:::\n\n\n## `keras` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"keras\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n```\n:::\n\n\n## `LiblineaR` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"LiblineaR\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> $TypeDetail\n#> [1] \"L2-regularized logistic regression primal (L2R_LR)\"\n#> \n#> $Type\n#> [1] 0\n#> \n#> $W\n#>             A        B      Bias\n#> [1,] 1.014233 -2.65166 0.3363362\n#> \n#> $Bias\n#> [1] 1\n#> \n#> $ClassNames\n#> [1] Class1 Class2\n#> Levels: Class1 Class2\n#> \n#> $NbClass\n#> [1] 2\n#> \n#> attr(,\"class\")\n#> [1] \"LiblineaR\"\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.397       0.603 \n#> 2        0.847       0.153 \n#> 3        0.539       0.461 \n#> 4        0.973       0.0267\n#> 5        0.903       0.0974\n#> 6        0.837       0.163\n```\n:::\n\n\n## `stan` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"stan\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> stan_glm\n#>  family:       binomial [logit]\n#>  formula:      class ~ .\n#>  observations: 785\n#>  predictors:   3\n#> ------\n#>             Median MAD_SD\n#> (Intercept) -0.4    0.1  \n#> A           -1.1    0.2  \n#> B            2.8    0.2  \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.399       0.601 \n#> 2        0.860       0.140 \n#> 3        0.541       0.459 \n#> 4        0.977       0.0234\n#> 5        0.909       0.0906\n#> 6        0.852       0.148\npredict(logistic_reg_fit, type = \"conf_int\", new_data = bin_test)\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_upper_Class1 .pred_lower_Class2 .pred_upper_Class2\n#>                <dbl>              <dbl>              <dbl>              <dbl>\n#> 1              0.338              0.463             0.537              0.662 \n#> 2              0.815              0.897             0.103              0.185 \n#> 3              0.493              0.588             0.412              0.507 \n#> 4              0.961              0.986             0.0135             0.0389\n#> 5              0.876              0.936             0.0643             0.124 \n#> 6              0.798              0.893             0.107              0.202\npredict(logistic_reg_fit, type = \"pred_int\", new_data = bin_test)\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_upper_Class1 .pred_lower_Class2 .pred_upper_Class2\n#>                <dbl>              <dbl>              <dbl>              <dbl>\n#> 1                  0                  1                  0                  1\n#> 2                  0                  1                  0                  1\n#> 3                  0                  1                  0                  1\n#> 4                  0                  1                  0                  1\n#> 5                  0                  1                  0                  1\n#> 6                  0                  1                  0                  1\n```\n:::\n\n\n## `stan_glmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"stan_glmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(class ~ ., data = bin_train)\nlogistic_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = bin_test)\npredict(logistic_reg_fit, type = \"prob\", new_data = bin_test)\npredict(logistic_reg_fit, type = \"conf_int\", new_data = bin_test)\npredict(logistic_reg_fit, type = \"pred_int\", new_data = bin_test)\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_spec <- logistic_reg() |> \n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg_fit <- logistic_reg_spec |> fit(Class ~ ., data = tbl_bin$training)\nlogistic_reg_fit\n#> parsnip model object\n#> \n#> Formula: Class ~ .\n#> \n#> Coefficients:\n#> (Intercept)           A           B \n#>   -3.731170   -1.214355    3.794186\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logistic_reg_fit, type = \"class\", new_data = tbl_bin$test)\npredict(logistic_reg_fit, type = \"prob\", new_data = tbl_bin$test)\n```\n:::\n\n\n\n## Multivariate Adaptive Regression Splines (`mars()`) \n\n## `earth` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_spec <- mars() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and earth is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_fit <- mars_spec |> fit(class ~ ., data = bin_train)\nmars_fit\n#> parsnip model object\n#> \n#> GLM (family binomial, link logit):\n#>  nulldev  df       dev  df   devratio     AIC iters converged\n#>  1079.45 784   638.975 779      0.408     651     5         1\n#> \n#> Earth selected 6 of 13 terms, and 2 of 2 predictors\n#> Termination condition: Reached nk 21\n#> Importance: B, A\n#> Number of terms at each degree of interaction: 1 5 (additive model)\n#> Earth GCV 0.1342746    RSS 102.4723    GRSq 0.4582121    RSq 0.4719451\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mars_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(mars_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.410       0.590 \n#> 2        0.794       0.206 \n#> 3        0.356       0.644 \n#> 4        0.927       0.0729\n#> 5        0.927       0.0729\n#> 6        0.836       0.164\n```\n:::\n\n\n## Neural Networks (`mlp()`) \n\n## `brulee` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"brulee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)\nmlp_fit\n#> parsnip model object\n#> \n#> Multilayer perceptron\n#> \n#> relu activation,\n#> 3 hidden units,\n#> 17 model parameters\n#> 785 samples, 2 features, 2 classes \n#> class weights Class1=1, Class2=1 \n#> weight decay: 0.001 \n#> dropout proportion: 0 \n#> batch size: 707 \n#> learn rate: 0.01 \n#> validation loss after 4 epochs: 0.508\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(mlp_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.390        0.610\n#> 2        0.854        0.146\n#> 3        0.507        0.493\n#> 4        0.830        0.170\n#> 5        0.828        0.172\n#> 6        0.851        0.149\n```\n:::\n\n\n## `brulee_two_layer` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"brulee_two_layer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)\nmlp_fit\n#> parsnip model object\n#> \n#> Multilayer perceptron\n#> \n#> c(relu,relu) activation,\n#> c(3,3) hidden units,\n#> 29 model parameters\n#> 785 samples, 2 features, 2 classes \n#> class weights Class1=1, Class2=1 \n#> weight decay: 0.001 \n#> dropout proportion: 0 \n#> batch size: 707 \n#> learn rate: 0.01 \n#> validation loss after 16 epochs: 0.307\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(mlp_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.411       0.589 \n#> 2        0.883       0.117 \n#> 3        0.520       0.480 \n#> 4        0.971       0.0293\n#> 5        0.938       0.0618\n#> 6        0.871       0.129\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)\nmlp_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: deeplearning\n#> Model ID:  DeepLearning_model_R_1763571327438_3932 \n#> Status of Neuron Layers: predicting .outcome, 2-class classification, bernoulli distribution, CrossEntropy loss, 1,002 weights/biases, 16.9 KB, 7,850 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1     2     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.008994 0.023584 0.000000\n#> 3     3     2   Softmax      NA 0.000000 0.000000  0.002983 0.000548 0.000000\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2    0.006098   0.105669  0.492018 0.020146\n#> 3    0.033179   0.403317 -0.015716 0.023938\n#> \n#> \n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on full training frame **\n#> \n#> MSE:  0.130512\n#> RMSE:  0.3612645\n#> LogLoss:  0.4275074\n#> Mean Per-Class Error:  0.1685671\n#> AUC:  0.8893418\n#> AUCPR:  0.8486687\n#> Gini:  0.7786837\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error      Rate\n#> Class1    373     61 0.140553   =61/434\n#> Class2     69    282 0.196581   =69/351\n#> Totals    442    343 0.165605  =130/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.466071   0.812680 192\n#> 2                       max f2  0.210358   0.870370 283\n#> 3                 max f0point5  0.482168   0.819964 186\n#> 4                 max accuracy  0.466071   0.834395 192\n#> 5                max precision  0.885661   0.950495  47\n#> 6                   max recall  0.004683   1.000000 396\n#> 7              max specificity  0.991894   0.997696   0\n#> 8             max absolute_mcc  0.466071   0.664455 192\n#> 9   max min_per_class_accuracy  0.427673   0.823362 206\n#> 10 max mean_per_class_accuracy  0.466071   0.831433 192\n#> 11                     max tns  0.991894 433.000000   0\n#> 12                     max fns  0.991894 349.000000   0\n#> 13                     max fps  0.000622 434.000000 399\n#> 14                     max tps  0.004683 351.000000 396\n#> 15                     max tnr  0.991894   0.997696   0\n#> 16                     max fnr  0.991894   0.994302   0\n#> 17                     max fpr  0.000622   1.000000 399\n#> 18                     max tpr  0.004683   1.000000 396\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(mlp_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.469       0.531 \n#> 2        0.898       0.102 \n#> 3        0.581       0.419 \n#> 4        0.981       0.0191\n#> 5        0.919       0.0808\n#> 6        0.898       0.102\n```\n:::\n\n\n## `keras` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"keras\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)\nmlp_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, type = \"class\", new_data = bin_test)\npredict(mlp_fit, type = \"prob\", new_data = bin_test)\n```\n:::\n\n\n## `nnet` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and nnet is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(class ~ ., data = bin_train)\nmlp_fit\n#> parsnip model object\n#> \n#> a 2-5-1 network with 21 weights\n#> inputs: A B \n#> output(s): class \n#> options were - entropy fitting\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(mlp_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.418        0.582\n#> 2        0.658        0.342\n#> 3        0.406        0.594\n#> 4        0.725        0.275\n#> 5        0.714        0.286\n#> 6        0.633        0.367\n```\n:::\n\n\n## Multinom Regression (`multinom_reg()`) \n\n## `brulee` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_spec <- multinom_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"brulee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)\nmultinom_reg_fit\n#> parsnip model object\n#> \n#> Multinomial regression\n#> \n#> 192 samples, 2 features, 3 classes \n#> class weights one=1, two=1, three=1 \n#> weight decay: 0.001 \n#> batch size: 173 \n#> validation loss after 1 epoch: 0.816\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(multinom_reg_fit, type = \"class\", new_data = mtl_test)\n#> # A tibble: 8 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 three      \n#> 2 three      \n#> 3 three      \n#> 4 one        \n#> 5 one        \n#> 6 two        \n#> 7 three      \n#> 8 one\npredict(multinom_reg_fit, type = \"prob\", new_data = mtl_test)\n#> # A tibble: 8 × 3\n#>   .pred_one .pred_two .pred_three\n#>       <dbl>     <dbl>       <dbl>\n#> 1   0.133     0.207        0.660 \n#> 2   0.298     0.189        0.512 \n#> 3   0.346     0.206        0.448 \n#> 4   0.985     0.00158      0.0134\n#> 5   0.956     0.00343      0.0404\n#> 6   0.00328   0.742        0.254 \n#> 7   0.0570    0.411        0.532 \n#> 8   0.487     0.0488       0.465\n```\n:::\n\n\n## `glmnet` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_spec <- multinom_reg(penalty = 0.01) |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmnet\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)\nmultinom_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"multinomial\") \n#> \n#>    Df  %Dev   Lambda\n#> 1   0  0.00 0.219200\n#> 2   1  1.61 0.199700\n#> 3   2  3.90 0.181900\n#> 4   2  6.07 0.165800\n#> 5   2  7.93 0.151100\n#> 6   2  9.52 0.137600\n#> 7   2 10.90 0.125400\n#> 8   2 12.09 0.114300\n#> 9   2 13.13 0.104100\n#> 10  2 14.22 0.094870\n#> 11  2 15.28 0.086440\n#> 12  2 16.20 0.078760\n#> 13  2 16.99 0.071760\n#> 14  2 17.68 0.065390\n#> 15  2 18.28 0.059580\n#> 16  2 18.80 0.054290\n#> 17  2 19.24 0.049460\n#> 18  2 19.63 0.045070\n#> 19  2 19.96 0.041070\n#> 20  2 20.25 0.037420\n#> 21  2 20.49 0.034090\n#> 22  2 20.70 0.031070\n#> 23  2 20.88 0.028310\n#> 24  2 21.04 0.025790\n#> 25  2 21.17 0.023500\n#> 26  2 21.28 0.021410\n#> 27  2 21.38 0.019510\n#> 28  2 21.46 0.017780\n#> 29  2 21.53 0.016200\n#> 30  2 21.58 0.014760\n#> 31  2 21.63 0.013450\n#> 32  2 21.67 0.012250\n#> 33  2 21.71 0.011160\n#> 34  2 21.74 0.010170\n#> 35  2 21.77 0.009269\n#> 36  2 21.79 0.008445\n#> 37  2 21.82 0.007695\n#> 38  2 21.83 0.007011\n#> 39  2 21.85 0.006389\n#> 40  2 21.86 0.005821\n#> 41  2 21.87 0.005304\n#> 42  2 21.88 0.004833\n#> 43  2 21.89 0.004403\n#> 44  2 21.89 0.004012\n#> 45  2 21.90 0.003656\n#> 46  2 21.90 0.003331\n#> 47  2 21.91 0.003035\n#> 48  2 21.91 0.002765\n#> 49  2 21.91 0.002520\n#> 50  2 21.91 0.002296\n#> 51  2 21.92 0.002092\n#> 52  2 21.92 0.001906\n#> 53  2 21.92 0.001737\n#> 54  2 21.92 0.001582\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(multinom_reg_fit, type = \"class\", new_data = mtl_test)\n#> # A tibble: 8 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 three      \n#> 2 three      \n#> 3 three      \n#> 4 one        \n#> 5 one        \n#> 6 two        \n#> 7 three      \n#> 8 one\npredict(multinom_reg_fit, type = \"prob\", new_data = mtl_test)\n#> # A tibble: 8 × 3\n#>   .pred_one .pred_two .pred_three\n#>       <dbl>     <dbl>       <dbl>\n#> 1   0.163     0.211        0.626 \n#> 2   0.318     0.185        0.496 \n#> 3   0.358     0.198        0.444 \n#> 4   0.976     0.00268      0.0217\n#> 5   0.940     0.00529      0.0544\n#> 6   0.00617   0.699        0.295 \n#> 7   0.0757    0.390        0.534 \n#> 8   0.506     0.0563       0.438\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_spec <- multinom_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)\nmultinom_reg_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OMultinomialModel: glm\n#> Model ID:  GLM_model_R_1763571327438_3935 \n#> GLM Model: summary\n#>        family        link                                regularization\n#> 1 multinomial multinomial Elastic Net (alpha = 0.5, lambda = 4.372E-4 )\n#>   number_of_predictors_total number_of_active_predictors number_of_iterations\n#> 1                          9                           6                    4\n#>      training_frame\n#> 1 object_avyvxbooiq\n#> \n#> Coefficients: glm multinomial coefficients\n#>       names coefs_class_0 coefs_class_1 coefs_class_2 std_coefs_class_0\n#> 1 Intercept     -1.119482     -0.831434     -1.706488         -1.083442\n#> 2         A     -1.119327      0.002894      0.750746         -1.029113\n#> 3         B     -1.208210      0.078752      0.162842         -1.187423\n#>   std_coefs_class_1 std_coefs_class_2\n#> 1         -0.819868         -1.830487\n#> 2          0.002661          0.690238\n#> 3          0.077397          0.160041\n#> \n#> H2OMultinomialMetrics: glm\n#> ** Reported on training data. **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> Extract training frame with `h2o.getFrame(\"object_avyvxbooiq\")`\n#> MSE: (Extract with `h2o.mse`) 0.2982118\n#> RMSE: (Extract with `h2o.rmse`) 0.5460878\n#> Logloss: (Extract with `h2o.logloss`) 0.822443\n#> Mean Per-Class Error: 0.4583896\n#> AUC: (Extract with `h2o.auc`) NaN\n#> AUCPR: (Extract with `h2o.aucpr`) NaN\n#> Null Deviance: (Extract with `h2o.nulldeviance`) 404.5036\n#> Residual Deviance: (Extract with `h2o.residual_deviance`) 315.8181\n#> R^2: (Extract with `h2o.r2`) 0.4682043\n#> AIC: (Extract with `h2o.aic`) NaN\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>        one three two  Error       Rate\n#> one     59    18   1 0.2436 =  19 / 78\n#> three   19    52   5 0.3158 =  24 / 76\n#> two      7    24   7 0.8158 =  31 / 38\n#> Totals  85    94  13 0.3854 = 74 / 192\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-3 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.614583\n#> 2 2  0.890625\n#> 3 3  1.000000\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(multinom_reg_fit, type = \"class\", new_data = mtl_test)\n#> # A tibble: 8 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 three      \n#> 2 three      \n#> 3 three      \n#> 4 one        \n#> 5 one        \n#> 6 two        \n#> 7 three      \n#> 8 one\npredict(multinom_reg_fit, type = \"prob\", new_data = mtl_test)\n#> # A tibble: 8 × 3\n#>   .pred_one .pred_three .pred_two\n#>       <dbl>       <dbl>     <dbl>\n#> 1   0.146        0.641    0.213  \n#> 2   0.308        0.513    0.179  \n#> 3   0.350        0.460    0.190  \n#> 4   0.983        0.0158   0.00128\n#> 5   0.955        0.0422   0.00284\n#> 6   0.00329      0.244    0.752  \n#> 7   0.0599       0.527    0.413  \n#> 8   0.521        0.432    0.0469\n```\n:::\n\n\n## `keras` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_spec <- multinom_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"keras\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)\nmultinom_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(multinom_reg_fit, type = \"class\", new_data = mtl_test)\npredict(multinom_reg_fit, type = \"prob\", new_data = mtl_test)\n```\n:::\n\n\n## `nnet` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and nnet is the default engine so there is no need to set that either.\nmultinom_reg_spec <- multinom_reg()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = mtl_train)\nmultinom_reg_fit\n#> parsnip model object\n#> \n#> Call:\n#> nnet::multinom(formula = class ~ ., data = data, trace = FALSE)\n#> \n#> Coefficients:\n#>       (Intercept)        A        B\n#> two    -0.5868435 1.881920 1.379106\n#> three   0.2910810 1.129622 1.292802\n#> \n#> Residual Deviance: 315.8164 \n#> AIC: 327.8164\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(multinom_reg_fit, type = \"class\", new_data = mtl_test)\n#> # A tibble: 8 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 three      \n#> 2 three      \n#> 3 three      \n#> 4 one        \n#> 5 one        \n#> 6 two        \n#> 7 three      \n#> 8 one\npredict(multinom_reg_fit, type = \"prob\", new_data = mtl_test)\n#> # A tibble: 8 × 3\n#>   .pred_one .pred_two .pred_three\n#>       <dbl>     <dbl>       <dbl>\n#> 1   0.145     0.213        0.641 \n#> 2   0.308     0.178        0.514 \n#> 3   0.350     0.189        0.461 \n#> 4   0.983     0.00123      0.0155\n#> 5   0.956     0.00275      0.0415\n#> 6   0.00318   0.754        0.243 \n#> 7   0.0591    0.414        0.527 \n#> 8   0.522     0.0465       0.431\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_spec <- multinom_reg() |> \n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultinom_reg_fit <- multinom_reg_spec |> fit(class ~ ., data = tbl_mtl$training)\nmultinom_reg_fit\n#> parsnip model object\n#> \n#> Formula: class ~ .\n#> \n#> Coefficients:\n#>       (Intercept)          A          B\n#> one    0.05447853 -1.0569131 -0.9049194\n#> three  0.41207949  0.1458870  0.3959664\n#> two   -0.46655802  0.9110261  0.5089529\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(multinom_reg_fit, type = \"class\", new_data = tbl_mtl$test)\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>   pred_class\n#>   <chr>     \n#> 1 one       \n#> 2 one       \n#> 3 three     \n#> 4 three     \n#> 5 three     \n#> 6 three     \n#> 7 three\npredict(multinom_reg_fit, type = \"prob\", new_data = tbl_mtl$test)\n#> # Source:   SQL [?? x 3]\n#> # Database: spark_connection\n#>   pred_one pred_three pred_two\n#>      <dbl>      <dbl>    <dbl>\n#> 1   0.910      0.0814  0.00904\n#> 2   0.724      0.233   0.0427 \n#> 3   0.124      0.620   0.256  \n#> 4   0.0682     0.610   0.322  \n#> 5   0.130      0.571   0.300  \n#> 6   0.115      0.549   0.336  \n#> 7   0.0517     0.524   0.424\n```\n:::\n\n\n\n## Naive Bayes (`naive_Bayes()`) \n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnaive_Bayes_spec <- naive_Bayes() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnaive_Bayes_fit <- naive_Bayes_spec |> fit(class ~ ., data = bin_train)\nnaive_Bayes_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: naivebayes\n#> Model ID:  NaiveBayes_model_R_1763571327438_3936 \n#> Model Summary: \n#>   number_of_response_levels min_apriori_probability max_apriori_probability\n#> 1                         2                 0.44713                 0.55287\n#> \n#> \n#> H2OBinomialMetrics: naivebayes\n#> ** Reported on training data. **\n#> \n#> MSE:  0.1737113\n#> RMSE:  0.4167869\n#> LogLoss:  0.5473431\n#> Mean Per-Class Error:  0.2356138\n#> AUC:  0.8377152\n#> AUCPR:  0.788608\n#> Gini:  0.6754303\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error      Rate\n#> Class1    274    160 0.368664  =160/434\n#> Class2     36    315 0.102564   =36/351\n#> Totals    310    475 0.249682  =196/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.175296   0.762712 286\n#> 2                       max f2  0.133412   0.851119 306\n#> 3                 max f0point5  0.497657   0.731343 183\n#> 4                 max accuracy  0.281344   0.765605 248\n#> 5                max precision  0.999709   1.000000   0\n#> 6                   max recall  0.020983   1.000000 390\n#> 7              max specificity  0.999709   1.000000   0\n#> 8             max absolute_mcc  0.280325   0.541898 249\n#> 9   max min_per_class_accuracy  0.398369   0.758065 215\n#> 10 max mean_per_class_accuracy  0.280325   0.771945 249\n#> 11                     max tns  0.999709 434.000000   0\n#> 12                     max fns  0.999709 347.000000   0\n#> 13                     max fps  0.006522 434.000000 399\n#> 14                     max tps  0.020983 351.000000 390\n#> 15                     max tnr  0.999709   1.000000   0\n#> 16                     max fnr  0.999709   0.988604   0\n#> 17                     max fpr  0.006522   1.000000 399\n#> 18                     max tpr  0.020983   1.000000 390\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(naive_Bayes_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class2     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class2\npredict(naive_Bayes_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.181      0.819  \n#> 2        0.750      0.250  \n#> 3        0.556      0.444  \n#> 4        0.994      0.00643\n#> 5        0.967      0.0331 \n#> 6        0.630      0.370\n```\n:::\n\n\n## `klaR` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and klaR is the default engine so there is no need to set that either.\nnaive_Bayes_spec <- naive_Bayes()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnaive_Bayes_fit <- naive_Bayes_spec |> fit(class ~ ., data = bin_train)\n\n# No real print method\n# naive_Bayes_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(naive_Bayes_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(naive_Bayes_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.250      0.750  \n#> 2        0.593      0.407  \n#> 3        0.333      0.667  \n#> 4        0.993      0.00658\n#> 5        0.978      0.0223 \n#> 6        0.531      0.469\n```\n:::\n\n\n## `naivebayes` Engine \n\nThis engine requires the discrim extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnaive_Bayes_spec <- naive_Bayes() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"naivebayes\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnaive_Bayes_fit <- naive_Bayes_spec |> fit(class ~ ., data = bin_train)\nnaive_Bayes_fit\n#> parsnip model object\n#> \n#> \n#> ================================= Naive Bayes ==================================\n#> \n#> Call:\n#> naive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE)\n#> \n#> -------------------------------------------------------------------------------- \n#>  \n#> Laplace smoothing: 0\n#> \n#> -------------------------------------------------------------------------------- \n#>  \n#> A priori probabilities: \n#> \n#>    Class1    Class2 \n#> 0.5528662 0.4471338 \n#> \n#> -------------------------------------------------------------------------------- \n#>  \n#> Tables: \n#> \n#> -------------------------------------------------------------------------------- \n#> :: A::Class1 (KDE)\n#> -------------------------------------------------------------------------------- \n#> \n#> Call:\n#> \tdensity.default(x = x, na.rm = TRUE)\n#> \n#> Data: x (434 obs.);\tBandwidth 'bw' = 0.2548\n#> \n#>        x                 y            \n#>  Min.   :-2.5638   Min.   :0.0002915  \n#>  1st Qu.:-1.2013   1st Qu.:0.0506201  \n#>  Median : 0.1612   Median :0.1619843  \n#>  Mean   : 0.1612   Mean   :0.1831190  \n#>  3rd Qu.: 1.5237   3rd Qu.:0.2581668  \n#>  Max.   : 2.8862   Max.   :0.5370762  \n#> -------------------------------------------------------------------------------- \n#> :: A::Class2 (KDE)\n#> -------------------------------------------------------------------------------- \n#> \n#> Call:\n#> \tdensity.default(x = x, na.rm = TRUE)\n#> \n#> Data: x (351 obs.);\tBandwidth 'bw' = 0.2596\n#> \n#>        x                 y            \n#>  Min.   :-2.5428   Min.   :4.977e-05  \n#>  1st Qu.:-1.1840   1st Qu.:2.672e-02  \n#>  Median : 0.1748   Median :2.239e-01  \n#>  Mean   : 0.1748   Mean   :1.836e-01  \n#>  3rd Qu.: 1.5336   3rd Qu.:2.926e-01  \n#>  Max.   : 2.8924   Max.   :3.740e-01  \n#> \n#> -------------------------------------------------------------------------------- \n#> :: B::Class1 (KDE)\n#> -------------------------------------------------------------------------------- \n#> \n#> Call:\n#> \tdensity.default(x = x, na.rm = TRUE)\n#> \n#> Data: x (434 obs.);\tBandwidth 'bw' = 0.1793\n#> \n#>        x                 y            \n#>  Min.   :-2.4501   Min.   :5.747e-05  \n#>  1st Qu.:-1.0894   1st Qu.:1.424e-02  \n#>  Median : 0.2713   Median :8.798e-02  \n#>  Mean   : 0.2713   Mean   :1.834e-01  \n#>  3rd Qu.: 1.6320   3rd Qu.:2.758e-01  \n#>  Max.   : 2.9927   Max.   :6.872e-01  \n#> \n#> -------------------------------------------------------------------------------- \n#> :: B::Class2 (KDE)\n#> -------------------------------------------------------------------------------- \n#> \n#> Call:\n#> \tdensity.default(x = x, na.rm = TRUE)\n#> \n#> Data: x (351 obs.);\tBandwidth 'bw' = 0.2309\n#> \n#>        x                 y            \n#>  Min.   :-2.4621   Min.   :5.623e-05  \n#>  1st Qu.:-0.8979   1st Qu.:1.489e-02  \n#>  Median : 0.6663   Median :7.738e-02  \n#>  Mean   : 0.6663   Mean   :1.595e-01  \n#>  3rd Qu.: 2.2305   3rd Qu.:3.336e-01  \n#>  Max.   : 3.7948   Max.   :4.418e-01  \n#> \n#> --------------------------------------------------------------------------------\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(naive_Bayes_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(naive_Bayes_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.249      0.751  \n#> 2        0.593      0.407  \n#> 3        0.332      0.668  \n#> 4        0.993      0.00674\n#> 5        0.978      0.0224 \n#> 6        0.532      0.468\n```\n:::\n\n\n## K-Nearest Neighbors (`nearest_neighbor()`) \n\n## `kknn` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnearest_neighbor_spec <- nearest_neighbor() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and kknn is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnearest_neighbor_fit <- nearest_neighbor_spec |> fit(class ~ ., data = bin_train)\nnearest_neighbor_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> kknn::train.kknn(formula = class ~ ., data = data, ks = min_rows(5,     data, 5))\n#> \n#> Type of response variable: nominal\n#> Minimal misclassification: 0.2101911\n#> Best kernel: optimal\n#> Best k: 5\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(nearest_neighbor_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(nearest_neighbor_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1         0.2          0.8 \n#> 2         0.72         0.28\n#> 3         0.32         0.68\n#> 4         1            0   \n#> 5         1            0   \n#> 6         1            0\n```\n:::\n\n\n## Null Model (`null_model()`) \n\n## `parsnip` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnull_model_spec <- null_model() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and parsnip is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnull_model_fit <- null_model_spec |> fit(class ~ ., data = bin_train)\nnull_model_fit\n#> parsnip model object\n#> \n#> Null Regression Model\n#> Predicted Value: Class1\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(null_model_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class1     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(null_model_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.553        0.447\n#> 2        0.553        0.447\n#> 3        0.553        0.447\n#> 4        0.553        0.447\n#> 5        0.553        0.447\n#> 6        0.553        0.447\n```\n:::\n\n\n## Partial Least Squares (`pls()`) \n\n## `mixOmics` Engine \n\nThis engine requires the plsmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(plsmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_spec <- pls() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and mixOmics is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_fit <- pls_spec |> fit(class ~ ., data = bin_train)\npls_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#>  mixOmics::splsda(X = x, Y = y, ncomp = ncomp, keepX = keepX) \n#> \n#>  sPLS-DA (regression mode) with 2 sPLS-DA components. \n#>  You entered data X of dimensions: 785 2 \n#>  You entered data Y with 2 classes. \n#> \n#>  Selection of [2] [2] variables on each of the sPLS-DA components on the X data set. \n#>  No Y variables can be selected. \n#> \n#>  Main numerical outputs: \n#>  -------------------- \n#>  loading vectors: see object$loadings \n#>  variates: see object$variates \n#>  variable names: see object$names \n#> \n#>  Functions to visualise samples: \n#>  -------------------- \n#>  plotIndiv, plotArrow, cim \n#> \n#>  Functions to visualise variables: \n#>  -------------------- \n#>  plotVar, plotLoadings, network, cim \n#> \n#>  Other functions: \n#>  -------------------- \n#>  selectVar, tune, perf, auc\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(pls_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(pls_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.462        0.538\n#> 2        0.631        0.369\n#> 3        0.512        0.488\n#> 4        0.765        0.235\n#> 5        0.675        0.325\n#> 6        0.624        0.376\n```\n:::\n\n\n## Random Forests (`rand_forest()`) \n\n## `aorsf` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"aorsf\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> ---------- Oblique random classification forest\n#> \n#>      Linear combinations: Accelerated Logistic regression\n#>           N observations: 785\n#>                N classes: 2\n#>                  N trees: 500\n#>       N predictors total: 2\n#>    N predictors per node: 2\n#>  Average leaves per tree: 24.166\n#> Min observations in leaf: 5\n#>           OOB stat value: 0.87\n#>            OOB stat type: AUC-ROC\n#>      Variable importance: anova\n#> \n#> -----------------------------------------\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rand_forest_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.199       0.801 \n#> 2        0.882       0.118 \n#> 3        0.361       0.639 \n#> 4        0.978       0.0220\n#> 5        0.936       0.0642\n#> 6        0.904       0.0957\n```\n:::\n\n\n## `grf` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"grf\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)\nrand_forest_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = bin_test)\npredict(rand_forest_fit, type = \"prob\", new_data = bin_test)\npredict(rand_forest_fit, type = \"conf_int\", new_data = bin_test)\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: drf\n#> Model ID:  DRF_model_R_1763571327438_3938 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              50                       50               91643        13\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1        20   16.38000        114        158   141.50000\n#> \n#> \n#> H2OBinomialMetrics: drf\n#> ** Reported on training data. **\n#> ** Metrics reported on Out-Of-Bag training samples **\n#> \n#> MSE:  0.1644052\n#> RMSE:  0.4054691\n#> LogLoss:  1.62537\n#> Mean Per-Class Error:  0.2084695\n#> AUC:  0.8379252\n#> AUCPR:  0.7897947\n#> Gini:  0.6758504\n#> R^2:  0.3349444\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error      Rate\n#> Class1    326    108 0.248848  =108/434\n#> Class2     59    292 0.168091   =59/351\n#> Totals    385    400 0.212739  =167/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.363636   0.777630 128\n#> 2                       max f2  0.263158   0.827455 147\n#> 3                 max f0point5  0.642857   0.762215  78\n#> 4                 max accuracy  0.384615   0.787261 125\n#> 5                max precision  0.944444   0.876033  10\n#> 6                   max recall  0.000000   1.000000 217\n#> 7              max specificity  1.000000   0.972350   0\n#> 8             max absolute_mcc  0.363636   0.579899 128\n#> 9   max min_per_class_accuracy  0.458333   0.780627 112\n#> 10 max mean_per_class_accuracy  0.363636   0.791530 128\n#> 11                     max tns  1.000000 422.000000   0\n#> 12                     max fns  1.000000 275.000000   0\n#> 13                     max fps  0.000000 434.000000 217\n#> 14                     max tps  0.000000 351.000000 217\n#> 15                     max tnr  1.000000   0.972350   0\n#> 16                     max fnr  1.000000   0.783476   0\n#> 17                     max fpr  0.000000   1.000000 217\n#> 18                     max tpr  0.000000   1.000000 217\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rand_forest_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1         0.12         0.88\n#> 2         0.88         0.12\n#> 3         0.11         0.89\n#> 4         1            0   \n#> 5         0.76         0.24\n#> 6         1            0\n```\n:::\n\n\n## `partykit` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"partykit\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)\n\n# Too long to print\n# rand_forest_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rand_forest_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.396       0.604 \n#> 2        0.804       0.196 \n#> 3        0.313       0.687 \n#> 4        0.966       0.0343\n#> 5        0.887       0.113 \n#> 6        0.931       0.0689\n```\n:::\n\n\n## `randomForest` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"randomForest\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#>  randomForest(x = maybe_data_frame(x), y = y) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 1\n#> \n#>         OOB estimate of  error rate: 21.66%\n#> Confusion matrix:\n#>        Class1 Class2 class.error\n#> Class1    348     86   0.1981567\n#> Class2     84    267   0.2393162\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rand_forest_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.174        0.826\n#> 2        0.88         0.12 \n#> 3        0.112        0.888\n#> 4        1            0    \n#> 5        0.692        0.308\n#> 6        0.922        0.078\n```\n:::\n\n\n## `ranger` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and ranger is the default engine so there is no need to set that either.\n  set_engine(\"ranger\", keep.inbag = TRUE) |> \n  # However, we'll set the engine and use the keep.inbag=TRUE option so that we \n  # can produce interval predictions. This is not generally required. \n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(class ~ ., data = bin_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, keep.inbag = ~TRUE,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  500 \n#> Sample size:                      785 \n#> Number of independent variables:  2 \n#> Mtry:                             1 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1486808\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rand_forest_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.228       0.772 \n#> 2        0.828       0.172 \n#> 3        0.214       0.786 \n#> 4        0.942       0.0578\n#> 5        0.763       0.237 \n#> 6        0.900       0.100\npredict(rand_forest_fit, type = \"conf_int\", new_data = bin_test)\n#> Warning in rInfJack(x, inbag.counts): Sample size <=20, no calibration\n#> performed.\n#> Warning in rInfJack(x, inbag.counts): Sample size <=20, no calibration\n#> performed.\n#> # A tibble: 6 × 4\n#>   .pred_lower_Class1 .pred_upper_Class1 .pred_lower_Class2 .pred_upper_Class2\n#>                <dbl>              <dbl>              <dbl>              <dbl>\n#> 1              0                  0.510            0.490                1    \n#> 2              0.660              0.997            0.00288              0.340\n#> 3              0                  0.461            0.539                1    \n#> 4              0.798              1                0                    0.202\n#> 5              0.567              0.959            0.0408               0.433\n#> 6              0.745              1                0                    0.255\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  set_mode(\"classification\") |>\n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(Class ~ ., data = tbl_bin$training)\nrand_forest_fit\n#> parsnip model object\n#> \n#> Formula: Class ~ .\n#> \n#> RandomForestClassificationModel: uid=random_forest__3204ae4e_77ac_4f0c_b642_fef909ba5c81, numTrees=20, numClasses=2, numFeatures=2\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"class\", new_data = tbl_bin$test)\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>   pred_class\n#>   <chr>     \n#> 1 Class2    \n#> 2 Class2    \n#> 3 Class1    \n#> 4 Class2    \n#> 5 Class2    \n#> 6 Class1    \n#> 7 Class2\npredict(rand_forest_fit, type = \"prob\", new_data = tbl_bin$test)\n#> # Source:   SQL [?? x 2]\n#> # Database: spark_connection\n#>   pred_Class1 pred_Class2\n#>         <dbl>       <dbl>\n#> 1      0.244       0.756 \n#> 2      0.249       0.751 \n#> 3      0.836       0.164 \n#> 4      0.227       0.773 \n#> 5      0.260       0.740 \n#> 6      0.962       0.0383\n#> 7      0.0937      0.906\n```\n:::\n\n\n## Rule Fit (`rule_fit()`) \n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_spec <- rule_fit() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_fit <- rule_fit_spec |> fit(class ~ ., data = bin_train)\nrule_fit_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: rulefit\n#> Model ID:  RuleFit_model_R_1763571327438_3989 \n#> Rulefit Model Summary: \n#>     family  link            regularization number_of_predictors_total\n#> 1 binomial logit Lasso (lambda = 0.03081 )                       2377\n#>   number_of_active_predictors number_of_iterations rule_ensemble_size\n#> 1                           4                    5               2375\n#>   number_of_trees number_of_internal_trees min_depth max_depth mean_depth\n#> 1             150                      150         0         5    4.00000\n#>   min_leaves max_leaves mean_leaves\n#> 1          0         31    15.83333\n#> \n#> \n#> H2OBinomialMetrics: rulefit\n#> ** Reported on training data. **\n#> \n#> MSE:  0.1422931\n#> RMSE:  0.3772176\n#> LogLoss:  0.4500322\n#> Mean Per-Class Error:  0.1867902\n#> AUC:  0.8764064\n#> AUCPR:  0.8338422\n#> Gini:  0.7528129\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>        Class1 Class2    Error      Rate\n#> Class1    351     83 0.191244   =83/434\n#> Class2     64    287 0.182336   =64/351\n#> Totals    415    370 0.187261  =147/785\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.485283   0.796117 204\n#> 2                       max f2  0.263811   0.861522 270\n#> 3                 max f0point5  0.620200   0.799574 147\n#> 4                 max accuracy  0.485283   0.812739 204\n#> 5                max precision  0.984770   1.000000   0\n#> 6                   max recall  0.048801   1.000000 393\n#> 7              max specificity  0.984770   1.000000   0\n#> 8             max absolute_mcc  0.485283   0.623934 204\n#> 9   max min_per_class_accuracy  0.489555   0.808756 202\n#> 10 max mean_per_class_accuracy  0.485283   0.813210 204\n#> 11                     max tns  0.984770 434.000000   0\n#> 12                     max fns  0.984770 350.000000   0\n#> 13                     max fps  0.037559 434.000000 399\n#> 14                     max tps  0.048801 351.000000 393\n#> 15                     max tnr  0.984770   1.000000   0\n#> 16                     max fnr  0.984770   0.997151   0\n#> 17                     max fpr  0.037559   1.000000 399\n#> 18                     max tpr  0.048801   1.000000 393\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rule_fit_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rule_fit_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.377       0.623 \n#> 2        0.737       0.263 \n#> 3        0.487       0.513 \n#> 4        0.956       0.0440\n#> 5        0.879       0.121 \n#> 6        0.693       0.307\n```\n:::\n\n\n## `xrf` Engine \n\nThis engine requires the rules extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rules)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_spec <- rule_fit() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and xrf is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_fit <- rule_fit_spec |> fit(class ~ ., data = bin_train)\nrule_fit_fit\n#> parsnip model object\n#> \n#> An eXtreme RuleFit model of 358 rules.\n#> \n#> Original Formula:\n#> \n#> class ~ A + B\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rule_fit_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(rule_fit_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.419        0.581\n#> 2        0.651        0.349\n#> 3        0.506        0.494\n#> 4        0.891        0.109\n#> 5        0.805        0.195\n#> 6        0.616        0.384\n```\n:::\n\n\n## Support Vector Machine (Linear Kernel) (`svm_linear()`) \n\n## `kernlab` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_spec <- svm_linear() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"kernlab\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_fit <- svm_linear_spec |> fit(class ~ ., data = bin_train)\nsvm_linear_fit\n#> parsnip model object\n#> \n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: C-svc  (classification) \n#>  parameter : cost C = 1 \n#> \n#> Linear (vanilla) kernel function. \n#> \n#> Number of Support Vectors : 357 \n#> \n#> Objective Function Value : -353.0043 \n#> Training error : 0.17707 \n#> Probability model included.\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_linear_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(svm_linear_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.403       0.597 \n#> 2        0.858       0.142 \n#> 3        0.540       0.460 \n#> 4        0.975       0.0254\n#> 5        0.905       0.0949\n#> 6        0.849       0.151\n```\n:::\n\n\n## `LiblineaR` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_spec <- svm_linear() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and LiblineaR is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_fit <- svm_linear_spec |> fit(class ~ ., data = bin_train)\nsvm_linear_fit\n#> parsnip model object\n#> \n#> $TypeDetail\n#> [1] \"L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)\"\n#> \n#> $Type\n#> [1] 1\n#> \n#> $W\n#>              A          B      Bias\n#> [1,] 0.3641925 -0.9648581 0.1182515\n#> \n#> $Bias\n#> [1] 1\n#> \n#> $ClassNames\n#> [1] Class1 Class2\n#> Levels: Class1 Class2\n#> \n#> $NbClass\n#> [1] 2\n#> \n#> attr(,\"class\")\n#> [1] \"LiblineaR\"\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_linear_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\n```\n:::\n\n\n## Support Vector Machine (Polynomial Kernel) (`svm_poly()`) \n\n## `kernlab` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_poly_spec <- svm_poly() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and kernlab is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_poly_fit <- svm_poly_spec |> fit(class ~ ., data = bin_train)\n#>  Setting default kernel parameters\nsvm_poly_fit\n#> parsnip model object\n#> \n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: C-svc  (classification) \n#>  parameter : cost C = 1 \n#> \n#> Polynomial kernel function. \n#>  Hyperparameters : degree =  1  scale =  1  offset =  1 \n#> \n#> Number of Support Vectors : 357 \n#> \n#> Objective Function Value : -353.0043 \n#> Training error : 0.17707 \n#> Probability model included.\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_poly_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class1     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(svm_poly_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.412       0.588 \n#> 2        0.863       0.137 \n#> 3        0.549       0.451 \n#> 4        0.976       0.0242\n#> 5        0.909       0.0912\n#> 6        0.855       0.145\n```\n:::\n\n\n## Support Vector Machine (Radial Basis Function Kernel) (`svm_rbf()`) \n\n## `kernlab` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_spec <- svm_rbf() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and kernlab is the default engine so there is no need to set that either.\n  set_mode(\"classification\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_fit <- svm_rbf_spec |> fit(class ~ ., data = bin_train)\nsvm_rbf_fit\n#> parsnip model object\n#> \n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: C-svc  (classification) \n#>  parameter : cost C = 1 \n#> \n#> Gaussian Radial Basis kernel function. \n#>  Hyperparameter : sigma =  2.60157241724157 \n#> \n#> Number of Support Vectors : 338 \n#> \n#> Objective Function Value : -292.4523 \n#> Training error : 0.170701 \n#> Probability model included.\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_rbf_fit, type = \"class\", new_data = bin_test)\n#> # A tibble: 6 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 Class2     \n#> 2 Class1     \n#> 3 Class2     \n#> 4 Class1     \n#> 5 Class1     \n#> 6 Class1\npredict(svm_rbf_fit, type = \"prob\", new_data = bin_test)\n#> # A tibble: 6 × 2\n#>   .pred_Class1 .pred_Class2\n#>          <dbl>        <dbl>\n#> 1        0.524        0.476\n#> 2        0.893        0.107\n#> 3        0.239        0.761\n#> 4        0.866        0.134\n#> 5        0.867        0.133\n#> 6        0.876        0.124\n```\n:::\n\n\n## `liquidSVM` Engine \n\nNote that this package is not on CRAN. You can install it via its :\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npak::pak(\"cran/liquidSVM\") # fails\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_spec <- svm_rbf() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"classification\") |>\n  set_engine(\"liquidSVM\")\n#> Warning: The `engine` argument of `set_engine()` cannot be liquidSVM as of\n#> parsnip 0.1.6.\n#> ℹ The liquidSVM package is no longer available on CRAN.\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_fit <- svm_rbf_spec |> fit(class ~ ., data = bin_train)\nsvm_rbf_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_rbf_fit, type = \"class\", new_data = bin_test)\npredict(svm_rbf_fit, type = \"prob\", new_data = bin_test)\n```\n:::\n\n\n# Regression Models\n\n\nTo demonstrate regression, we'll subset some data. make a training/test split, and standardize the predictors: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(938)\nreg_split <-\n  modeldata::concrete |> \n  slice_sample(n = 100) |> \n  select(strength = compressive_strength, cement, age) |> \n  initial_split(prop = 0.95, strata = strength)\nreg_split\n#> <Training/Testing/Total>\n#> <92/8/100>\n\nreg_rec <- \n  recipe(strength ~ ., data = training(reg_split)) |> \n  step_normalize(all_numeric_predictors()) |> \n  prep()\n\nreg_train <- bake(reg_rec, new_data = NULL)\nreg_test <- bake(reg_rec, new_data = testing(reg_split))\n```\n:::\n\n\nWe also have some models that are specific to integer count outcomes. The data for these are:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(207)\ncount_split <-\n  attrition |>\n  select(num_years = TotalWorkingYears, age = Age, income = MonthlyIncome) |>\n  initial_split(prop = 0.994)\ncount_split\n#> <Training/Testing/Total>\n#> <1461/9/1470>\n\ncount_rec <-\n  recipe(num_years ~ ., data = training(count_split)) |>\n  step_normalize(all_numeric_predictors()) |>\n  prep()\n\ncount_train <- bake(count_rec, new_data = NULL)\ncount_test <- bake(count_rec, new_data = testing(count_split))\n```\n:::\n\n\nIf using the **Apache Spark** engine, we will need to identify the data source, \nand then use it to create the splits. For this article, we will copy the \n`concrete` data set into the Spark session.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntbl_concrete <- copy_to(sc, modeldata::concrete)\n\ntbl_reg <- sdf_random_split(tbl_concrete, training = 0.95, test = 0.05, seed = 100)\n```\n:::\n\n\n\n## Auto Ml (`auto_ml()`) \n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_ml_spec <- auto_ml() |>\n  # We dont need to set the engine (since there is only one) but we'll set\n  # a time limit\n  set_engine(\"h2o\", max_runtime_secs = 60 * 3) |> \n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_ml_fit <- auto_ml_spec |> fit(strength ~ ., data = reg_train)\nauto_ml_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(auto_ml_fit, new_data = reg_test)\n```\n:::\n\n\n## Bagged MARS (`bag_mars()`) \n\n## `earth` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mars_spec <- bag_mars() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and earth is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mars_fit <- bag_mars_spec |> fit(strength ~ ., data = reg_train)\nbag_mars_fit\n#> parsnip model object\n#> \n#> Bagged MARS (regression with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term   value std.error  used\n#>   <chr>  <dbl>     <dbl> <int>\n#> 1 age     86.9      5.54    11\n#> 2 cement  76.6      5.73    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_mars_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  21.5\n#> 2  41.3\n#> 3  27.3\n#> 4  56.6\n#> 5  35.9\n#> 6  36.5\n#> 7  38.5\n#> 8  38.2\n```\n:::\n\n\n## Bagged Neural Networks (`bag_mlp()`) \n\n## `nnet` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mlp_spec <- bag_mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and nnet is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_mlp_fit <- bag_mlp_spec |> fit(strength ~ ., data = reg_train)\nbag_mlp_fit\n#> parsnip model object\n#> \n#> Bagged nnet (regression with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term   value std.error  used\n#>   <chr>  <dbl>     <dbl> <int>\n#> 1 age     59.3      1.66    11\n#> 2 cement  40.7      1.66    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_mlp_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  22.7\n#> 2  42.0\n#> 3  27.8\n#> 4  76.0\n#> 5  37.3\n#> 6  39.0\n#> 7  35.9\n#> 8  42.4\n```\n:::\n\n\n## Bagged Decision Trees (`bag_tree()`) \n\n## `rpart` Engine \n\nThis engine requires the baguette extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_spec <- bag_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and rpart is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_fit <- bag_tree_spec |> fit(strength ~ ., data = reg_train)\nbag_tree_fit\n#> parsnip model object\n#> \n#> Bagged CART (regression with 11 members)\n#> \n#> Variable importance scores include:\n#> \n#> # A tibble: 2 × 4\n#>   term    value std.error  used\n#>   <chr>   <dbl>     <dbl> <int>\n#> 1 cement 17674.     1795.    11\n#> 2 age    12753.      489.    11\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  24.0\n#> 2  32.4\n#> 3  29.7\n#> 4  58.0\n#> 5  37.8\n#> 6  44.4\n#> 7  42.5\n#> 8  38.2\n```\n:::\n\n\n## Bayesian Additive Regression Trees (`bart()`) \n\n## `dbarts` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbart_spec <- bart() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and dbarts is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbart_fit <- bart_spec |> fit(strength ~ ., data = reg_train)\nbart_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> `NULL`()\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bart_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  24.2\n#> 2  41.0\n#> 3  26.5\n#> 4  52.6\n#> 5  36.0\n#> 6  36.8\n#> 7  39.1\n#> 8  37.9\npredict(bart_fit, type = \"conf_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        16.7        32.0\n#> 2        33.0        49.2\n#> 3        20.5        31.5\n#> 4        41.8        63.5\n#> 5        28.1        43.9\n#> 6        30.2        42.6\n#> 7        33.3        45.3\n#> 8        27.2        50.0\npredict(bart_fit, type = \"pred_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        4.90        44.3\n#> 2       22.5         60.4\n#> 3        8.62        44.8\n#> 4       35.0         71.9\n#> 5       16.6         53.3\n#> 6       19.9         54.5\n#> 7       22.5         57.3\n#> 8       16.4         58.6\n```\n:::\n\n\n## Boosted Decision Trees (`boost_tree()`) \n\n## `catboost` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"catboost\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> CatBoost model (1000 trees)\n#> Loss function: RMSE\n#> Fit to 2 feature(s)\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  25.3\n#> 2  33.9\n#> 3  28.1\n#> 4  60.7\n#> 5  35.4\n#> 6  38.2\n#> 7  43.3\n#> 8  29.8\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"h2o_gbm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: gbm\n#> Model ID:  GBM_model_R_1763571327438_4145 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              50                       50               20476         6\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1         6    6.00000         14         43    27.92000\n#> \n#> \n#> H2ORegressionMetrics: gbm\n#> ** Reported on training data. **\n#> \n#> MSE:  0.001563879\n#> RMSE:  0.03954591\n#> MAE:  0.02903684\n#> RMSLE:  0.001771464\n#> Mean Residual Deviance :  0.001563879\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  29.7\n#> 2  32.2\n#> 3  26.9\n#> 4  63.2\n#> 5  34.9\n#> 6  39.0\n#> 7  40.0\n#> 8  32.9\n```\n:::\n\n\n## `h2o_gbm` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"h2o_gbm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: gbm\n#> Model ID:  GBM_model_R_1763571327438_4146 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              50                       50               20476         6\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1         6    6.00000         14         43    27.92000\n#> \n#> \n#> H2ORegressionMetrics: gbm\n#> ** Reported on training data. **\n#> \n#> MSE:  0.001563879\n#> RMSE:  0.03954591\n#> MAE:  0.02903684\n#> RMSLE:  0.001771464\n#> Mean Residual Deviance :  0.001563879\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  29.7\n#> 2  32.2\n#> 3  26.9\n#> 4  63.2\n#> 5  34.9\n#> 6  39.0\n#> 7  40.0\n#> 8  32.9\n```\n:::\n\n\n## `lightgbm` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"lightgbm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> LightGBM Model (100 trees)\n#> Objective: regression\n#> Fitted to dataset with 2 columns\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  20.6\n#> 2  42.5\n#> 3  27.0\n#> 4  49.2\n#> 5  43.7\n#> 6  38.3\n#> 7  41.1\n#> 8  36.9\n```\n:::\n\n\n## `xgboost` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and xgboost is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(strength ~ ., data = reg_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> ##### xgb.Booster\n#> raw: 35 Kb \n#> call:\n#>   xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n#>     colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n#>     subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n#>     verbose = 0, nthread = 1, objective = \"reg:squarederror\")\n#> params (as set within xgb.train):\n#>   eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\n#> xgb.attributes:\n#>   niter\n#> callbacks:\n#>   cb.evaluation.log()\n#> # of features: 2 \n#> niter: 15\n#> nfeatures : 2 \n#> evaluation_log:\n#>   iter training_rmse\n#>  <num>         <num>\n#>      1     27.511751\n#>      2     20.726236\n#>    ---           ---\n#>     14      2.774394\n#>     15      2.632224\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  22.3\n#> 2  32.9\n#> 3  26.7\n#> 4  57.6\n#> 5  34.9\n#> 6  33.8\n#> 7  42.6\n#> 8  26.3\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |>\n  set_mode(\"regression\") |>\n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)\nboost_tree_fit\n#> parsnip model object\n#> \n#> Formula: compressive_strength ~ .\n#> \n#> GBTRegressionModel: uid=gradient_boosted_trees__d4414e35_351c_433f_958b_847ee38e9416, numTrees=20, numFeatures=8\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, new_data = tbl_reg$test)\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>     pred\n#>    <dbl>\n#>  1 20.8 \n#>  2 28.1 \n#>  3 15.5 \n#>  4 22.4 \n#>  5  9.37\n#>  6 40.1 \n#>  7 14.2 \n#>  8 32.1 \n#>  9 37.4 \n#> 10 49.5 \n#> # ℹ more rows\n```\n:::\n\n\n## Cubist Rules (`cubist_rules()`) \n\n## `Cubist` Engine \n\nThis engine requires the rules extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rules)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and Cubist is the default engine so there is no need to set that either.\ncubist_rules_spec <- cubist_rules()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncubist_rules_fit <- cubist_rules_spec |> fit(strength ~ ., data = reg_train)\ncubist_rules_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> cubist.default(x = x, y = y, committees = 1)\n#> \n#> Number of samples: 92 \n#> Number of predictors: 2 \n#> \n#> Number of committees: 1 \n#> Number of rules: 2\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(cubist_rules_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  24.2\n#> 2  46.3\n#> 3  23.6\n#> 4  54.4\n#> 5  32.7\n#> 6  37.8\n#> 7  38.8\n#> 8  38.6\n```\n:::\n\n\n## Decision Tree (`decision_tree()`) \n\n## `partykit` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"partykit\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(strength ~ ., data = reg_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> \n#> Model formula:\n#> strength ~ cement + age\n#> \n#> Fitted party:\n#> [1] root\n#> |   [2] cement <= 0.72078\n#> |   |   [3] age <= -0.60316\n#> |   |   |   [4] cement <= -0.38732: 11.141 (n = 12, err = 292.8)\n#> |   |   |   [5] cement > -0.38732: 18.005 (n = 11, err = 401.5)\n#> |   |   [6] age > -0.60316\n#> |   |   |   [7] cement <= 0.24945\n#> |   |   |   |   [8] age <= -0.2359: 28.756 (n = 24, err = 1450.6)\n#> |   |   |   |   [9] age > -0.2359: 39.014 (n = 11, err = 634.8)\n#> |   |   |   [10] cement > 0.24945: 42.564 (n = 11, err = 1041.7)\n#> |   [11] cement > 0.72078: 50.864 (n = 23, err = 5390.3)\n#> \n#> Number of inner nodes:    5\n#> Number of terminal nodes: 6\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  18.0\n#> 2  39.0\n#> 3  28.8\n#> 4  50.9\n#> 5  50.9\n#> 6  42.6\n#> 7  42.6\n#> 8  50.9\n```\n:::\n\n\n## `rpart` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and rpart is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(strength ~ ., data = reg_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> n= 92 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 92 26564.7400 33.57728  \n#>    2) cement< 0.7861846 69 12009.9000 27.81493  \n#>      4) age< -0.5419541 23   964.6417 14.42348  \n#>        8) cement< -0.3695209 12   292.7811 11.14083 *\n#>        9) cement>=-0.3695209 11   401.4871 18.00455 *\n#>      5) age>=-0.5419541 46  4858.3440 34.51065  \n#>       10) age< 0.008934354 32  2208.3040 31.16781  \n#>         20) cement< 0.311975 24  1450.6200 28.75583 *\n#>         21) cement>=0.311975 8   199.1900 38.40375 *\n#>       11) age>=0.008934354 14  1475.1130 42.15143 *\n#>    3) cement>=0.7861846 23  5390.3320 50.86435  \n#>      6) age< -0.5419541 7   390.4204 40.08429 *\n#>      7) age>=-0.5419541 16  3830.5510 55.58062 *\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  18.0\n#> 2  42.2\n#> 3  28.8\n#> 4  55.6\n#> 5  40.1\n#> 6  38.4\n#> 7  38.4\n#> 8  40.1\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  set_mode(\"regression\") |> \n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> Formula: compressive_strength ~ .\n#> \n#> DecisionTreeRegressionModel: uid=decision_tree_regressor__224bd5f4_4a90_4afe_9056_f064491ee63e, depth=5, numNodes=63, numFeatures=8\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, new_data = tbl_reg$test)\n```\n:::\n\n\n\n\n## Generalized Additive Models (`gen_additive_mod()`) \n\n## `mgcv` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngen_additive_mod_spec <- gen_additive_mod() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and mgcv is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngen_additive_mod_fit <- \n  gen_additive_mod_spec |> \n  fit(strength ~ s(age) + s(cement), data = reg_train)\ngen_additive_mod_fit\n#> parsnip model object\n#> \n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> strength ~ s(age) + s(cement)\n#> \n#> Estimated degrees of freedom:\n#> 4.18 3.56  total = 8.74 \n#> \n#> GCV score: 108.4401\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(gen_additive_mod_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  23.1\n#> 2  41.2\n#> 3  26.7\n#> 4  55.9\n#> 5  35.2\n#> 6  37.1\n#> 7  38.5\n#> 8  39.6\npredict(gen_additive_mod_fit, type = \"conf_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>     <dbl[1d]>   <dbl[1d]>\n#> 1        18.9        27.4\n#> 2        35.7        46.6\n#> 3        22.4        31.0\n#> 4        47.0        64.7\n#> 5        30.1        40.4\n#> 6        32.9        41.2\n#> 7        34.3        42.6\n#> 8        30.3        49.0\n```\n:::\n\n\n## Linear Reg (`linear_reg()`) \n\n## `brulee` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"brulee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> Linear regression\n#> \n#> 92 samples, 2 features, numeric outcome \n#> weight decay: 0.001 \n#> batch size: 83 \n#> scaled validation loss after 1 epoch: 291\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  33.2\n#> 2  30.0\n#> 3  21.3\n#> 4  53.7\n#> 5  42.2\n#> 6  36.2\n#> 7  37.3\n#> 8  51.6\n```\n:::\n\n\n## `gee` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"gee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `glm` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glm\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  stats::glm(formula = strength ~ ., family = stats::gaussian, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)       cement          age  \n#>      33.577        8.795        5.471  \n#> \n#> Degrees of Freedom: 91 Total (i.e. Null);  89 Residual\n#> Null Deviance:\t    26560 \n#> Residual Deviance: 15480 \tAIC: 740.6\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.1\n#> 2  30.3\n#> 3  21.6\n#> 4  51.4\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.8\npredict(linear_reg_fit, type = \"conf_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        28.8        35.4\n#> 2        27.1        33.5\n#> 3        17.3        25.9\n#> 4        44.6        58.1\n#> 5        35.6        45.0\n#> 6        32.3        38.3\n#> 7        33.2        39.4\n#> 8        41.6        56.0\n```\n:::\n\n\n## `glmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.1\n#> 2  30.3\n#> 3  21.6\n#> 4  51.4\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.8\n```\n:::\n\n\n## `glmnet` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg(penalty = 0.01) |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmnet\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\") \n#> \n#>    Df  %Dev Lambda\n#> 1   0  0.00 9.5680\n#> 2   1  5.38 8.7180\n#> 3   1  9.85 7.9430\n#> 4   1 13.56 7.2380\n#> 5   1 16.64 6.5950\n#> 6   2 19.99 6.0090\n#> 7   2 23.68 5.4750\n#> 8   2 26.75 4.9890\n#> 9   2 29.29 4.5450\n#> 10  2 31.40 4.1420\n#> 11  2 33.15 3.7740\n#> 12  2 34.61 3.4380\n#> 13  2 35.82 3.1330\n#> 14  2 36.82 2.8550\n#> 15  2 37.65 2.6010\n#> 16  2 38.34 2.3700\n#> 17  2 38.92 2.1590\n#> 18  2 39.39 1.9680\n#> 19  2 39.79 1.7930\n#> 20  2 40.12 1.6340\n#> 21  2 40.39 1.4880\n#> 22  2 40.62 1.3560\n#> 23  2 40.80 1.2360\n#> 24  2 40.96 1.1260\n#> 25  2 41.09 1.0260\n#> 26  2 41.20 0.9348\n#> 27  2 41.29 0.8517\n#> 28  2 41.36 0.7761\n#> 29  2 41.42 0.7071\n#> 30  2 41.47 0.6443\n#> 31  2 41.52 0.5871\n#> 32  2 41.55 0.5349\n#> 33  2 41.58 0.4874\n#> 34  2 41.60 0.4441\n#> 35  2 41.63 0.4046\n#> 36  2 41.64 0.3687\n#> 37  2 41.66 0.3359\n#> 38  2 41.67 0.3061\n#> 39  2 41.68 0.2789\n#> 40  2 41.68 0.2541\n#> 41  2 41.69 0.2316\n#> 42  2 41.70 0.2110\n#> 43  2 41.70 0.1922\n#> 44  2 41.71 0.1752\n#> 45  2 41.71 0.1596\n#> 46  2 41.71 0.1454\n#> 47  2 41.71 0.1325\n#> 48  2 41.71 0.1207\n#> 49  2 41.72 0.1100\n#> 50  2 41.72 0.1002\n#> 51  2 41.72 0.0913\n#> 52  2 41.72 0.0832\n#> 53  2 41.72 0.0758\n#> 54  2 41.72 0.0691\n#> 55  2 41.72 0.0630\n#> 56  2 41.72 0.0574\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.2\n#> 2  30.3\n#> 3  21.7\n#> 4  51.3\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.7\n```\n:::\n\n\n## `gls` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"gls\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: glm\n#> Model ID:  GLM_model_R_1763571327438_4147 \n#> GLM Model: summary\n#>     family     link                               regularization\n#> 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.01903 )\n#>   number_of_predictors_total number_of_active_predictors number_of_iterations\n#> 1                          2                           2                    1\n#>      training_frame\n#> 1 object_ftjflovkts\n#> \n#> Coefficients: glm coefficients\n#>       names coefficients standardized_coefficients\n#> 1 Intercept    33.577283                 33.577283\n#> 2    cement     8.708461                  8.708461\n#> 3       age     5.422201                  5.422201\n#> \n#> H2ORegressionMetrics: glm\n#> ** Reported on training data. **\n#> \n#> MSE:  168.2822\n#> RMSE:  12.97236\n#> MAE:  10.62672\n#> RMSLE:  0.4645554\n#> Mean Residual Deviance :  168.2822\n#> R^2 :  0.4171988\n#> Null Deviance :26564.74\n#> Null D.o.F. :91\n#> Residual Deviance :15481.96\n#> Residual D.o.F. :89\n#> AIC :740.6438\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.1\n#> 2  30.3\n#> 3  21.7\n#> 4  51.2\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.7\n```\n:::\n\n\n## `keras` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"keras\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `lm` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and lm is the default engine so there is no need to set that either.\nlinear_reg_spec <- linear_reg()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> stats::lm(formula = strength ~ ., data = data)\n#> \n#> Coefficients:\n#> (Intercept)       cement          age  \n#>      33.577        8.795        5.471\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.1\n#> 2  30.3\n#> 3  21.6\n#> 4  51.4\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.8\npredict(linear_reg_fit, type = \"conf_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        28.8        35.4\n#> 2        27.1        33.5\n#> 3        17.3        25.9\n#> 4        44.6        58.1\n#> 5        35.6        45.0\n#> 6        32.3        38.3\n#> 7        33.2        39.4\n#> 8        41.6        56.0\npredict(linear_reg_fit, type = \"pred_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        5.72        58.5\n#> 2        3.89        56.7\n#> 3       -4.94        48.2\n#> 4       24.3         78.5\n#> 5       13.7         67.0\n#> 6        8.95        61.7\n#> 7        9.89        62.7\n#> 8       21.6         76.0\n```\n:::\n\n\n## `lme` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"lme\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `lmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"lmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `stan` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"stan\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> stan_glm\n#>  family:       gaussian [identity]\n#>  formula:      strength ~ .\n#>  observations: 92\n#>  predictors:   3\n#> ------\n#>             Median MAD_SD\n#> (Intercept) 33.6    1.4  \n#> cement       8.8    1.4  \n#> age          5.5    1.5  \n#> \n#> Auxiliary parameter(s):\n#>       Median MAD_SD\n#> sigma 13.3    1.0  \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.1\n#> 2  30.3\n#> 3  21.6\n#> 4  51.4\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.8\npredict(linear_reg_fit, type = \"conf_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        28.8        35.6\n#> 2        27.1        33.5\n#> 3        17.3        26.0\n#> 4        44.7        58.0\n#> 5        35.8        45.0\n#> 6        32.3        38.3\n#> 7        33.2        39.5\n#> 8        41.8        55.8\npredict(linear_reg_fit, type = \"pred_int\", new_data = reg_test)\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        6.24        58.5\n#> 2        3.92        56.5\n#> 3       -4.87        48.0\n#> 4       24.2         78.2\n#> 5       14.3         68.1\n#> 6        8.85        61.7\n#> 7       10.8         62.6\n#> 8       22.3         75.6\n```\n:::\n\n\n## `stan_glmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"stan_glmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = reg_train)\nlinear_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = reg_test)\npredict(linear_reg_fit, type = \"pred_int\", new_data = reg_test)\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  set_engine(\"spark\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> Formula: compressive_strength ~ .\n#> \n#> Coefficients:\n#>        (Intercept)             cement blast_furnace_slag            fly_ash \n#>       -21.80239627         0.12003251         0.10399582         0.08747677 \n#>              water   superplasticizer   coarse_aggregate     fine_aggregate \n#>        -0.15701342         0.28531613         0.01777782         0.02018358 \n#>                age \n#>         0.11678247\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, new_data = tbl_reg$test)\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>     pred\n#>    <dbl>\n#>  1  16.5\n#>  2  19.7\n#>  3  26.1\n#>  4  23.6\n#>  5  24.2\n#>  6  29.1\n#>  7  21.3\n#>  8  24.2\n#>  9  33.9\n#> 10  57.7\n#> # ℹ more rows\n```\n:::\n\n\n## Multivariate Adaptive Regression Splines (`mars()`) \n\n## `earth` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_spec <- mars() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and earth is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_fit <- mars_spec |> fit(strength ~ ., data = reg_train)\nmars_fit\n#> parsnip model object\n#> \n#> Selected 4 of 9 terms, and 2 of 2 predictors\n#> Termination condition: RSq changed by less than 0.001 at 9 terms\n#> Importance: age, cement\n#> Number of terms at each degree of interaction: 1 3 (additive model)\n#> GCV 113.532    RSS 8915.965    GRSq 0.6153128    RSq 0.6643684\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mars_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  22.0\n#> 2  43.1\n#> 3  28.1\n#> 4  58.0\n#> 5  33.8\n#> 6  34.9\n#> 7  36.3\n#> 8  43.5\n```\n:::\n\n\n## Neural Networks (`mlp()`) \n\n## `brulee` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"brulee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)\nmlp_fit\n#> parsnip model object\n#> \n#> Multilayer perceptron\n#> \n#> relu activation,\n#> 3 hidden units,\n#> 13 model parameters\n#> 92 samples, 2 features, numeric outcome \n#> weight decay: 0.001 \n#> dropout proportion: 0 \n#> batch size: 83 \n#> learn rate: 0.01 \n#> scaled validation loss after 6 epochs: 0.21\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  21.3\n#> 2  33.9\n#> 3  23.7\n#> 4  46.9\n#> 5  42.3\n#> 6  32.2\n#> 7  34.8\n#> 8  46.9\n```\n:::\n\n\n## `brulee_two_layer` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"brulee_two_layer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)\nmlp_fit\n#> parsnip model object\n#> \n#> Multilayer perceptron\n#> \n#> c(relu,relu) activation,\n#> c(3,3) hidden units,\n#> 25 model parameters\n#> 92 samples, 2 features, numeric outcome \n#> weight decay: 0.001 \n#> dropout proportion: 0 \n#> batch size: 83 \n#> learn rate: 0.01 \n#> scaled validation loss after 21 epochs: 0.129\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  24.8\n#> 2  41.9\n#> 3  26.5\n#> 4  56.6\n#> 5  33.1\n#> 6  40.5\n#> 7  41.5\n#> 8  38.0\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)\nmlp_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: deeplearning\n#> Model ID:  DeepLearning_model_R_1763571327438_4148 \n#> Status of Neuron Layers: predicting .outcome, regression, gaussian distribution, Quadratic loss, 801 weights/biases, 14.5 KB, 920 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1     2     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.005416 0.010833 0.000000\n#> 3     3     1    Linear      NA 0.000000 0.000000  0.000501 0.000097 0.000000\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2   -0.009259   0.111978  0.497921 0.008852\n#> 3   -0.003265   0.101694  0.014595 0.000000\n#> \n#> \n#> H2ORegressionMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on full training frame **\n#> \n#> MSE:  156.8178\n#> RMSE:  12.52269\n#> MAE:  9.742575\n#> RMSLE:  0.4096152\n#> Mean Residual Deviance :  156.8178\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  26.9\n#> 2  28.8\n#> 3  18.3\n#> 4  47.1\n#> 5  34.8\n#> 6  31.5\n#> 7  32.5\n#> 8  42.5\n```\n:::\n\n\n## `keras` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"keras\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)\nmlp_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, new_data = reg_test)\n```\n:::\n\n\n## `nnet` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_spec <- mlp() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and nnet is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_fit <- mlp_spec |> fit(strength ~ ., data = reg_train)\nmlp_fit\n#> parsnip model object\n#> \n#> a 2-5-1 network with 21 weights\n#> inputs: cement age \n#> output(s): strength \n#> options were - linear output units\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mlp_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  26.0\n#> 2  42.1\n#> 3  29.2\n#> 4  67.8\n#> 5  36.7\n#> 6  33.3\n#> 7  33.3\n#> 8  33.9\n```\n:::\n\n\n## K-Nearest Neighbors (`nearest_neighbor()`) \n\n## `kknn` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnearest_neighbor_spec <- nearest_neighbor() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and kknn is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnearest_neighbor_fit <- nearest_neighbor_spec |> fit(strength ~ ., data = reg_train)\nnearest_neighbor_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> kknn::train.kknn(formula = strength ~ ., data = data, ks = min_rows(5,     data, 5))\n#> \n#> Type of response variable: continuous\n#> minimal mean absolute error: 8.257735\n#> Minimal mean squared error: 115.8737\n#> Best kernel: optimal\n#> Best k: 5\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(nearest_neighbor_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  16.3\n#> 2  35.7\n#> 3  27.5\n#> 4  56.7\n#> 5  42.6\n#> 6  41.7\n#> 7  41.2\n#> 8  50.2\n```\n:::\n\n\n## Null Model (`null_model()`) \n\n## `parsnip` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnull_model_spec <- null_model() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and parsnip is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnull_model_fit <- null_model_spec |> fit(strength ~ ., data = reg_train)\nnull_model_fit\n#> parsnip model object\n#> \n#> Null Classification Model\n#> Predicted Value: 33.57728\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(null_model_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  33.6\n#> 2  33.6\n#> 3  33.6\n#> 4  33.6\n#> 5  33.6\n#> 6  33.6\n#> 7  33.6\n#> 8  33.6\n```\n:::\n\n\n## Partial Least Squares (`pls()`) \n\n## `mixOmics` Engine \n\nThis engine requires the plsmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(plsmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_spec <- pls() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and mixOmics is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_fit <- pls_spec |> fit(strength ~ ., data = reg_train)\npls_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#>  mixOmics::spls(X = x, Y = y, ncomp = ncomp, keepX = keepX) \n#> \n#>  sPLS with a 'regression' mode with 2 sPLS components. \n#>  You entered data X of dimensions: 92 2 \n#>  You entered data Y of dimensions: 92 1 \n#> \n#>  Selection of [2] [2] variables on each of the sPLS components on the X data set. \n#>  Selection of [1] [1] variables on each of the sPLS components on the Y data set. \n#> \n#>  Main numerical outputs: \n#>  -------------------- \n#>  loading vectors: see object$loadings \n#>  variates: see object$variates \n#>  variable names: see object$names \n#> \n#>  Functions to visualise samples: \n#>  -------------------- \n#>  plotIndiv, plotArrow \n#> \n#>  Functions to visualise variables: \n#>  -------------------- \n#>  plotVar, plotLoadings, network, cim\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(pls_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  32.1\n#> 2  30.3\n#> 3  21.6\n#> 4  51.4\n#> 5  40.3\n#> 6  35.3\n#> 7  36.3\n#> 8  48.8\n```\n:::\n\n\n## Poisson Reg (`poisson_reg()`) \n\n## `gee` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"gee\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(strength ~ ., data = reg_train)\npoisson_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `glm` Engine \n\nThis engine requires the poissonreg extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(poissonreg)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and glm is the default engine so there is no need to set that either.\npoisson_reg_spec <- poisson_reg()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)\npoisson_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  stats::glm(formula = num_years ~ ., family = stats::poisson, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)          age       income  \n#>      2.2861       0.2804       0.2822  \n#> \n#> Degrees of Freedom: 1460 Total (i.e. Null);  1458 Residual\n#> Null Deviance:\t    7434 \n#> Residual Deviance: 2597 \tAIC: 8446\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = count_test)\n#> # A tibble: 9 × 1\n#>   .pred\n#>   <dbl>\n#> 1 31.6 \n#> 2  6.66\n#> 3 11.8 \n#> 4 24.8 \n#> 5 26.6 \n#> 6  8.23\n#> 7 32.1 \n#> 8  4.86\n#> 9 28.3\n```\n:::\n\n\n## `glmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(strength ~ ., data = reg_train)\npoisson_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = reg_test)\n```\n:::\n\n\n## `glmnet` Engine \n\nThis engine requires the poissonreg extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(poissonreg)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg(penalty = 0.01) |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmnet\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)\npoisson_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"poisson\") \n#> \n#>    Df  %Dev Lambda\n#> 1   0  0.00 5.9710\n#> 2   1 10.26 5.4400\n#> 3   1 18.31 4.9570\n#> 4   2 24.84 4.5170\n#> 5   2 32.06 4.1150\n#> 6   2 37.94 3.7500\n#> 7   2 42.73 3.4170\n#> 8   2 46.65 3.1130\n#> 9   2 49.87 2.8370\n#> 10  2 52.51 2.5850\n#> 11  2 54.69 2.3550\n#> 12  2 56.48 2.1460\n#> 13  2 57.96 1.9550\n#> 14  2 59.18 1.7810\n#> 15  2 60.19 1.6230\n#> 16  2 61.03 1.4790\n#> 17  2 61.72 1.3480\n#> 18  2 62.29 1.2280\n#> 19  2 62.76 1.1190\n#> 20  2 63.16 1.0190\n#> 21  2 63.48 0.9289\n#> 22  2 63.75 0.8463\n#> 23  2 63.98 0.7712\n#> 24  2 64.16 0.7026\n#> 25  2 64.31 0.6402\n#> 26  2 64.44 0.5833\n#> 27  2 64.55 0.5315\n#> 28  2 64.64 0.4843\n#> 29  2 64.71 0.4413\n#> 30  2 64.77 0.4021\n#> 31  2 64.82 0.3664\n#> 32  2 64.86 0.3338\n#> 33  2 64.90 0.3042\n#> 34  2 64.92 0.2771\n#> 35  2 64.95 0.2525\n#> 36  2 64.97 0.2301\n#> 37  2 64.98 0.2096\n#> 38  2 65.00 0.1910\n#> 39  2 65.01 0.1741\n#> 40  2 65.02 0.1586\n#> 41  2 65.03 0.1445\n#> 42  2 65.03 0.1317\n#> 43  2 65.04 0.1200\n#> 44  2 65.04 0.1093\n#> 45  2 65.05 0.0996\n#> 46  2 65.05 0.0907\n#> 47  2 65.05 0.0827\n#> 48  2 65.05 0.0753\n#> 49  2 65.06 0.0687\n#> 50  2 65.06 0.0625\n#> 51  2 65.06 0.0570\n#> 52  2 65.06 0.0519\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = count_test)\n#> # A tibble: 9 × 1\n#>   .pred\n#>   <dbl>\n#> 1 31.4 \n#> 2  6.70\n#> 3 11.8 \n#> 4 24.6 \n#> 5 26.4 \n#> 6  8.27\n#> 7 31.8 \n#> 8  4.91\n#> 9 28.1\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)\npoisson_reg_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: glm\n#> Model ID:  GLM_model_R_1763571327438_4149 \n#> GLM Model: summary\n#>    family link                               regularization\n#> 1 poisson  log Elastic Net (alpha = 0.5, lambda = 0.01194 )\n#>   number_of_predictors_total number_of_active_predictors number_of_iterations\n#> 1                          2                           2                    4\n#>      training_frame\n#> 1 object_xqwzxdmwtf\n#> \n#> Coefficients: glm coefficients\n#>       names coefficients standardized_coefficients\n#> 1 Intercept     2.286411                  2.286411\n#> 2       age     0.279967                  0.279967\n#> 3    income     0.281952                  0.281952\n#> \n#> H2ORegressionMetrics: glm\n#> ** Reported on training data. **\n#> \n#> MSE:  18.40519\n#> RMSE:  4.290128\n#> MAE:  3.297048\n#> RMSLE:  0.467537\n#> Mean Residual Deviance :  1.777749\n#> R^2 :  0.6934292\n#> Null Deviance :7434.374\n#> Null D.o.F. :1460\n#> Residual Deviance :2597.291\n#> Residual D.o.F. :1458\n#> AIC :8445.967\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = count_test)\n#> # A tibble: 9 × 1\n#>   .pred\n#>   <dbl>\n#> 1 31.6 \n#> 2  6.67\n#> 3 11.8 \n#> 4 24.8 \n#> 5 26.5 \n#> 6  8.24\n#> 7 32.0 \n#> 8  4.87\n#> 9 28.2\n```\n:::\n\n\n## `hurdle` Engine \n\nThis engine requires the poissonreg extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(poissonreg)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"hurdle\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)\npoisson_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> pscl::hurdle(formula = num_years ~ ., data = data)\n#> \n#> Count model coefficients (truncated poisson with log link):\n#> (Intercept)          age       income  \n#>      2.2911       0.2749       0.2820  \n#> \n#> Zero hurdle model coefficients (binomial with logit link):\n#> (Intercept)          age       income  \n#>      24.656        5.611       13.092\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = count_test)\n#> # A tibble: 9 × 1\n#>   .pred\n#>   <dbl>\n#> 1 31.5 \n#> 2  6.74\n#> 3 11.9 \n#> 4 24.6 \n#> 5 26.4 \n#> 6  8.32\n#> 7 31.9 \n#> 8  4.89\n#> 9 28.2\n```\n:::\n\n\n## `stan` Engine \n\nThis engine requires the poissonreg extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(poissonreg)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"stan\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(strength ~ ., data = reg_train)\npoisson_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = reg_test)\npredict(poisson_reg_fit, type = \"conf_int\", new_data = reg_test)\npredict(poisson_reg_fit, type = \"pred_int\", new_data = reg_test)\n```\n:::\n\n\n## `stan_glmer` Engine \n\nThis engine requires the multilevelmod extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"stan_glmer\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(strength ~ ., data = reg_train)\npoisson_reg_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = reg_test)\npredict(poisson_reg_fit, type = \"pred_int\", new_data = reg_test)\n```\n:::\n\n\n## `zeroinfl` Engine \n\nThis engine requires the poissonreg extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(poissonreg)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_spec <- poisson_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"zeroinfl\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson_reg_fit <- poisson_reg_spec |> fit(num_years ~ ., data = count_train)\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\npoisson_reg_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> pscl::zeroinfl(formula = num_years ~ ., data = data)\n#> \n#> Count model coefficients (poisson with log link):\n#> (Intercept)          age       income  \n#>      2.2912       0.2748       0.2821  \n#> \n#> Zero-inflation model coefficients (binomial with logit link):\n#> (Intercept)          age       income  \n#>      -48.26       -18.22       -11.72\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(poisson_reg_fit, new_data = count_test)\n#> # A tibble: 9 × 1\n#>   .pred\n#>   <dbl>\n#> 1 31.5 \n#> 2  6.74\n#> 3 11.9 \n#> 4 24.6 \n#> 5 26.4 \n#> 6  8.31\n#> 7 31.9 \n#> 8  4.93\n#> 9 28.2\n```\n:::\n\n\n## Random Forests (`rand_forest()`) \n\n## `aorsf` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"aorsf\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> ---------- Oblique random regression forest\n#> \n#>      Linear combinations: Accelerated Linear regression\n#>           N observations: 92\n#>                  N trees: 500\n#>       N predictors total: 2\n#>    N predictors per node: 2\n#>  Average leaves per tree: 13.968\n#> Min observations in leaf: 5\n#>           OOB stat value: 0.59\n#>            OOB stat type: RSQ\n#>      Variable importance: anova\n#> \n#> -----------------------------------------\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  25.0\n#> 2  36.6\n#> 3  30.4\n#> 4  55.7\n#> 5  42.0\n#> 6  38.8\n#> 7  40.6\n#> 8  53.5\n```\n:::\n\n\n## `grf` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"grf\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)\nrand_forest_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = reg_test)\npredict(rand_forest_fit, type = \"conf_int\", new_data = reg_test)\n```\n:::\n\n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: drf\n#> Model ID:  DRF_model_R_1763571327438_4150 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              50                       50               21666         6\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1        13    8.90000         12         47    29.82000\n#> \n#> \n#> H2ORegressionMetrics: drf\n#> ** Reported on training data. **\n#> ** Metrics reported on Out-Of-Bag training samples **\n#> \n#> MSE:  90.66979\n#> RMSE:  9.522068\n#> MAE:  7.491973\n#> RMSLE:  0.3441902\n#> Mean Residual Deviance :  90.66979\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  24.8\n#> 2  34.6\n#> 3  29.1\n#> 4  56.9\n#> 5  36.7\n#> 6  36.3\n#> 7  39.6\n#> 8  29.3\n```\n:::\n\n\n## `partykit` Engine \n\nThis engine requires the bonsai extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bonsai)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"partykit\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)\n\n# Too long to print\n# rand_forest_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  16.8\n#> 2  38.2\n#> 3  28.4\n#> 4  49.9\n#> 5  48.5\n#> 6  36.3\n#> 7  38.5\n#> 8  48.6\n```\n:::\n\n\n## `randomForest` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"randomForest\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#>  randomForest(x = maybe_data_frame(x), y = y) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 1\n#> \n#>           Mean of squared residuals: 90.27832\n#>                     % Var explained: 68.73\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  23.6\n#> 2  36.6\n#> 3  28.3\n#> 4  57.2\n#> 5  38.5\n#> 6  35.0\n#> 7  38.8\n#> 8  35.1\n```\n:::\n\n\n## `ranger` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and ranger is the default engine so there is no need to set that either.\n  set_engine(\"ranger\", keep.inbag = TRUE) |> \n  # However, we'll set the engine and use the keep.inbag=TRUE option so that we \n  # can produce interval predictions. This is not generally required. \n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = reg_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, keep.inbag = ~TRUE,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  500 \n#> Sample size:                      92 \n#> Number of independent variables:  2 \n#> Mtry:                             1 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       93.38443 \n#> R squared (OOB):                  0.6801029\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  24.0\n#> 2  37.4\n#> 3  28.5\n#> 4  56.5\n#> 5  38.4\n#> 6  35.8\n#> 7  38.5\n#> 8  34.5\npredict(rand_forest_fit, type = \"conf_int\", new_data = reg_test)\n#> Warning in rInfJack(pred = result$predictions, inbag = inbag.counts, used.trees\n#> = 1:num.trees): Sample size <=20, no calibration performed.\n#> # A tibble: 8 × 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1        20.4        27.6\n#> 2        31.3        43.6\n#> 3        24.2        32.7\n#> 4        44.5        68.4\n#> 5        33.4        43.4\n#> 6        31.3        40.4\n#> 7        35.5        41.4\n#> 8        27.0        42.0\n```\n:::\n\n\n## `spark` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  set_engine(\"spark\") |> \n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(compressive_strength ~ ., data = tbl_reg$training)\nrand_forest_fit\n#> parsnip model object\n#> \n#> Formula: compressive_strength ~ .\n#> \n#> RandomForestRegressionModel: uid=random_forest__9f449384_cf84_4bcb_afa5_43e10c342627, numTrees=20, numFeatures=8\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, new_data = tbl_reg$test)\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>     pred\n#>    <dbl>\n#>  1  27.1\n#>  2  28.6\n#>  3  25.9\n#>  4  29.6\n#>  5  16.4\n#>  6  34.5\n#>  7  19.2\n#>  8  30.1\n#>  9  37.5\n#> 10  44.2\n#> # ℹ more rows\n```\n:::\n\n\n## Rule Fit (`rule_fit()`) \n\n## `h2o` Engine \n\nThis engine requires the agua extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(agua)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_spec <- rule_fit() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"h2o\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_fit <- rule_fit_spec |> fit(strength ~ ., data = reg_train)\nrule_fit_fit\n#> parsnip model object\n#> \n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: rulefit\n#> Model ID:  RuleFit_model_R_1763571327438_4151 \n#> Rulefit Model Summary: \n#>     family     link           regularization number_of_predictors_total\n#> 1 gaussian identity Lasso (lambda = 0.9516 )                       1783\n#>   number_of_active_predictors number_of_iterations rule_ensemble_size\n#> 1                          70                    1               1781\n#>   number_of_trees number_of_internal_trees min_depth max_depth mean_depth\n#> 1             150                      150         0         5    4.00000\n#>   min_leaves max_leaves mean_leaves\n#> 1          0         26    11.87333\n#> \n#> \n#> H2ORegressionMetrics: rulefit\n#> ** Reported on training data. **\n#> \n#> MSE:  91.07972\n#> RMSE:  9.54357\n#> MAE:  7.180123\n#> RMSLE:  0.3532356\n#> Mean Residual Deviance :  91.07972\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rule_fit_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  27.0\n#> 2  36.1\n#> 3  26.8\n#> 4  49.8\n#> 5  42.2\n#> 6  34.7\n#> 7  39.4\n#> 8  40.8\n```\n:::\n\n\n## `xrf` Engine \n\nThis engine requires the rules extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rules)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_spec <- rule_fit() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and xrf is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_fit_fit <- rule_fit_spec |> fit(strength ~ ., data = reg_train)\nrule_fit_fit\n#> parsnip model object\n#> \n#> An eXtreme RuleFit model of 179 rules.\n#> \n#> Original Formula:\n#> \n#> strength ~ cement + age\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rule_fit_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  27.5\n#> 2  32.0\n#> 3  26.5\n#> 4  52.9\n#> 5  35.9\n#> 6  31.8\n#> 7  46.2\n#> 8  30.8\n```\n:::\n\n\n## Support Vector Machine (Linear Kernel) (`svm_linear()`) \n\n## `kernlab` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_spec <- svm_linear() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_fit <- svm_linear_spec |> fit(strength ~ ., data = reg_train)\nsvm_linear_fit\n#> parsnip model object\n#> \n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: eps-svr  (regression) \n#>  parameter : epsilon = 0.1  cost C = 1 \n#> \n#> Linear (vanilla) kernel function. \n#> \n#> Number of Support Vectors : 85 \n#> \n#> Objective Function Value : -47.4495 \n#> Training error : 0.606701\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_linear_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  29.4\n#> 2  30.9\n#> 3  21.7\n#> 4  47.1\n#> 5  36.4\n#> 6  33.4\n#> 7  34.2\n#> 8  43.2\n```\n:::\n\n\n## `LiblineaR` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_spec <- svm_linear() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and LiblineaR is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_linear_fit <- svm_linear_spec |> fit(strength ~ ., data = reg_train)\nsvm_linear_fit\n#> parsnip model object\n#> \n#> $TypeDetail\n#> [1] \"L2-regularized L2-loss support vector regression primal (L2R_L2LOSS_SVR)\"\n#> \n#> $Type\n#> [1] 11\n#> \n#> $W\n#>        cement      age     Bias\n#> [1,] 8.665447 5.486263 33.34299\n#> \n#> $Bias\n#> [1] 1\n#> \n#> $NbClass\n#> [1] 2\n#> \n#> attr(,\"class\")\n#> [1] \"LiblineaR\"\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_linear_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  31.9\n#> 2  30.1\n#> 3  21.5\n#> 4  50.9\n#> 5  39.9\n#> 6  35.0\n#> 7  36.0\n#> 8  48.3\n```\n:::\n\n\n## Support Vector Machine (Polynomial Kernel) (`svm_poly()`) \n\n## `kernlab` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_poly_spec <- svm_poly() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and kernlab is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_poly_fit <- svm_poly_spec |> fit(strength ~ ., data = reg_train)\n#>  Setting default kernel parameters\nsvm_poly_fit\n#> parsnip model object\n#> \n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: eps-svr  (regression) \n#>  parameter : epsilon = 0.1  cost C = 1 \n#> \n#> Polynomial kernel function. \n#>  Hyperparameters : degree =  1  scale =  1  offset =  1 \n#> \n#> Number of Support Vectors : 85 \n#> \n#> Objective Function Value : -47.4495 \n#> Training error : 0.606702\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_poly_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  29.4\n#> 2  30.9\n#> 3  21.7\n#> 4  47.1\n#> 5  36.4\n#> 6  33.4\n#> 7  34.2\n#> 8  43.2\n```\n:::\n\n\n## Support Vector Machine (Radial Basis Function Kernel) (`svm_rbf()`) \n\n## `kernlab` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_spec <- svm_rbf() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and kernlab is the default engine so there is no need to set that either.\n  set_mode(\"regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_fit <- svm_rbf_spec |> fit(strength ~ ., data = reg_train)\nsvm_rbf_fit\n#> parsnip model object\n#> \n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: eps-svr  (regression) \n#>  parameter : epsilon = 0.1  cost C = 1 \n#> \n#> Gaussian Radial Basis kernel function. \n#>  Hyperparameter : sigma =  2.50601403779482 \n#> \n#> Number of Support Vectors : 81 \n#> \n#> Objective Function Value : -29.5383 \n#> Training error : 0.206927\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_rbf_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  16.0\n#> 2  33.9\n#> 3  28.7\n#> 4  57.2\n#> 5  37.0\n#> 6  36.2\n#> 7  37.5\n#> 8  40.1\n```\n:::\n\n\n## `liquidSVM` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_spec <- svm_rbf() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"regression\") |>\n  set_engine(\"liquidSVM\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rbf_fit <- svm_rbf_spec |> fit(strength ~ ., data = reg_train)\nsvm_rbf_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(svm_rbf_fit, new_data = reg_test)\n#> # A tibble: 8 × 1\n#>   .pred\n#>   <dbl>\n#> 1  16.0\n#> 2  33.9\n#> 3  28.7\n#> 4  57.2\n#> 5  37.0\n#> 6  36.2\n#> 7  37.5\n#> 8  40.1\n```\n:::\n\n\n# Censored Regression Models\n\nLet's simulate a data set using the prodlim and survival packages: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(survival)\n#> \n#> Attaching package: 'survival'\n#> The following object is masked from 'package:future':\n#> \n#>     cluster\nlibrary(prodlim)\n\nset.seed(1000)\ncns_data <- \n  SimSurv(250) |> \n  mutate(event_time = Surv(time, event)) |> \n  select(event_time, X1, X2)\n\ncns_split <- initial_split(cns_data, prop = 0.98)\ncns_split\n#> <Training/Testing/Total>\n#> <245/5/250>\ncns_train <- training(cns_split)\ncns_test <- testing(cns_split)\n```\n:::\n\n\nFor some types of predictions, we need the _evaluation time(s)_ for the predictions. We'll use these three times to demonstrate: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neval_times <- c(1, 3, 5)\n```\n:::\n\n\n## Bagged Decision Trees (`bag_tree()`) \n\n## `rpart` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_spec <- bag_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and rpart is the default engine so there is no need to set that either.\n  set_mode(\"censored regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbag_tree_fit <- bag_tree_spec |> fit(event_time ~ ., data = cns_train)\nbag_tree_fit\n#> parsnip model object\n#> \n#> \n#> Bagging survival trees with 25 bootstrap replications \n#> \n#> Call: bagging.data.frame(formula = event_time ~ ., data = data, cp = ~0, \n#>     minsplit = ~2)\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(bag_tree_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       5.65\n#> 2       4.12\n#> 3       5.03\n#> 4       5.58\n#> 5       4.88\npredict(bag_tree_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\n```\n:::\n\n\n## Boosted Decision Trees (`boost_tree()`) \n\n## `mboost` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_spec <- boost_tree() |> \n  set_mode(\"censored regression\") |> \n  set_engine(\"mboost\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_tree_fit <- boost_tree_spec |> fit(event_time ~ ., data = cns_train)\nboost_tree_fit\n#> parsnip model object\n#> \n#> \n#> \t Model-based Boosting\n#> \n#> Call:\n#> mboost::blackboost(formula = formula, data = data, family = family,     control = mboost::boost_control(), tree_controls = partykit::ctree_control(teststat = \"quadratic\",         testtype = \"Teststatistic\", mincriterion = 0, minsplit = 10,         minbucket = 4, maxdepth = 2, saveinfo = FALSE))\n#> \n#> \n#> \t Cox Partial Likelihood \n#> \n#> Loss function:  \n#> \n#> Number of boosting iterations: mstop = 100 \n#> Step size:  0.1 \n#> Offset:  0 \n#> Number of baselearners:  1\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(boost_tree_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       6.51\n#> 2       3.92\n#> 3       4.51\n#> 4       7.17\n#> 5       4.51\npredict(boost_tree_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(boost_tree_fit, type = \"linear_pred\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_linear_pred\n#>               <dbl>\n#> 1           0.00839\n#> 2          -1.14   \n#> 3          -0.823  \n#> 4           0.229  \n#> 5          -0.823\n```\n:::\n\n\n## Decision Tree (`decision_tree()`) \n\n## `partykit` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"censored regression\") |>\n  set_engine(\"partykit\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(event_time ~ ., data = cns_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> \n#> Model formula:\n#> event_time ~ X1 + X2\n#> \n#> Fitted party:\n#> [1] root\n#> |   [2] X2 <= -0.36159\n#> |   |   [3] X1 <= 0: 13.804 (n = 41)\n#> |   |   [4] X1 > 0: 8.073 (n = 47)\n#> |   [5] X2 > -0.36159\n#> |   |   [6] X1 <= 0: 6.274 (n = 89)\n#> |   |   [7] X1 > 0\n#> |   |   |   [8] X2 <= 0.56098: 5.111 (n = 39)\n#> |   |   |   [9] X2 > 0.56098: 2.713 (n = 29)\n#> \n#> Number of inner nodes:    4\n#> Number of terminal nodes: 5\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       6.27\n#> 2       5.11\n#> 3       6.27\n#> 4       6.27\n#> 5       6.27\npredict(decision_tree_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\n```\n:::\n\n\n## `rpart` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_spec <- decision_tree() |>\n  # We need to set the mode since this engine works with multiple modes\n  # and rpart is the default engine so there is no need to set that either.\n  set_mode(\"censored regression\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree_fit <- decision_tree_spec |> fit(event_time ~ ., data = cns_train)\ndecision_tree_fit\n#> parsnip model object\n#> \n#> $rpart\n#> n= 245 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 245 329.03530 1.0000000  \n#>    2) X2< -0.09937043 110 119.05180 0.5464982  \n#>      4) X2< -0.9419799 41  42.43138 0.3153769  \n#>        8) X1< 0.5 20  12.93725 0.1541742 *\n#>        9) X1>=0.5 21  23.29519 0.5656502 *\n#>      5) X2>=-0.9419799 69  67.76223 0.7336317 *\n#>    3) X2>=-0.09937043 135 157.14990 1.7319010  \n#>      6) X1< 0.5 79  66.30972 1.2572690 *\n#>      7) X1>=0.5 56  69.62652 3.0428230  \n#>       14) X2< 1.222057 44  40.33335 2.5072040 *\n#>       15) X2>=1.222057 12  17.95790 6.3934130 *\n#> \n#> $survfit\n#> \n#> Call: prodlim::prodlim(formula = form, data = data)\n#> Stratified Kaplan-Meier estimator for the conditional event time survival function\n#> Discrete predictor variable: rpartFactor (0.154174164904031, 0.565650228981439, 0.733631734872791, 1.25726850344687, 2.50720371146533, 6.39341334321542)\n#> \n#> Right-censored response of a survival model\n#> \n#> No.Observations: 245 \n#> \n#> Pattern:\n#>                 Freq\n#>  event          161 \n#>  right.censored 84  \n#> \n#> $levels\n#> [1] \"0.154174164904031\" \"0.565650228981439\" \"0.733631734872791\"\n#> [4] \"1.25726850344687\"  \"2.50720371146533\"  \"6.39341334321542\" \n#> \n#> attr(,\"class\")\n#> [1] \"pecRpart\"\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(decision_tree_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       1.26\n#> 2       2.51\n#> 3       1.26\n#> 4       1.26\n#> 5       1.26\npredict(decision_tree_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\n```\n:::\n\n\n## Proportional Hazards (`proportional_hazards()`) \n\n## `glmnet` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproportional_hazards_spec <- proportional_hazards(penalty = 0.01) |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"glmnet\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproportional_hazards_fit <- proportional_hazards_spec |> fit(event_time ~ ., data = cns_train)\nproportional_hazards_fit\n#> parsnip model object\n#> \n#> \n#> Call:  glmnet::glmnet(x = data_obj$x, y = data_obj$y, family = \"cox\",      weights = weights, alpha = alpha, lambda = lambda) \n#> \n#>    Df %Dev  Lambda\n#> 1   0 0.00 0.39700\n#> 2   1 0.82 0.36170\n#> 3   1 1.51 0.32960\n#> 4   1 2.07 0.30030\n#> 5   1 2.54 0.27360\n#> 6   1 2.94 0.24930\n#> 7   2 3.28 0.22720\n#> 8   2 3.95 0.20700\n#> 9   2 4.50 0.18860\n#> 10  2 4.95 0.17180\n#> 11  2 5.33 0.15660\n#> 12  2 5.65 0.14270\n#> 13  2 5.91 0.13000\n#> 14  2 6.13 0.11840\n#> 15  2 6.31 0.10790\n#> 16  2 6.46 0.09833\n#> 17  2 6.58 0.08960\n#> 18  2 6.69 0.08164\n#> 19  2 6.77 0.07439\n#> 20  2 6.85 0.06778\n#> 21  2 6.91 0.06176\n#> 22  2 6.96 0.05627\n#> 23  2 7.00 0.05127\n#> 24  2 7.03 0.04672\n#> 25  2 7.06 0.04257\n#> 26  2 7.08 0.03879\n#> 27  2 7.10 0.03534\n#> 28  2 7.12 0.03220\n#> 29  2 7.13 0.02934\n#> 30  2 7.14 0.02673\n#> 31  2 7.15 0.02436\n#> 32  2 7.16 0.02219\n#> 33  2 7.17 0.02022\n#> 34  2 7.17 0.01843\n#> 35  2 7.18 0.01679\n#> 36  2 7.18 0.01530\n#> 37  2 7.18 0.01394\n#> 38  2 7.19 0.01270\n#> 39  2 7.19 0.01157\n#> 40  2 7.19 0.01054\n#> 41  2 7.19 0.00961\n#> 42  2 7.19 0.00875\n#> The training data has been saved for prediction.\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(proportional_hazards_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       7.80\n#> 2       4.21\n#> 3       4.63\n#> 4       5.18\n#> 5       4.42\npredict(proportional_hazards_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(proportional_hazards_fit, type = \"linear_pred\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_linear_pred\n#>               <dbl>\n#> 1            -0.108\n#> 2            -1.43 \n#> 3            -1.23 \n#> 4            -0.993\n#> 5            -1.33\n```\n:::\n\n\n## `survival` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and survival is the default engine so there is no need to set that either.\nproportional_hazards_spec <- proportional_hazards()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproportional_hazards_fit <- proportional_hazards_spec |> fit(event_time ~ ., data = cns_train)\nproportional_hazards_fit\n#> parsnip model object\n#> \n#> Call:\n#> survival::coxph(formula = event_time ~ ., data = data, model = TRUE, \n#>     x = TRUE)\n#> \n#>       coef exp(coef) se(coef)     z        p\n#> X1 0.99547   2.70599  0.16799 5.926 3.11e-09\n#> X2 0.91398   2.49422  0.09566 9.555  < 2e-16\n#> \n#> Likelihood ratio test=106.8  on 2 df, p=< 2.2e-16\n#> n= 245, number of events= 161\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(proportional_hazards_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       7.87\n#> 2       4.16\n#> 3       4.62\n#> 4       5.19\n#> 5       4.41\npredict(proportional_hazards_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(proportional_hazards_fit, type = \"linear_pred\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_linear_pred\n#>               <dbl>\n#> 1            -0.111\n#> 2            -1.49 \n#> 3            -1.27 \n#> 4            -1.02 \n#> 5            -1.37\n```\n:::\n\n\n## Random Forests (`rand_forest()`) \n\n## `aorsf` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"censored regression\") |>\n  set_engine(\"aorsf\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(event_time ~ ., data = cns_train)\nrand_forest_fit\n#> parsnip model object\n#> \n#> ---------- Oblique random survival forest\n#> \n#>      Linear combinations: Accelerated Cox regression\n#>           N observations: 245\n#>                 N events: 161\n#>                  N trees: 500\n#>       N predictors total: 2\n#>    N predictors per node: 2\n#>  Average leaves per tree: 12.4\n#> Min observations in leaf: 5\n#>       Min events in leaf: 1\n#>           OOB stat value: 0.71\n#>            OOB stat type: Harrell's C-index\n#>      Variable importance: anova\n#> \n#> -----------------------------------------\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       5.98\n#> 2       3.96\n#> 3       4.39\n#> 4       5.53\n#> 5       4.26\npredict(rand_forest_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\n```\n:::\n\n\n## `partykit` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |>\n  # We need to set the mode since this engine works with multiple modes\n  set_mode(\"censored regression\") |>\n  set_engine(\"partykit\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(event_time ~ ., data = cns_train)\n\n# Too long to print\n# rand_forest_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       5.22\n#> 2       3.99\n#> 3       3.87\n#> 4       5.54\n#> 5       3.87\npredict(rand_forest_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\n```\n:::\n\n\n## Parametric Survival Models (`survival_reg()`) \n\n## `flexsurv` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsurvival_reg_spec <- survival_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"flexsurv\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsurvival_reg_fit <- survival_reg_spec |> fit(event_time ~ ., data = cns_train)\nsurvival_reg_fit\n#> parsnip model object\n#> \n#> Call:\n#> flexsurv::flexsurvreg(formula = event_time ~ ., data = data, \n#>     dist = \"weibull\")\n#> \n#> Estimates: \n#>        data mean  est       L95%      U95%      se        exp(est)  L95%    \n#> shape        NA    2.11486   1.87774   2.38192   0.12832        NA        NA\n#> scale        NA    9.34809   8.38852  10.41743   0.51658        NA        NA\n#> X1      0.46939   -0.46483  -0.61347  -0.31619   0.07584   0.62824   0.54147\n#> X2     -0.00874   -0.42229  -0.50641  -0.33817   0.04292   0.65554   0.60266\n#>        U95%    \n#> shape        NA\n#> scale        NA\n#> X1      0.72892\n#> X2      0.71307\n#> \n#> N = 245,  Events: 161,  Censored: 84\n#> Total time at risk: 1388.951\n#> Log-likelihood = -427.4387, df = 4\n#> AIC = 862.8774\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(survival_reg_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       7.87\n#> 2       4.13\n#> 3       4.61\n#> 4       5.16\n#> 5       4.40\npredict(survival_reg_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(survival_reg_fit, type = \"hazard\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(survival_reg_fit, type = \"linear_pred\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_linear_pred\n#>               <dbl>\n#> 1              2.18\n#> 2              1.54\n#> 3              1.65\n#> 4              1.76\n#> 5              1.60\npredict(survival_reg_fit, type = \"quantile\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_quantile\n#>        <qtls(9)>\n#> 1         [7.47]\n#> 2         [3.92]\n#> 3         [4.37]\n#> 4          [4.9]\n#> 5         [4.18]\n```\n:::\n\n\n## `flexsurvspline` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsurvival_reg_spec <- survival_reg() |> \n  # This engine works with a single mode so no need to set that\n  set_engine(\"flexsurvspline\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsurvival_reg_fit <- survival_reg_spec |> fit(event_time ~ ., data = cns_train)\nsurvival_reg_fit\n#> parsnip model object\n#> \n#> Call:\n#> flexsurv::flexsurvspline(formula = event_time ~ ., data = data)\n#> \n#> Estimates: \n#>         data mean  est       L95%      U95%      se        exp(est)  L95%    \n#> gamma0        NA   -4.72712  -5.31772  -4.13651   0.30134        NA        NA\n#> gamma1        NA    2.11487   1.86338   2.36637   0.12832        NA        NA\n#> X1       0.46939    0.98305   0.65928   1.30683   0.16519   2.67261   1.93340\n#> X2      -0.00874    0.89308   0.70943   1.07673   0.09370   2.44265   2.03283\n#>         U95%    \n#> gamma0        NA\n#> gamma1        NA\n#> X1       3.69444\n#> X2       2.93508\n#> \n#> N = 245,  Events: 161,  Censored: 84\n#> Total time at risk: 1388.951\n#> Log-likelihood = -427.4387, df = 4\n#> AIC = 862.8774\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(survival_reg_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       7.87\n#> 2       4.13\n#> 3       4.61\n#> 4       5.16\n#> 5       4.40\npredict(survival_reg_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(survival_reg_fit, type = \"hazard\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(survival_reg_fit, type = \"linear_pred\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_linear_pred\n#>               <dbl>\n#> 1             -4.62\n#> 2             -3.26\n#> 3             -3.49\n#> 4             -3.73\n#> 5             -3.39\npredict(survival_reg_fit, type = \"quantile\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_quantile\n#>        <qtls(9)>\n#> 1         [7.47]\n#> 2         [3.92]\n#> 3         [4.37]\n#> 4          [4.9]\n#> 5         [4.18]\n```\n:::\n\n\n## `survival` Engine \n\nThis engine requires the censored extension package, so let's load this first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(censored)\n```\n:::\n\n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This engine works with a single mode so no need to set that\n# and survival is the default engine so there is no need to set that either.\nsurvival_reg_spec <- survival_reg()\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsurvival_reg_fit <- survival_reg_spec |> fit(event_time ~ ., data = cns_train)\nsurvival_reg_fit\n#> parsnip model object\n#> \n#> Call:\n#> survival::survreg(formula = event_time ~ ., data = data, model = TRUE)\n#> \n#> Coefficients:\n#> (Intercept)          X1          X2 \n#>   2.2351722  -0.4648296  -0.4222887 \n#> \n#> Scale= 0.4728442 \n#> \n#> Loglik(model)= -427.4   Loglik(intercept only)= -481.3\n#> \tChisq= 107.73 on 2 degrees of freedom, p= <2e-16 \n#> n= 245\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(survival_reg_fit, type = \"time\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_time\n#>        <dbl>\n#> 1       8.88\n#> 2       4.67\n#> 3       5.20\n#> 4       5.83\n#> 5       4.97\npredict(survival_reg_fit, type = \"survival\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(survival_reg_fit, type = \"hazard\", new_data = cns_test, eval_time = eval_times)\n#> # A tibble: 5 × 1\n#>   .pred           \n#>   <list>          \n#> 1 <tibble [3 × 2]>\n#> 2 <tibble [3 × 2]>\n#> 3 <tibble [3 × 2]>\n#> 4 <tibble [3 × 2]>\n#> 5 <tibble [3 × 2]>\npredict(survival_reg_fit, type = \"linear_pred\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_linear_pred\n#>               <dbl>\n#> 1              2.18\n#> 2              1.54\n#> 3              1.65\n#> 4              1.76\n#> 5              1.60\npredict(survival_reg_fit, type = \"quantile\", new_data = cns_test)\n#> # A tibble: 5 × 1\n#>   .pred_quantile\n#>        <qtls(9)>\n#> 1         [7.47]\n#> 2         [3.92]\n#> 3         [4.37]\n#> 4          [4.9]\n#> 5         [4.18]\n```\n:::\n\n\n# Quantile Regression Models\n\nTo demonstrate quantile regression, let's make a larger version of our regression data: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(938)\nqnt_split <-\n  modeldata::concrete |> \n  slice_sample(n = 100) |> \n  select(strength = compressive_strength, cement, age) |> \n  initial_split(prop = 0.95, strata = strength)\nqnt_split\n#> <Training/Testing/Total>\n#> <92/8/100>\n\nqnt_rec <- \n  recipe(strength ~ ., data = training(qnt_split)) |> \n  step_normalize(all_numeric_predictors()) |> \n  prep()\n\nqnt_train <- bake(qnt_rec, new_data = NULL)\nqnt_test <- bake(qnt_rec, new_data = testing(qnt_split))\n```\n:::\n\n\nWe'll also predict these quantile levels: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqnt_lvls <- (1:3) / 4\n```\n:::\n\n\n\n## Linear Regression (`linear_reg()`) \n\n## `quantreg` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_spec <- linear_reg() |> \n  set_engine(\"quantreg\") |> \n  set_mode(\"quantile regression\", quantile_levels = qnt_lvls)\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg_fit <- linear_reg_spec |> fit(strength ~ ., data = qnt_train)\nlinear_reg_fit\n#> parsnip model object\n#> \n#> Call:\n#> quantreg::rq(formula = strength ~ ., tau = quantile_levels, data = data)\n#> \n#> Coefficients:\n#>             tau= 0.25 tau= 0.50 tau= 0.75\n#> (Intercept) 23.498029 33.265428 42.046031\n#> cement       6.635233  7.955658  8.181235\n#> age          5.566668  9.514832  7.110702\n#> \n#> Degrees of freedom: 92 total; 89 residual\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(linear_reg_fit, type = \"quantile\", new_data = qnt_test)\n#> # A tibble: 8 × 1\n#>   .pred_quantile\n#>        <qtls(3)>\n#> 1         [29.2]\n#> 2         [31.5]\n#> 3         [21.4]\n#> 4         [48.3]\n#> 5         [36.6]\n#> 6         [33.8]\n#> 7         [34.6]\n#> 8         [43.8]\n```\n:::\n\n\n## Random Forests (`rand_forest()`) \n\n## `grf` Engine \n\nWe create a model specification via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_spec <- rand_forest() |> \n  set_mode(\"quantile regression\", quantile_levels = qnt_lvls) |>\n  set_engine(\"grf\")\n```\n:::\n\n\nNow we create the model fit object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest_fit <- rand_forest_spec |> fit(strength ~ ., data = qnt_train)\nrand_forest_fit\n```\n:::\n\n\nThe holdout data can be predicted:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(rand_forest_fit, type = \"quantile\", new_data = qnt_test)\n```\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}